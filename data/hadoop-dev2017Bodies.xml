<?xml version="1.0" encoding="utf-8"?>
<emails><email><emailId>20170501095404</emailId><senderName>"Akira Ajisaka (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-05-01 09:54:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14367) Remove unused setting from pom.xml</subject><body>

Akira Ajisaka created HADOOP-14367:
--------------------------------------

             Summary: Remove unused setting from pom.xml
                 Key: HADOOP-14367
                 URL: https://issues.apache.org/jira/browse/HADOOP-14367
             Project: Hadoop Common
          Issue Type: Improvement
          Components: build
            Reporter: Akira Ajisaka
            Priority: Minor


The settings is not used anywhere.
{code:title=hadoop-project/pom.xml}
    &lt;tomcat.version&gt;6.0.48&lt;/tomcat.version&gt;
{code}



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20171001190400</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-10-01 19:04:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14918) remove the Local Dynamo DB test option</subject><body>

Steve Loughran created HADOOP-14918:
---------------------------------------

             Summary: remove the Local Dynamo DB test option
                 Key: HADOOP-14918
                 URL: https://issues.apache.org/jira/browse/HADOOP-14918
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/s3
    Affects Versions: 3.0.0
            Reporter: Steve Loughran


I'm going to propose cutting out the localdynamo test option for s3guard

* the local DDB JAR is unmaintained/lags the SDK We work with...eventually there'll \
                be differences in API.
* as the local dynamo DB is unshaded. it complicates classpath setup for the build. \
Remove it and there's no need to worry about versions of anything other than the \
                shaded AWS
* it complicates test runs. Now we need to test for both localdynamo *and* real \
                dynamo
* but we can't ignore real dynamo, because that's the one which matters

While the local option promises to reduce test costs, really, it's just adding \
complexity. If you are testing with s3guard, you need to have a real table to test \
against., And with the exception of those people testing s3a against non-AWS, \
consistent endpoints, everyone should be testing with S3Guard.

Straightforward to remove.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170125191433</emailId><senderName>Karthik Kambatla</senderName><senderEmail>kasha@cloudera.com</senderEmail><timestampReceived>2017-01-25 19:14:33-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>


Thanks for driving the alphas, Andrew. I don't see the need to restart the
vote and I feel it is okay to fix the minor issues before releasing.

+1 (binding). Downloaded source, stood up a pseudo-distributed cluster with
FairScheduler, ran example jobs, and played around with the UI.

Thanks
Karthik


On Fri, Jan 20, 2017 at 2:36 PM, Andrew Wang &lt;andrew.wang@cloudera.com&gt;
wrote:

&gt; Hi all,
&gt;
&gt; With heartfelt thanks to many contributors, the RC0 for 3.0.0-alpha2 is
&gt; ready.
&gt;
&gt; 3.0.0-alpha2 is the second alpha in the planned 3.0.0 release line leading
&gt; up to a 3.0.0 GA. It comprises 857 fixes, improvements, and new features
&gt; since alpha1 was released on September 3rd, 2016.
&gt;
&gt; More information about the 3.0.0 release plan can be found here:
&gt;
&gt; https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+3.0.0+release
&gt;
&gt; The artifacts can be found here:
&gt;
&gt; http://home.apache.org/~wang/3.0.0-alpha2-RC0/
&gt;
&gt; This vote will run 5 days, ending on 01/25/2017 at 2PM pacific.
&gt;
&gt; I ran basic validation with a local pseudo cluster and a Pi job. RAT output
&gt; was clean.
&gt;
&gt; My +1 to start.
&gt;
&gt; Thanks,
&gt; Andrew
&gt;


</body></email><email><emailId>20171201201844</emailId><senderName>Rushabh Shah</senderName><senderEmail>rushabhs@oath.com.invalid</senderEmail><timestampReceived>2017-12-01 20:18:44-0400</timestampReceived><subject>Same jenkins build running on 2 patches.</subject><body>


Hi All,
Recently I uploaded trunk, branch-2 and branch-2.8 patch
(HDFS-12396.006.patch, HDFS-12396-branch-2.patch, HDFS-12396-branch-2.8.patch
respectively) for HDFS-12396
&lt;https://issues.apache.org/jira/browse/HDFS-12396&gt;.
Jenkins triggered 2 builds, one for trunk patch and another for branch-2.8.

Jenkins build for trunk:
https://builds.apache.org/job/PreCommit-HDFS-Build/22240
Jenkins build for branch-2.8:
https://builds.apache.org/job/PreCommit-HDFS-Build/22241

Something weird happened in trunk build (build:22240)
Initially it downloaded trunk version of patch.
Below are some relevant lines from console output.

&gt; Processing: HDFS-12396
&gt; HDFS-12396 patch is being downloaded at Thu Nov 30 22:21:19 UTC 2017 from
&gt;
&gt; https://issues.apache.org/jira/secure/attachment/12900094/HDFS-12396.006.patch
&gt; -&gt; Downloaded




But 11 minutes later, the same build downloaded branch-2.8 version of the
patch.

&gt; Processing: HDFS-12396
&gt; HDFS-12396 patch is being downloaded at Thu Nov 30 22:32:38 UTC 2017 from
&gt;
&gt; https://issues.apache.org/jira/secure/attachment/12900099/HDFS-12396-branch-2.8.patch
&gt; -&gt; Downloaded


Genericqa attached 2 reports on the jira both for branch-2.8 patch.

Can someone explain me what happened ?

Adding Allen as cc.


Thanks,
Rushabh Shah


</body></email><email><emailId>20171201204338</emailId><senderName>Allen Wittenauer</senderName><senderEmail>aw@effectivemachines.com</senderEmail><timestampReceived>2017-12-01 20:43:38-0400</timestampReceived><subject>Re: Same jenkins build running on 2 patches.</subject><body>


&gt; On Dec 1, 2017, at 12:18 PM, Rushabh Shah &lt;rushabhs@oath.com&gt; wrote:
&gt; Can someone explain me what happened ?

	Yetus downloaded the patch to make sure it applied before bothering to do anything \
else to make sure it wasn't going to burn cycles on the build boxes for no reason.  \
Docker mode was active so it then went to re-exec itself under Docker.  But it had to \
build the Docker image first. This can take anywhere from 9 minutes to 20 minutes, \
depending primarily on which branch's Dockerfile was in use.  While this was going \
on, another two patches were uploaded.  Docker build finishes. When Yetus re-exec'ed \
itself under Docker, it re-grabs the patch (since the world—including Yetus \
itself!--may be different now that it is Docker).  In this case, it grabbed the last \
of the newly uploaded patches and attempted to churn it's way through it.

	a) Uploading two patches at once has never ever worked and will likely never be made \
to work. (There are lots of reasons for this.)

	b) Before uploading a new patch, wait for the feedback or at least make sure the \
Jenkins job is actually past "Determining needed tests" before uploading a new one.  \
Just be aware that test output is going to get very hard to follow with all of the \
                cross posting.
---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20171201213645</emailId><senderName>Jason Lowe</senderName><senderEmail>jlowe@oath.com.invalid</senderEmail><timestampReceived>2017-12-01 21:36:45-0400</timestampReceived><subject>Re: Same jenkins build running on 2 patches.</subject><body>


Is it possible to track the patch by JIRA attachment ID rather than assume
the most recent attachment is the right one?  I thought the admin precommit
build was kicking off the project-specific precommit build with an
attachment ID argument so the project precommit can be consistent with the
admin precommit build on what triggered the precommit process.  If the
patch is tracked by attachment ID then I think the build would remain
consistent even when users attach new patches in the middle of the
precommit process.

Jason


On Fri, Dec 1, 2017 at 2:43 PM, Allen Wittenauer &lt;aw@effectivemachines.com&gt;
wrote:

&gt;
&gt; &gt; On Dec 1, 2017, at 12:18 PM, Rushabh Shah &lt;rushabhs@oath.com&gt; wrote:
&gt; &gt; Can someone explain me what happened ?
&gt;
&gt;         Yetus downloaded the patch to make sure it applied before
&gt; bothering to do anything else to make sure it wasn't going to burn cycles
&gt; on the build boxes for no reason.  Docker mode was active so it then went
&gt; to re-exec itself under Docker.  But it had to build the Docker image
&gt; first. This can take anywhere from 9 minutes to 20 minutes, depending
&gt; primarily on which branch's Dockerfile was in use.  While this was going
&gt; on, another two patches were uploaded.  Docker build finishes. When Yetus
&gt; re-exec'ed itself under Docker, it re-grabs the patch (since the
&gt; world—including Yetus itself!--may be different now that it is Docker).  In
&gt; this case, it grabbed the last of the newly uploaded patches and attempted
&gt; to churn it's way through it.
&gt;
&gt;         a) Uploading two patches at once has never ever worked and will
&gt; likely never be made to work. (There are lots of reasons for this.)
&gt;
&gt;         b) Before uploading a new patch, wait for the feedback or at least
&gt; make sure the Jenkins job is actually past "Determining needed tests"
&gt; before uploading a new one.  Just be aware that test output is going to get
&gt; very hard to follow with all of the cross posting.
&gt; ---------------------------------------------------------------------
&gt; To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
&gt; For additional commands, e-mail: common-dev-help@hadoop.apache.org
&gt;
&gt;


</body></email><email><emailId>20171201222957</emailId><senderName>Rushabh Shah</senderName><senderEmail>rushabhs@oath.com.invalid</senderEmail><timestampReceived>2017-12-01 22:29:57-0400</timestampReceived><subject>Re: Same jenkins build running on 2 patches.</subject><body>


Thanks Allen for your response.

On Fri, Dec 1, 2017 at 2:43 PM, Allen Wittenauer &lt;aw@effectivemachines.com&gt;
wrote:

&gt;
&gt; &gt; On Dec 1, 2017, at 12:18 PM, Rushabh Shah &lt;rushabhs@oath.com&gt; wrote:
&gt; &gt; Can someone explain me what happened ?
&gt;
&gt;         Yetus downloaded the patch to make sure it applied before
&gt; bothering to do anything else to make sure it wasn't going to burn cycles
&gt; on the build boxes for no reason.  Docker mode was active so it then went
&gt; to re-exec itself under Docker.  But it had to build the Docker image
&gt; first. This can take anywhere from 9 minutes to 20 minutes, depending
&gt; primarily on which branch's Dockerfile was in use.  While this was going
&gt; on, another two patches were uploaded.  Docker build finishes. When Yetus
&gt; re-exec'ed itself under Docker, it re-grabs the patch (since the
&gt; world—including Yetus itself!--may be different now that it is Docker).  In
&gt; this case, it grabbed the last of the newly uploaded patches and attempted
&gt; to churn it's way through it.
&gt;
&gt;         a) Uploading two patches at once has never ever worked and will
&gt; likely never be made to work. (There are lots of reasons for this.)
&gt;
&gt;         b) Before uploading a new patch, wait for the feedback or at least
&gt; make sure the Jenkins job is actually past "Determining needed tests"
&gt; before uploading a new one.  Just be aware that test output is going to get
&gt; very hard to follow with all of the cross posting.


</body></email><email><emailId>20170301190245</emailId><senderName>"Erik Krogen (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-03-01 19:02:45-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14136) Default FS For ViewFS</subject><body>

Erik Krogen created HADOOP-14136:
------------------------------------

             Summary: Default FS For ViewFS
                 Key: HADOOP-14136
                 URL: https://issues.apache.org/jira/browse/HADOOP-14136
             Project: Hadoop Common
          Issue Type: Improvement
          Components: viewfs
            Reporter: Erik Krogen


It would be useful if ViewFS had the ability to designate one FileSystem as a \
"primary"/"default" to fall back to in the case that an entry in the mount table is \
not found. Consider the situation when you have a mount table that looks like:

{code}
/data -&gt; hdfs://nn1/data
/logs -&gt; hdfs://nn1/logs
/user -&gt; hdfs://nn1/user
/remote -&gt; hdfs://nn2/remote
{code}

{{nn1}} here is being used as the primary, with a specific directory 'remote' being \
offloaded to another namenode. This works, but if we want to add another top-level \
directory to {{nn1}}, we have to update all of our client-side mount tables. Merge \
links (HADOOP-8298) could be used to achieve this but they do not seem to be coming \
soon; this special case of a default FS is much simpler - try to resolve through the \
VFS mount table, if not found, then resolve to the defaultFS.

There is a good discussion of this at \
https://issues.apache.org/jira/browse/HADOOP-13055?focusedCommentId=15733822&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15733822





--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170801041902</emailId><senderName>"Xiao Chen (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 04:19:02-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14705) Add batched reencryptEncryptedKey interface to KMS</subject><body>

Xiao Chen created HADOOP-14705:
----------------------------------

             Summary: Add batched reencryptEncryptedKey interface to KMS
                 Key: HADOOP-14705
                 URL: https://issues.apache.org/jira/browse/HADOOP-14705
             Project: Hadoop Common
          Issue Type: Improvement
          Components: kms
            Reporter: Xiao Chen
            Assignee: Xiao Chen


HADOOP-13827 already enabled the KMS to re-encrypt a {{EncryptedKeyVersion}}.

But as the performance results of HDFS-10899 turns out, communication overhead with \
the KMS occupies the majority of the time. So we need a batched interface to \
re-encrypt multiple EDEKs in 1 call.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170901185311</emailId><senderName>Andrew Wang</senderName><senderEmail>andrew.wang@cloudera.com</senderEmail><timestampReceived>2017-09-01 18:53:11-0400</timestampReceived><subject>Re: [DISCUSS] Branches and versions for Hadoop 3</subject><body>


Hi folks,

We've landed two of our beta1 features, S3Guard and TSv2, into trunk. Jian
just sent out the vote for our remaining beta1 feature, YARN native
services, but I think it's time to branch to unblock the resource profiles
merge to 3.1.

I'll cut just branch-3.0 for now, since we don't have anything urgent that
needs to go into 3.0.0-beta1 vs. 3.0.0 GA.

Cheers,
Andrew

On Tue, Aug 29, 2017 at 11:21 PM, varunsaxena@apache.org &lt;
varun.saxena.apache@gmail.com&gt; wrote:

&gt; Hi Andrew,
&gt;
&gt; We have completed the merge of TSv2 to trunk.
&gt; You can now go ahead with the branching.
&gt;
&gt; Regards,
&gt; Varun Saxena.
&gt;
&gt; On Tue, Aug 29, 2017 at 11:35 PM, Andrew Wang &lt;andrew.wang@cloudera.com&gt;
&gt; wrote:
&gt;
&gt;&gt; Sure. Ping me when the TSv2 goes in, and I can take care of branching.
&gt;&gt;
&gt;&gt; We're still waiting on the native services and S3Guard merges, but I
&gt;&gt; don't want to hold branching to the last minute.
&gt;&gt;
&gt;&gt; On Tue, Aug 29, 2017 at 10:51 AM, Vrushali C &lt;vrushalic2016@gmail.com&gt;
&gt;&gt; wrote:
&gt;&gt;
&gt;&gt;&gt; Hi Andrew,
&gt;&gt;&gt; As Rohith mentioned, if you are good with it, from the TSv2 side, we are
&gt;&gt;&gt; ready to go for merge tonight itself (Pacific time)  right after the voting
&gt;&gt;&gt; period ends. Varun Saxena has been diligently rebasing up until now so most
&gt;&gt;&gt; likely our merge should be reasonably straightforward.
&gt;&gt;&gt;
&gt;&gt;&gt; @Wangda: your resource profile vote ends tomorrow, could we please
&gt;&gt;&gt; coordinate our merges?
&gt;&gt;&gt;
&gt;&gt;&gt; thanks
&gt;&gt;&gt; Vrushali
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; On Mon, Aug 28, 2017 at 10:45 PM, Rohith Sharma K S &lt;
&gt;&gt;&gt; rohithsharmaks@apache.org&gt; wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On 29 August 2017 at 06:24, Andrew Wang &lt;andrew.wang@cloudera.com&gt;
&gt;&gt;&gt;&gt; wrote:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; &gt; So far I've seen no -1's to the branching proposal, so I plan to
&gt;&gt;&gt;&gt; execute
&gt;&gt;&gt;&gt; &gt; this tomorrow unless there's further feedback.
&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt; For on going branch merge threads i.e TSv2, voting will be closing
&gt;&gt;&gt;&gt; tomorrow. Does it end up in merging into trunk(3.1.0-SNAPSHOT) and
&gt;&gt;&gt;&gt; branch-3.0(3.0.0-beta1-SNAPSHOT) ? If so, would you be able to wait for
&gt;&gt;&gt;&gt; couple of more days before creating branch-3.0 so that TSv2 branch merge
&gt;&gt;&gt;&gt; would be done directly to trunk?
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt; &gt; Regarding the above discussion, I think Jason and I have essentially
&gt;&gt;&gt;&gt; the
&gt;&gt;&gt;&gt; &gt; same opinion.
&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt; &gt; I hope that keeping trunk a release branch means a higher bar for
&gt;&gt;&gt;&gt; merges
&gt;&gt;&gt;&gt; &gt; and code review in general. In the past, I've seen some patches
&gt;&gt;&gt;&gt; committed
&gt;&gt;&gt;&gt; &gt; to trunk-only as a way of passing responsibility to a future user or
&gt;&gt;&gt;&gt; &gt; reviewer. That doesn't help anyone; patches should be committed with
&gt;&gt;&gt;&gt; the
&gt;&gt;&gt;&gt; &gt; intent of running them in production.
&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt; &gt; I'd also like to repeat the above thanks to the many, many
&gt;&gt;&gt;&gt; contributors
&gt;&gt;&gt;&gt; &gt; who've helped with release improvements. Allen's work on
&gt;&gt;&gt;&gt; create-release and
&gt;&gt;&gt;&gt; &gt; automated changes and release notes were essential, as was Xiao's
&gt;&gt;&gt;&gt; work on
&gt;&gt;&gt;&gt; &gt; LICENSE and NOTICE files. I'm also looking forward to Marton's site
&gt;&gt;&gt;&gt; &gt; improvements, which addresses one of the remaining sore spots in the
&gt;&gt;&gt;&gt; &gt; release process.
&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt; &gt; Things have gotten smoother with each alpha we've done over the last
&gt;&gt;&gt;&gt; year,
&gt;&gt;&gt;&gt; &gt; and it's a testament to everyone's work that we have a good
&gt;&gt;&gt;&gt; probability of
&gt;&gt;&gt;&gt; &gt; shipping beta and GA later this year.
&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt; &gt; Cheers,
&gt;&gt;&gt;&gt; &gt; Andrew
&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;
&gt;


</body></email><email><emailId>20170901190208</emailId><senderName>Andrew Wang</senderName><senderEmail>andrew.wang@cloudera.com</senderEmail><timestampReceived>2017-09-01 19:02:08-0400</timestampReceived><subject>Heads up: branch-3.0 has been cut, commit here for 3.0.0-beta1</subject><body>


Hi folks,

I've proceeded with the plan from our earlier thread and cut branch-3.0.
The branches and maven versions are now set as follows:

trunk: 3.1.0-SNAPSHOT
branch-3.0: 3.0.0-beta1-SNAPSHOT

branch-2's are still the same.

This means if you want to commit something for beta1, commit it to
branch-3.0 too. Excepting features already committed for beta1 (e.g. EC,
native services, S3Guard, TSv2, YARN federation), please treat branch-3.0
the same as a maintenance release branch.

I'm planning to cut the release branch branch-3.0.0-beta1 just before RC.
If you have anything we pushed out of 3.0.0-beta1 and is waiting for 3.0.0
GA, please hold it in trunk until after we release 3.0.0-beta1 (which
should be relatively soon).

Best,
Andrew


</body></email><email><emailId>20170403160414</emailId><senderName>Allen Wittenauer</senderName><senderEmail>aw@effectivemachines.com</senderEmail><timestampReceived>2017-04-03 16:04:14-0400</timestampReceived><subject>[DISCUSS] Changing the default class path for clients</subject><body>


	This morning I had a bit of a shower thought:

	With the new shaded hadoop client in 3.0, is there any reason the default classpath \
should remain the full blown jar list?  e.g., shouldn't ‘hadoop classpath' just \
return configuration, user supplied bits (e.g., HADOOP_USER_CLASSPATH, etc), \
HADOOP_OPTIONAL_TOOLS, and hadoop-client-runtime? We'd obviously have to add some \
plumbing for daemons and the capability for the user to get the full list, but that \
should be trivial.  

	Thoughts?
---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170403172626</emailId><senderName>Andrew Wang</senderName><senderEmail>andrew.wang@cloudera.com</senderEmail><timestampReceived>2017-04-03 17:26:26-0400</timestampReceived><subject>Re: [DISCUSS] Changing the default class path for clients</subject><body>


What's the current contract for `hadoop classpath`? Would it be safer to
introduce `hadoop userclasspath` or similar for this behavior?

I'm betting that changing `hadoop classpath` will lead to some breakages,
so I'd prefer to make this new behavior opt-in.

Best,
Andrew

On Mon, Apr 3, 2017 at 9:04 AM, Allen Wittenauer &lt;aw@effectivemachines.com&gt;
wrote:

&gt;
&gt;         This morning I had a bit of a shower thought:
&gt;
&gt;         With the new shaded hadoop client in 3.0, is there any reason the
&gt; default classpath should remain the full blown jar list?  e.g., shouldn't
&gt; ‘hadoop classpath' just return configuration, user supplied bits (e.g.,
&gt; HADOOP_USER_CLASSPATH, etc), HADOOP_OPTIONAL_TOOLS, and
&gt; hadoop-client-runtime? We'd obviously have to add some plumbing for daemons
&gt; and the capability for the user to get the full list, but that should be
&gt; trivial.
&gt;
&gt;         Thoughts?
&gt; ---------------------------------------------------------------------
&gt; To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
&gt; For additional commands, e-mail: common-dev-help@hadoop.apache.org
&gt;
&gt;


</body></email><email><emailId>20171020151903</emailId><senderName>Anu Engineer</senderName><senderEmail>aengineer@hortonworks.com</senderEmail><timestampReceived>2017-10-20 15:19:03-0400</timestampReceived><subject>=?utf-8?B?UmU6IOetlOWkjTogW0RJU0NVU1NJT05dIE1lcmdpbmcgSERGUy03MjQwIE9i?= =?utf-8?Q?ject_Store_(Ozone</subject><body>
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGNvbW1vbi1kZXYt
dW5zdWJzY3JpYmVAaGFkb29wLmFwYWNoZS5vcmcNCkZvciBhZGRpdGlvbmFsIGNvbW1hbmRz
LCBlLW1haWw6IGNvbW1vbi1kZXYtaGVscEBoYWRvb3AuYXBhY2hlLm9yZw0K
</body></email><email><emailId>20171020163726</emailId><senderName>larry mccay</senderName><senderEmail>lmccay@apache.org</senderEmail><timestampReceived>2017-10-20 16:37:26-0400</timestampReceived><subject>=?UTF-8?B?UmU6IOetlOWkjTogW0RJU0NVU1NJT05dIE1lcmdpbmcgSERGUy03MjQwIE9iamVjdCBTdA==?= =?UTF-8?B?b3JlI</subject><body>


I previously sent this same email from my work email and it doesn't seem to
have gone through - resending from apache account (apologizing up from for
the length)....

For such sizable merges in Hadoop, I would like to start doing security
audits in order to have an initial idea of the attack surface, the
protections available for known threats, what sort of configuration is
being used to launch processes, etc.

I dug into the architecture documents while in the middle of this list -
nice docs!
I do intend to try and make a generic check list like this for such
security audits in the future so a lot of this is from that but I tried to
also direct specific questions from those docs as well.

1. UIs
I see there are at least two UIs - Storage Container Manager and Key Space
Manager. There are a number of typical vulnerabilities that we find in UIs

1.1. What sort of validation is being done on any accepted user input?
(pointers to code would be appreciated)
1.2. What explicit protections have been built in for (pointers to code
would be appreciated):
  1.2.1. cross site scripting
  1.2.2. cross site request forgery
  1.2.3. click jacking (X-Frame-Options)
1.3. What sort of authentication is required for access to the UIs?
1.4. What authorization is available for determining who can access what
capabilities of the UIs for either viewing, modifying data or affecting
object stores and related processes?
1.5. Are the UIs built with proxying in mind by leveraging X-Forwarded
headers?
1.6. Is there any input that will ultimately be persisted in configuration
for executing shell commands or processes?
1.7. Do the UIs support the trusted proxy pattern with doas impersonation?
1.8. Is there TLS/SSL support?

2. REST APIs

2.1. Do the REST APIs support the trusted proxy pattern with doas
impersonation capabilities?
2.2. What explicit protections have been built in for:
  2.2.1. cross site scripting (XSS)
  2.2.2. cross site request forgery (CSRF)
  2.2.3. XML External Entity (XXE)
2.3. What is being used for authentication - Hadoop Auth Module?
2.4. Are there separate processes for the HTTP resources (UIs and REST
endpoints) or are the part of existing HDFS processes?
2.5. Is there TLS/SSL support?
2.6. Are there new CLI commands and/or clients for access the REST APIs?
2.7. Bucket Level API allows for setting of ACLs on a bucket - what
authorization is required here - is there a restrictive ACL set on creation?
2.8. Bucket Level API allows for deleting a bucket - I assume this is
dependent on ACLs based access control?
2.9. Bucket Level API to list bucket returns up to 1000 keys - is there
paging available?
2.10. Storage Level APIs indicate "Signed with User Authorization" what
does this refer to exactly?
2.11. Object Level APIs indicate that there is no ACL support and only
bucket owners can read and write - but there are ACL APIs on the Bucket
Level are they meaningless for now?
2.12. How does a REST client know which Ozone Handler to connect to or am I
missing some well known NN type endpoint in the architecture doc somewhere?

3. Encryption

3.1. Is there any support for encryption of persisted data?
3.2. If so, is KMS and the hadoop key command used for key management?

4. Configuration

4.1. Are there any passwords or secrets being added to configuration?
4.2. If so, are they accessed via Configuration.getPassword() to allow for
provisioning in credential providers?
4.3. Are there any settings that are used to launch docker containers or
shell out any commands, etc?

5. HA

5.1. Are there provisions for HA?
5.2. Are we leveraging the existing HA capabilities in HDFS?
5.3. Is Storage Container Manager a SPOF?
5.4. I see HA listed in future work in the architecture doc - is this still
an open issue?

On Fri, Oct 20, 2017 at 11:19 AM, Anu Engineer &lt;aengineer@hortonworks.com&gt;
wrote:

&gt; Hi Steve,
&gt;
&gt; In addition to everything Weiwei mentioned (chapter 3 of user guide), if
&gt; you really want to drill down to REST protocol you might want to apply this
&gt; patch and build ozone.
&gt;
&gt; https://issues.apache.org/jira/browse/HDFS-12690
&gt;
&gt; This will generate an Open API (https://www.openapis.org ,
&gt; http://swagger.io) based specification which can be accessed from KSM UI
&gt; or just as a json file.
&gt; Unfortunately, this patch is still at code review stage, so you will have
&gt; to apply the patch and build it yourself.
&gt;
&gt; Thanks
&gt; Anu
&gt;
&gt;
&gt; On 10/20/17, 6:09 AM, "Yang Weiwei" &lt;cheersyang@hotmail.com&gt; wrote:
&gt;
&gt;     Hi Steve
&gt;
&gt;
&gt;     The code is available in HDFS-7240 feature branch, public git repo
&gt; here&lt;https://github.com/apache/hadoop/tree/HDFS-7240&gt;.
&gt;
&gt;     I am not sure if there is a "public" API for object stores, but the
&gt; design doc&lt;https://issues.apache.org/jira/secure/attachment/
&gt; 12799549/ozone_user_v0.pdf&gt; uses most common syntax so I believe it
&gt; should be compliance. You can find the rest API doc here&lt;
&gt; https://github.com/apache/hadoop/blob/HDFS-7240/
&gt; hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/OzoneRest.md&gt; (with
&gt; some example usages), and commandline API here&lt;https://github.com/
&gt; apache/hadoop/blob/HDFS-7240/hadoop-hdfs-project/hadoop-
&gt; hdfs/src/site/markdown/OzoneCommandShell.md&gt;.
&gt;
&gt;
&gt;     Look forward for your feedback!
&gt;
&gt;
&gt;     --Weiwei
&gt;
&gt;
&gt;     ________________________________
&gt;     发件人: Steve Loughran &lt;stevel@hortonworks.com&gt;
&gt;     发送时间: 2017年10月20日 11:49
&gt;     收件人: Yang Weiwei
&gt;     抄送: hdfs-dev@hadoop.apache.org; mapreduce-dev@hadoop.apache.org;
&gt; yarn-dev@hadoop.apache.org; common-dev@hadoop.apache.org
&gt;     主题: Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
&gt;
&gt;
&gt;     Wow, big piece of work
&gt;
&gt;     1. Where is a PR/branch on github with rendered docs for us to look at?
&gt;     2. Have you made any public APi changes related to object stores?
&gt; That's probably something I'll have opinions on more than implementation
&gt; details.
&gt;
&gt;     thanks
&gt;
&gt;     &gt; On 19 Oct 2017, at 02:54, Yang Weiwei &lt;cheersyang@hotmail.com&gt;
&gt; wrote:
&gt;     &gt;
&gt;     &gt; Hello everyone,
&gt;     &gt;
&gt;     &gt;
&gt;     &gt; I would like to start this thread to discuss merging Ozone
&gt; (HDFS-7240) to trunk. This feature implements an object store which can
&gt; co-exist with HDFS. Ozone is disabled by default. We have tested Ozone with
&gt; cluster sizes varying from 1 to 100 data nodes.
&gt;     &gt;
&gt;     &gt;
&gt;     &gt;
&gt;     &gt; The merge payload includes the following:
&gt;     &gt;
&gt;     &gt;  1.  All services, management scripts
&gt;     &gt;  2.  Object store APIs, exposed via both REST and RPC
&gt;     &gt;  3.  Master service UIs, command line interfaces
&gt;     &gt;  4.  Pluggable pipeline Integration
&gt;     &gt;  5.  Ozone File System (Hadoop compatible file system
&gt; implementation, passes all FileSystem contract tests)
&gt;     &gt;  6.  Corona - a load generator for Ozone.
&gt;     &gt;  7.  Essential documentation added to Hadoop site.
&gt;     &gt;  8.  Version specific Ozone Documentation, accessible via service UI.
&gt;     &gt;  9.  Docker support for ozone, which enables faster development
&gt; cycles.
&gt;     &gt;
&gt;     &gt;
&gt;     &gt; To build Ozone and run ozone using docker, please follow
&gt; instructions in this wiki page. https://cwiki.apache.org/
&gt; confluence/display/HADOOP/Dev+cluster+with+docker.
&gt;     Dev cluster with docker - Hadoop - Apache Software Foundation&lt;
&gt; https://cwiki.apache.org/confluence/display/HADOOP/Dev+cluster+with+docker
&gt; &gt;
&gt;     cwiki.apache.org
&gt;     First, it uses a much more smaller common image which doesn't contains
&gt; Hadoop. Second, the real Hadoop should be built from the source and the
&gt; dist director should be ...
&gt;
&gt;
&gt;
&gt;     &gt;
&gt;     &gt;
&gt;     &gt; We have built a passionate and diverse community to drive this
&gt; feature development. As a team, we have achieved significant progress in
&gt; past 3 years since first JIRA for HDFS-7240 was opened on Oct 2014. So far,
&gt; we have resolved almost 400 JIRAs by 20+ contributors/committers from
&gt; different countries and affiliations. We also want to thank the large
&gt; number of community members who were supportive of our efforts and
&gt; contributed ideas and participated in the design of ozone.
&gt;     &gt;
&gt;     &gt;
&gt;     &gt; Please share your thoughts, thanks!
&gt;     &gt;
&gt;     &gt;
&gt;     &gt; -- Weiwei Yang
&gt;
&gt;
&gt;
&gt;
&gt; ---------------------------------------------------------------------
&gt; To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
&gt; For additional commands, e-mail: common-dev-help@hadoop.apache.org
&gt;


</body></email><email><emailId>20171020214008</emailId><senderName>larry mccay</senderName><senderEmail>lmccay@apache.org</senderEmail><timestampReceived>2017-10-20 21:40:08-0400</timestampReceived><subject>=?UTF-8?B?UmU6IOetlOWkjTogW0RJU0NVU1NJT05dIE1lcmdpbmcgSERGUy03MjQwIE9iamVjdCBTdA==?= =?UTF-8?B?b3JlI</subject><body>


All -

I broke this list of questions out into a separate DISCUSS thread where we
can iterate over how a security audit process at merge time might look and
whether it is even something that we want to take on.

I will try and continue discussion on that thread and drive that to some
conclusion before bringing it into any particular merge discussion.

thanks,

--larry

On Fri, Oct 20, 2017 at 12:37 PM, larry mccay &lt;lmccay@apache.org&gt; wrote:

&gt; I previously sent this same email from my work email and it doesn't seem
&gt; to have gone through - resending from apache account (apologizing up from
&gt; for the length)....
&gt;
&gt; For such sizable merges in Hadoop, I would like to start doing security
&gt; audits in order to have an initial idea of the attack surface, the
&gt; protections available for known threats, what sort of configuration is
&gt; being used to launch processes, etc.
&gt;
&gt; I dug into the architecture documents while in the middle of this list -
&gt; nice docs!
&gt; I do intend to try and make a generic check list like this for such
&gt; security audits in the future so a lot of this is from that but I tried to
&gt; also direct specific questions from those docs as well.
&gt;
&gt; 1. UIs
&gt; I see there are at least two UIs - Storage Container Manager and Key Space
&gt; Manager. There are a number of typical vulnerabilities that we find in UIs
&gt;
&gt; 1.1. What sort of validation is being done on any accepted user input?
&gt; (pointers to code would be appreciated)
&gt; 1.2. What explicit protections have been built in for (pointers to code
&gt; would be appreciated):
&gt;   1.2.1. cross site scripting
&gt;   1.2.2. cross site request forgery
&gt;   1.2.3. click jacking (X-Frame-Options)
&gt; 1.3. What sort of authentication is required for access to the UIs?
&gt; 1.4. What authorization is available for determining who can access what
&gt; capabilities of the UIs for either viewing, modifying data or affecting
&gt; object stores and related processes?
&gt; 1.5. Are the UIs built with proxying in mind by leveraging X-Forwarded
&gt; headers?
&gt; 1.6. Is there any input that will ultimately be persisted in configuration
&gt; for executing shell commands or processes?
&gt; 1.7. Do the UIs support the trusted proxy pattern with doas impersonation?
&gt; 1.8. Is there TLS/SSL support?
&gt;
&gt; 2. REST APIs
&gt;
&gt; 2.1. Do the REST APIs support the trusted proxy pattern with doas
&gt; impersonation capabilities?
&gt; 2.2. What explicit protections have been built in for:
&gt;   2.2.1. cross site scripting (XSS)
&gt;   2.2.2. cross site request forgery (CSRF)
&gt;   2.2.3. XML External Entity (XXE)
&gt; 2.3. What is being used for authentication - Hadoop Auth Module?
&gt; 2.4. Are there separate processes for the HTTP resources (UIs and REST
&gt; endpoints) or are the part of existing HDFS processes?
&gt; 2.5. Is there TLS/SSL support?
&gt; 2.6. Are there new CLI commands and/or clients for access the REST APIs?
&gt; 2.7. Bucket Level API allows for setting of ACLs on a bucket - what
&gt; authorization is required here - is there a restrictive ACL set on creation?
&gt; 2.8. Bucket Level API allows for deleting a bucket - I assume this is
&gt; dependent on ACLs based access control?
&gt; 2.9. Bucket Level API to list bucket returns up to 1000 keys - is there
&gt; paging available?
&gt; 2.10. Storage Level APIs indicate "Signed with User Authorization" what
&gt; does this refer to exactly?
&gt; 2.11. Object Level APIs indicate that there is no ACL support and only
&gt; bucket owners can read and write - but there are ACL APIs on the Bucket
&gt; Level are they meaningless for now?
&gt; 2.12. How does a REST client know which Ozone Handler to connect to or am
&gt; I missing some well known NN type endpoint in the architecture doc
&gt; somewhere?
&gt;
&gt; 3. Encryption
&gt;
&gt; 3.1. Is there any support for encryption of persisted data?
&gt; 3.2. If so, is KMS and the hadoop key command used for key management?
&gt;
&gt; 4. Configuration
&gt;
&gt; 4.1. Are there any passwords or secrets being added to configuration?
&gt; 4.2. If so, are they accessed via Configuration.getPassword() to allow for
&gt; provisioning in credential providers?
&gt; 4.3. Are there any settings that are used to launch docker containers or
&gt; shell out any commands, etc?
&gt;
&gt; 5. HA
&gt;
&gt; 5.1. Are there provisions for HA?
&gt; 5.2. Are we leveraging the existing HA capabilities in HDFS?
&gt; 5.3. Is Storage Container Manager a SPOF?
&gt; 5.4. I see HA listed in future work in the architecture doc - is this
&gt; still an open issue?
&gt;
&gt; On Fri, Oct 20, 2017 at 11:19 AM, Anu Engineer &lt;aengineer@hortonworks.com&gt;
&gt; wrote:
&gt;
&gt;&gt; Hi Steve,
&gt;&gt;
&gt;&gt; In addition to everything Weiwei mentioned (chapter 3 of user guide), if
&gt;&gt; you really want to drill down to REST protocol you might want to apply this
&gt;&gt; patch and build ozone.
&gt;&gt;
&gt;&gt; https://issues.apache.org/jira/browse/HDFS-12690
&gt;&gt;
&gt;&gt; This will generate an Open API (https://www.openapis.org ,
&gt;&gt; http://swagger.io) based specification which can be accessed from KSM UI
&gt;&gt; or just as a json file.
&gt;&gt; Unfortunately, this patch is still at code review stage, so you will have
&gt;&gt; to apply the patch and build it yourself.
&gt;&gt;
&gt;&gt; Thanks
&gt;&gt; Anu
&gt;&gt;
&gt;&gt;
&gt;&gt; On 10/20/17, 6:09 AM, "Yang Weiwei" &lt;cheersyang@hotmail.com&gt; wrote:
&gt;&gt;
&gt;&gt;     Hi Steve
&gt;&gt;
&gt;&gt;
&gt;&gt;     The code is available in HDFS-7240 feature branch, public git repo
&gt;&gt; here&lt;https://github.com/apache/hadoop/tree/HDFS-7240&gt;.
&gt;&gt;
&gt;&gt;     I am not sure if there is a "public" API for object stores, but the
&gt;&gt; design doc&lt;https://issues.apache.org/jira/secure/attachment/1279954
&gt;&gt; 9/ozone_user_v0.pdf&gt; uses most common syntax so I believe it should be
&gt;&gt; compliance. You can find the rest API doc here&lt;https://github.com/apache
&gt;&gt; /hadoop/blob/HDFS-7240/hadoop-hdfs-project/hadoop-hdfs/src/
&gt;&gt; site/markdown/OzoneRest.md&gt; (with some example usages), and commandline
&gt;&gt; API here&lt;https://github.com/apache/hadoop/blob/HDFS-7240/hadoop-
&gt;&gt; hdfs-project/hadoop-hdfs/src/site/markdown/OzoneCommandShell.md&gt;.
&gt;&gt;
&gt;&gt;
&gt;&gt;     Look forward for your feedback!
&gt;&gt;
&gt;&gt;
&gt;&gt;     --Weiwei
&gt;&gt;
&gt;&gt;
&gt;&gt;     ________________________________
&gt;&gt;     发件人: Steve Loughran &lt;stevel@hortonworks.com&gt;
&gt;&gt;     发送时间: 2017年10月20日 11:49
&gt;&gt;     收件人: Yang Weiwei
&gt;&gt;     抄送: hdfs-dev@hadoop.apache.org; mapreduce-dev@hadoop.apache.org;
&gt;&gt; yarn-dev@hadoop.apache.org; common-dev@hadoop.apache.org
&gt;&gt;     主题: Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
&gt;&gt;
&gt;&gt;
&gt;&gt;     Wow, big piece of work
&gt;&gt;
&gt;&gt;     1. Where is a PR/branch on github with rendered docs for us to look
&gt;&gt; at?
&gt;&gt;     2. Have you made any public APi changes related to object stores?
&gt;&gt; That's probably something I'll have opinions on more than implementation
&gt;&gt; details.
&gt;&gt;
&gt;&gt;     thanks
&gt;&gt;
&gt;&gt;     &gt; On 19 Oct 2017, at 02:54, Yang Weiwei &lt;cheersyang@hotmail.com&gt;
&gt;&gt; wrote:
&gt;&gt;     &gt;
&gt;&gt;     &gt; Hello everyone,
&gt;&gt;     &gt;
&gt;&gt;     &gt;
&gt;&gt;     &gt; I would like to start this thread to discuss merging Ozone
&gt;&gt; (HDFS-7240) to trunk. This feature implements an object store which can
&gt;&gt; co-exist with HDFS. Ozone is disabled by default. We have tested Ozone with
&gt;&gt; cluster sizes varying from 1 to 100 data nodes.
&gt;&gt;     &gt;
&gt;&gt;     &gt;
&gt;&gt;     &gt;
&gt;&gt;     &gt; The merge payload includes the following:
&gt;&gt;     &gt;
&gt;&gt;     &gt;  1.  All services, management scripts
&gt;&gt;     &gt;  2.  Object store APIs, exposed via both REST and RPC
&gt;&gt;     &gt;  3.  Master service UIs, command line interfaces
&gt;&gt;     &gt;  4.  Pluggable pipeline Integration
&gt;&gt;     &gt;  5.  Ozone File System (Hadoop compatible file system
&gt;&gt; implementation, passes all FileSystem contract tests)
&gt;&gt;     &gt;  6.  Corona - a load generator for Ozone.
&gt;&gt;     &gt;  7.  Essential documentation added to Hadoop site.
&gt;&gt;     &gt;  8.  Version specific Ozone Documentation, accessible via service
&gt;&gt; UI.
&gt;&gt;     &gt;  9.  Docker support for ozone, which enables faster development
&gt;&gt; cycles.
&gt;&gt;     &gt;
&gt;&gt;     &gt;
&gt;&gt;     &gt; To build Ozone and run ozone using docker, please follow
&gt;&gt; instructions in this wiki page. https://cwiki.apache.org/confl
&gt;&gt; uence/display/HADOOP/Dev+cluster+with+docker.
&gt;&gt;     Dev cluster with docker - Hadoop - Apache Software Foundation&lt;
&gt;&gt; https://cwiki.apache.org/confluence/display/HADOO
&gt;&gt; P/Dev+cluster+with+docker&gt;
&gt;&gt;     cwiki.apache.org
&gt;&gt;     First, it uses a much more smaller common image which doesn't
&gt;&gt; contains Hadoop. Second, the real Hadoop should be built from the source
&gt;&gt; and the dist director should be ...
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;     &gt;
&gt;&gt;     &gt;
&gt;&gt;     &gt; We have built a passionate and diverse community to drive this
&gt;&gt; feature development. As a team, we have achieved significant progress in
&gt;&gt; past 3 years since first JIRA for HDFS-7240 was opened on Oct 2014. So far,
&gt;&gt; we have resolved almost 400 JIRAs by 20+ contributors/committers from
&gt;&gt; different countries and affiliations. We also want to thank the large
&gt;&gt; number of community members who were supportive of our efforts and
&gt;&gt; contributed ideas and participated in the design of ozone.
&gt;&gt;     &gt;
&gt;&gt;     &gt;
&gt;&gt;     &gt; Please share your thoughts, thanks!
&gt;&gt;     &gt;
&gt;&gt;     &gt;
&gt;&gt;     &gt; -- Weiwei Yang
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; ---------------------------------------------------------------------
&gt;&gt; To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
&gt;&gt; For additional commands, e-mail: common-dev-help@hadoop.apache.org
&gt;&gt;
&gt;
&gt;


</body></email><email><emailId>20171103062927</emailId><senderName>Lei Xu</senderName><senderEmail>lei@cloudera.com</senderEmail><timestampReceived>2017-11-03 06:29:27-0400</timestampReceived><subject>=?UTF-8?B?UmU6IOetlOWkjTogW0RJU0NVU1NJT05dIE1lcmdpbmcgSERGUy03MjQwIE9iamVjdCBTdA==?= =?UTF-8?B?b3JlI</subject><body>

Hey,  Weiwei and Jitendra

Thanks a lot for this large effort to bring us ozone.

* As the current state of Ozone implementation, what are the major
benefits of using today's Ozone over HDFS?  Giving that its missing
features like HDFS-12680 and HDFS-12697, being disabled by default,
and the closing of Hadoop 3.0 release, should we wait for a late merge
when Ozone is more mature ? Or more generally, why should this merge
to a release branch happen now, when Ozone is not yet usable by users?
Staying on a feature branch seems like it's still the right place to
me.
* For the existing HDFS user, could you address the semantic gaps
between Ozone / Ozone File System and HDFS. It would be great to
illustrate what is the expected use cases for Ozone giving its
different architecture and design decisions?  Like no append, no
atomic rename and etc.
* A follow question, was it able to run any of today's Hadoop
applications (MR, Spark, Impala, Presto and etc) on Ozone directly, or
against OZoneFileSystem? I think a performance / scalability gain or
extended functionality should be the prerequisites for the merge.
Additionally, I believe such tests will reveal the potential caveats
if any.
* Ozone's architecture shows great potential to address NN
scalability.  However it looks like a XXL effort to me, considering
the fact that 1) the community had multiple unfinished attempts to
simply separate namespace and block management within the same NN
process, and 2) many existing features like snapshot, append, erasure
coding, and etc, are not straightforward to be implemented in today's
ozone design. Could you share your opinions on this matter?
* How stable is the ozone client? Should we mark them as unstable for
now? Also giving the significant difference between OzoneClient and
HdfsClient, should move it to a separated package or even a project? I
second Konstantin's option to separate ozone from HDFS.
* Please add sections to the end-user and system admin oriented
documents for deploying and operating SCM, KSM, and also the chunk
servers on DataNodes. Additionally, the introduction in
"OZoneGettingStarted.md" is still building ozone from feature branch
HDFS-7240.

Best regards,

On Mon, Oct 23, 2017 at 11:10 AM, Jitendra Pandey
&lt;jitendra@hortonworks.com&gt; wrote:
&gt; I have filed https://issues.apache.org/jira/browse/HDFS-12697 to ensure ozone stays \
&gt; disabled in a secure environment. Since ozone is disabled by default and will not \
&gt; come with security on, it will not expose any new attack surface in a Hadoop \
&gt; deployment. Ozone security effort will need a detailed design and discussion on a \
&gt; community jira. Hopefully, that effort will start soon after the merge. 
&gt; Thanks
&gt; jitendra
&gt; 
&gt; On 10/20/17, 2:40 PM, "larry mccay" &lt;lmccay@apache.org&gt; wrote:
&gt; 
&gt; All -
&gt; 
&gt; I broke this list of questions out into a separate DISCUSS thread where we
&gt; can iterate over how a security audit process at merge time might look and
&gt; whether it is even something that we want to take on.
&gt; 
&gt; I will try and continue discussion on that thread and drive that to some
&gt; conclusion before bringing it into any particular merge discussion.
&gt; 
&gt; thanks,
&gt; 
&gt; --larry
&gt; 
&gt; On Fri, Oct 20, 2017 at 12:37 PM, larry mccay &lt;lmccay@apache.org&gt; wrote:
&gt; 
&gt; &gt; I previously sent this same email from my work email and it doesn't seem
&gt; &gt; to have gone through - resending from apache account (apologizing up from
&gt; &gt; for the length)....
&gt; &gt; 
&gt; &gt; For such sizable merges in Hadoop, I would like to start doing security
&gt; &gt; audits in order to have an initial idea of the attack surface, the
&gt; &gt; protections available for known threats, what sort of configuration is
&gt; &gt; being used to launch processes, etc.
&gt; &gt; 
&gt; &gt; I dug into the architecture documents while in the middle of this list -
&gt; &gt; nice docs!
&gt; &gt; I do intend to try and make a generic check list like this for such
&gt; &gt; security audits in the future so a lot of this is from that but I tried to
&gt; &gt; also direct specific questions from those docs as well.
&gt; &gt; 
&gt; &gt; 1. UIs
&gt; &gt; I see there are at least two UIs - Storage Container Manager and Key Space
&gt; &gt; Manager. There are a number of typical vulnerabilities that we find in UIs
&gt; &gt; 
&gt; &gt; 1.1. What sort of validation is being done on any accepted user input?
&gt; &gt; (pointers to code would be appreciated)
&gt; &gt; 1.2. What explicit protections have been built in for (pointers to code
&gt; &gt; would be appreciated):
&gt; &gt; 1.2.1. cross site scripting
&gt; &gt; 1.2.2. cross site request forgery
&gt; &gt; 1.2.3. click jacking (X-Frame-Options)
&gt; &gt; 1.3. What sort of authentication is required for access to the UIs?
&gt; &gt; 1.4. What authorization is available for determining who can access what
&gt; &gt; capabilities of the UIs for either viewing, modifying data or affecting
&gt; &gt; object stores and related processes?
&gt; &gt; 1.5. Are the UIs built with proxying in mind by leveraging X-Forwarded
&gt; &gt; headers?
&gt; &gt; 1.6. Is there any input that will ultimately be persisted in configuration
&gt; &gt; for executing shell commands or processes?
&gt; &gt; 1.7. Do the UIs support the trusted proxy pattern with doas impersonation?
&gt; &gt; 1.8. Is there TLS/SSL support?
&gt; &gt; 
&gt; &gt; 2. REST APIs
&gt; &gt; 
&gt; &gt; 2.1. Do the REST APIs support the trusted proxy pattern with doas
&gt; &gt; impersonation capabilities?
&gt; &gt; 2.2. What explicit protections have been built in for:
&gt; &gt; 2.2.1. cross site scripting (XSS)
&gt; &gt; 2.2.2. cross site request forgery (CSRF)
&gt; &gt; 2.2.3. XML External Entity (XXE)
&gt; &gt; 2.3. What is being used for authentication - Hadoop Auth Module?
&gt; &gt; 2.4. Are there separate processes for the HTTP resources (UIs and REST
&gt; &gt; endpoints) or are the part of existing HDFS processes?
&gt; &gt; 2.5. Is there TLS/SSL support?
&gt; &gt; 2.6. Are there new CLI commands and/or clients for access the REST APIs?
&gt; &gt; 2.7. Bucket Level API allows for setting of ACLs on a bucket - what
&gt; &gt; authorization is required here - is there a restrictive ACL set on creation?
&gt; &gt; 2.8. Bucket Level API allows for deleting a bucket - I assume this is
&gt; &gt; dependent on ACLs based access control?
&gt; &gt; 2.9. Bucket Level API to list bucket returns up to 1000 keys - is there
&gt; &gt; paging available?
&gt; &gt; 2.10. Storage Level APIs indicate "Signed with User Authorization" what
&gt; &gt; does this refer to exactly?
&gt; &gt; 2.11. Object Level APIs indicate that there is no ACL support and only
&gt; &gt; bucket owners can read and write - but there are ACL APIs on the Bucket
&gt; &gt; Level are they meaningless for now?
&gt; &gt; 2.12. How does a REST client know which Ozone Handler to connect to or am
&gt; &gt; I missing some well known NN type endpoint in the architecture doc
&gt; &gt; somewhere?
&gt; &gt; 
&gt; &gt; 3. Encryption
&gt; &gt; 
&gt; &gt; 3.1. Is there any support for encryption of persisted data?
&gt; &gt; 3.2. If so, is KMS and the hadoop key command used for key management?
&gt; &gt; 
&gt; &gt; 4. Configuration
&gt; &gt; 
&gt; &gt; 4.1. Are there any passwords or secrets being added to configuration?
&gt; &gt; 4.2. If so, are they accessed via Configuration.getPassword() to allow for
&gt; &gt; provisioning in credential providers?
&gt; &gt; 4.3. Are there any settings that are used to launch docker containers or
&gt; &gt; shell out any commands, etc?
&gt; &gt; 
&gt; &gt; 5. HA
&gt; &gt; 
&gt; &gt; 5.1. Are there provisions for HA?
&gt; &gt; 5.2. Are we leveraging the existing HA capabilities in HDFS?
&gt; &gt; 5.3. Is Storage Container Manager a SPOF?
&gt; &gt; 5.4. I see HA listed in future work in the architecture doc - is this
&gt; &gt; still an open issue?
&gt; &gt; 
&gt; &gt; On Fri, Oct 20, 2017 at 11:19 AM, Anu Engineer &lt;aengineer@hortonworks.com&gt;
&gt; &gt; wrote:
&gt; &gt; 
&gt; &gt; &gt; Hi Steve,
&gt; &gt; &gt; 
&gt; &gt; &gt; In addition to everything Weiwei mentioned (chapter 3 of user guide), if
&gt; &gt; &gt; you really want to drill down to REST protocol you might want to apply this
&gt; &gt; &gt; patch and build ozone.
&gt; &gt; &gt; 
&gt; &gt; &gt; https://issues.apache.org/jira/browse/HDFS-12690
&gt; &gt; &gt; 
&gt; &gt; &gt; This will generate an Open API (https://www.openapis.org ,
&gt; &gt; &gt; http://swagger.io) based specification which can be accessed from KSM UI
&gt; &gt; &gt; or just as a json file.
&gt; &gt; &gt; Unfortunately, this patch is still at code review stage, so you will have
&gt; &gt; &gt; to apply the patch and build it yourself.
&gt; &gt; &gt; 
&gt; &gt; &gt; Thanks
&gt; &gt; &gt; Anu
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; On 10/20/17, 6:09 AM, "Yang Weiwei" &lt;cheersyang@hotmail.com&gt; wrote:
&gt; &gt; &gt; 
&gt; &gt; &gt; Hi Steve
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; The code is available in HDFS-7240 feature branch, public git repo
&gt; &gt; &gt; here&lt;https://github.com/apache/hadoop/tree/HDFS-7240&gt;.
&gt; &gt; &gt; 
&gt; &gt; &gt; I am not sure if there is a "public" API for object stores, but the
&gt; &gt; &gt; design doc&lt;https://issues.apache.org/jira/secure/attachment/1279954
&gt; &gt; &gt; 9/ozone_user_v0.pdf&gt; uses most common syntax so I believe it should be
&gt; &gt; &gt; compliance. You can find the rest API doc here&lt;https://github.com/apache
&gt; &gt; &gt; /hadoop/blob/HDFS-7240/hadoop-hdfs-project/hadoop-hdfs/src/
&gt; &gt; &gt; site/markdown/OzoneRest.md&gt; (with some example usages), and commandline
&gt; &gt; &gt; API here&lt;https://github.com/apache/hadoop/blob/HDFS-7240/hadoop-
&gt; &gt; &gt; hdfs-project/hadoop-hdfs/src/site/markdown/OzoneCommandShell.md&gt;.
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; Look forward for your feedback!
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; --Weiwei
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; ________________________________
&gt; &gt; &gt; 发件人: Steve Loughran &lt;stevel@hortonworks.com&gt;
&gt; &gt; &gt; 发送时间: 2017年10月20日 11:49
&gt; &gt; &gt; 收件人: Yang Weiwei
&gt; &gt; &gt; 抄送: hdfs-dev@hadoop.apache.org; mapreduce-dev@hadoop.apache.org;
&gt; &gt; &gt; yarn-dev@hadoop.apache.org; common-dev@hadoop.apache.org
&gt; &gt; &gt; 主题: Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; Wow, big piece of work
&gt; &gt; &gt; 
&gt; &gt; &gt; 1. Where is a PR/branch on github with rendered docs for us to look
&gt; &gt; &gt; at?
&gt; &gt; &gt; 2. Have you made any public APi changes related to object stores?
&gt; &gt; &gt; That's probably something I'll have opinions on more than implementation
&gt; &gt; &gt; details.
&gt; &gt; &gt; 
&gt; &gt; &gt; thanks
&gt; &gt; &gt; 
&gt; &gt; &gt; &gt; On 19 Oct 2017, at 02:54, Yang Weiwei &lt;cheersyang@hotmail.com&gt;
&gt; &gt; &gt; wrote:
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; Hello everyone,
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; I would like to start this thread to discuss merging Ozone
&gt; &gt; &gt; (HDFS-7240) to trunk. This feature implements an object store which can
&gt; &gt; &gt; co-exist with HDFS. Ozone is disabled by default. We have tested Ozone with
&gt; &gt; &gt; cluster sizes varying from 1 to 100 data nodes.
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; The merge payload includes the following:
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 1.  All services, management scripts
&gt; &gt; &gt; &gt; 2.  Object store APIs, exposed via both REST and RPC
&gt; &gt; &gt; &gt; 3.  Master service UIs, command line interfaces
&gt; &gt; &gt; &gt; 4.  Pluggable pipeline Integration
&gt; &gt; &gt; &gt; 5.  Ozone File System (Hadoop compatible file system
&gt; &gt; &gt; implementation, passes all FileSystem contract tests)
&gt; &gt; &gt; &gt; 6.  Corona - a load generator for Ozone.
&gt; &gt; &gt; &gt; 7.  Essential documentation added to Hadoop site.
&gt; &gt; &gt; &gt; 8.  Version specific Ozone Documentation, accessible via service
&gt; &gt; &gt; UI.
&gt; &gt; &gt; &gt; 9.  Docker support for ozone, which enables faster development
&gt; &gt; &gt; cycles.
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; To build Ozone and run ozone using docker, please follow
&gt; &gt; &gt; instructions in this wiki page. https://cwiki.apache.org/confl
&gt; &gt; &gt; uence/display/HADOOP/Dev+cluster+with+docker.
&gt; &gt; &gt; Dev cluster with docker - Hadoop - Apache Software Foundation&lt;
&gt; &gt; &gt; https://cwiki.apache.org/confluence/display/HADOO
&gt; &gt; &gt; P/Dev+cluster+with+docker&gt;
&gt; &gt; &gt; cwiki.apache.org
&gt; &gt; &gt; First, it uses a much more smaller common image which doesn't
&gt; &gt; &gt; contains Hadoop. Second, the real Hadoop should be built from the source
&gt; &gt; &gt; and the dist director should be ...
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; We have built a passionate and diverse community to drive this
&gt; &gt; &gt; feature development. As a team, we have achieved significant progress in
&gt; &gt; &gt; past 3 years since first JIRA for HDFS-7240 was opened on Oct 2014. So far,
&gt; &gt; &gt; we have resolved almost 400 JIRAs by 20+ contributors/committers from
&gt; &gt; &gt; different countries and affiliations. We also want to thank the large
&gt; &gt; &gt; number of community members who were supportive of our efforts and
&gt; &gt; &gt; contributed ideas and participated in the design of ozone.
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; Please share your thoughts, thanks!
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; -- Weiwei Yang
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; ---------------------------------------------------------------------
&gt; &gt; &gt; To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
&gt; &gt; &gt; For additional commands, e-mail: common-dev-help@hadoop.apache.org
&gt; &gt; &gt; 
&gt; &gt; 
&gt; &gt; 
&gt; 
&gt; 



-- 
Lei (Eddy) Xu
Software Engineer, Cloudera

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170601042304</emailId><senderName>"Akira Ajisaka (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-01 04:23:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14474) Use OpenJDK 7 instead of Oracle JDK 7 to avoid oracle-java7-installe</subject><body>

Akira Ajisaka created HADOOP-14474:
--------------------------------------

             Summary: Use OpenJDK 7 instead of Oracle JDK 7 to avoid \
oracle-java7-installer failures  Key: HADOOP-14474
                 URL: https://issues.apache.org/jira/browse/HADOOP-14474
             Project: Hadoop Common
          Issue Type: Bug
          Components: build
    Affects Versions: 2.7.3, 2.8.0
            Reporter: Akira Ajisaka


Recently Oracle has changed the download link for Oracle JDK7, and that's why \
oracle-java7-installer fails. Precommit jobs for branch-2* are failing because of \
this failure.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170601133804</emailId><senderName>"Yonger (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-01 13:38:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14475) Metrics of S3A doesn't print out when enable it in Hadoop metrics pr</subject><body>

Yonger created HADOOP-14475:
-------------------------------

             Summary: Metrics of S3A doesn't print out  when enable it in Hadoop \
metrics property file  Key: HADOOP-14475
                 URL: https://issues.apache.org/jira/browse/HADOOP-14475
             Project: Hadoop Common
          Issue Type: Bug
          Components: s3
    Affects Versions: 2.8.0
         Environment: uname -a
Linux client01 4.4.0-74-generic #95-Ubuntu SMP Wed Apr 12 09:50:34 UTC 2017 x86_64 \
x86_64 x86_64 GNU/Linux

 cat /etc/issue
Ubuntu 16.04.2 LTS \n \l
            Reporter: Yonger


*.sink.file.class=org.apache.hadoop.metrics2.sink.FileSink
#*.sink.file.class=org.apache.hadoop.metrics2.sink.influxdb.InfluxdbSink
#*.sink.influxdb.url=http:/xxxxxxxxxx
#*.sink.influxdb.influxdb_port=8086
#*.sink.influxdb.database=hadoop
#*.sink.influxdb.influxdb_username=hadoop
#*.sink.influxdb.influxdb_password=hadoop
#*.sink.ingluxdb.cluster=c1
# default sampling period, in seconds
*.period=10
#namenode.sink.influxdb.class=org.apache.hadoop.metrics2.sink.influxdb.InfluxdbSink
#S3AFileSystem.sink.influxdb.class=org.apache.hadoop.metrics2.sink.influxdb.InfluxdbSink
 S3AFileSystem.sink.file.filename=s3afilesystem-metrics.out

I can't find the out put file even i run a MR job which should be used s3.




--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170901021503</emailId><senderName>"Allen Wittenauer (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-01 02:15:03-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14817) shelldocs fails mvn site</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14817?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Allen Wittenauer resolved HADOOP-14817.
---------------------------------------
    Resolution: Fixed

&gt; shelldocs fails mvn site
&gt; ------------------------
&gt; 
&gt; Key: HADOOP-14817
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14817
&gt; Project: Hadoop Common
&gt; Issue Type: Bug
&gt; Components: build, documentation
&gt; Affects Versions: 3.0.0-beta1
&gt; Reporter: Allen Wittenauer
&gt; Assignee: Allen Wittenauer
&gt; Priority: Blocker
&gt; 
&gt; When exec-maven-plugin calls Apache Yetus 0.5.0 shelldocs, it fails:
&gt; {code}
&gt; [INFO] 
&gt; [INFO] --- exec-maven-plugin:1.3.1:exec (shelldocs) @ hadoop-common ---
&gt; /usr/bin/env: python -B: No such file or directory
&gt; [INFO] ------------------------------------------------------------------------
&gt; [INFO] BUILD FAILURE
&gt; {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170901022551</emailId><senderName>Ping Liu</senderName><senderEmail>pingpinganan@gmail.com</senderEmail><timestampReceived>2017-09-01 02:25:51-0400</timestampReceived><subject>Re: native folder not found in hadoop-common build on Ubuntu</subject><body>


Thanks Allen for the information.  No wonder I got it on Windows
automatically.

By the way, with John and Ravi's help, I was able to have it work now by
installing CMake as well as Zlib (I already had Protobuf installed before).

Thanks everyone!!

Ping



On Thu, Aug 31, 2017 at 7:03 PM, Allen Wittenauer &lt;aw@effectivemachines.com&gt;
wrote:

&gt;
&gt; Just to close the loop on this a bit ...
&gt;
&gt;         Windows always triggers the 'native-win' profile because winutils
&gt; is currently required to actually use Apache Hadoop on that platform.  On
&gt; other platforms, the 'native' profile is optional since their is enough
&gt; support in the JDK to at least do all the basic of tasks.  (It's still
&gt; HIGHLY recommended, however, that the native code get built though.)
&gt;
&gt;         It'd probably be a good project for someone to see if modern JDKs
&gt; (with or without additional dependencies) now have enough support to make
&gt; winutils optional.
&gt;
&gt;
&gt;
&gt; &gt; On Aug 31, 2017, at 4:14 PM, Ping Liu &lt;pingpinganan@gmail.com&gt; wrote:
&gt; &gt;
&gt; &gt; Hi Ravi, John,
&gt; &gt;
&gt; &gt; Thanks!  Yeah, it's the first profile.  Now as I tried the build with
&gt; &gt; -Pnative, I saw the build failure.  It complains for cmake.
&gt; &gt;
&gt; &gt; It's also a requirement specified in BUILDING.txt that John pointed out.
&gt; &gt;
&gt; &gt; Thanks!!
&gt; &gt;
&gt; &gt; Ping
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt; On Thu, Aug 31, 2017 at 4:03 PM, Ravi Prakash &lt;ravihadoop@gmail.com&gt;
&gt; wrote:
&gt; &gt;
&gt; &gt;&gt; Please use -Pnative profile
&gt; &gt;&gt;
&gt; &gt;&gt; On Thu, Aug 31, 2017 at 3:53 PM, Ping Liu &lt;pingpinganan@gmail.com&gt;
&gt; wrote:
&gt; &gt;&gt;
&gt; &gt;&gt;&gt; Hi John,
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; Thank you for your quick response.
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; I used
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; mvn clean install -DskipTests
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; I just did a comparison with my Windows build result. winutils is
&gt; missing
&gt; &gt;&gt;&gt; too.
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; So both "native" and "winutils" folders are not generated in target
&gt; &gt;&gt;&gt; folder,
&gt; &gt;&gt;&gt; although it shows BUILD SUCCESS.
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; Thanks.
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; Ping
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; On Thu, Aug 31, 2017 at 3:36 PM, John Zhuge &lt;john.zhuge@gmail.com&gt;
&gt; wrote:
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt; Hi Ping,
&gt; &gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt; Thanks for using Hadoop. Linux is Unix-like. Hadoop supports native
&gt; code
&gt; &gt;&gt;&gt;&gt; on Linux. Please read BUILDING.txt in the root of the Hadoop source
&gt; &gt;&gt;&gt; tree.
&gt; &gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt; Could you provide the entire Maven command line when you built Hadoop?
&gt; &gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt; On Thu, Aug 31, 2017 at 3:06 PM, Ping Liu &lt;pingpinganan@gmail.com&gt;
&gt; &gt;&gt;&gt; wrote:
&gt; &gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt;&gt; I built hadoop-common on Ubuntu in my VirtualBox.  But in target
&gt; &gt;&gt;&gt; folder, I
&gt; &gt;&gt;&gt;&gt;&gt; didn't find "native" folder that is supposed to contain the generated
&gt; &gt;&gt;&gt; JNI
&gt; &gt;&gt;&gt;&gt;&gt; header files for C.  On my Windows, native folder is found in target.
&gt; &gt;&gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt;&gt; As I check the POM file, I found "native build only supported on Mac
&gt; or
&gt; &gt;&gt;&gt;&gt;&gt; Unix".  Does this mean native is not supported on Linux?
&gt; &gt;&gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt;&gt; Thanks!
&gt; &gt;&gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt;&gt; Ping
&gt; &gt;&gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt; --
&gt; &gt;&gt;&gt;&gt; John
&gt; &gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;
&gt; &gt;&gt;
&gt;
&gt;


</body></email><email><emailId>20170901133602</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-01 13:36:02-0400</timestampReceived><subject>=?utf-8?Q?[jira]_[Created]_(HADOOP-1482?= =?utf-8?Q?5)_=C3=9Cber-JIRA:_S3Guard_Phase_II?=</subject><body>

Steve Loughran created HADOOP-14825:
---------------------------------------

             Summary: =C3=9Cber-JIRA: S3Guard Phase II
                 Key: HADOOP-14825
                 URL: https://issues.apache.org/jira/browse/HADOOP-14825
             Project: Hadoop Common
          Issue Type: Improvement
          Components: fs/s3
    Affects Versions: 3.0.0-beta1
            Reporter: Steve Loughran
            Assignee: Steve Loughran


JIRA covering everything we want/need for a phase II of S3Guard

That includes

* features
* fixes
* Everything which was open under the HADOOP-13345 JIRA when S3Guard was co=
mmitted to trunk



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170303133027</emailId><senderName>Simon Scott</senderName><senderEmail>simon.scott@viavisolutions.com</senderEmail><timestampReceived>2017-03-03 13:30:27-0400</timestampReceived><subject>FsUrlStreamHandlerFactory and Windows file URLs</subject><body>


Apologies if this an old topic, however the following test program:

package com.viavi;

import org.apache.hadoop.fs.FsUrlStreamHandlerFactory;

import java.io.File;
import java.net.MalformedURLException;
import java.net.URI;
import java.net.URISyntaxException;
import java.net.URL;

public class Main {

    public static void main(String[] args) {
        System.out.println("URI is " + makeURI());
        URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());
        System.out.println("URI is " + makeURI());
    }

    private static URI makeURI() {
        try {
            File file = new File("C:/Users");
            final URL url = new URL("file:///" + file.getAbsolutePath());
            return url.toURI();
        } catch (MalformedURLException x) {
            x.printStackTrace();
            return null;
        } catch (URISyntaxException x) {
            x.printStackTrace();
            return null;
        }
    }
}

gives the following output:

URI is file:/C:/Users
URI is null
java.net.URISyntaxException: Illegal character in path at index 8: file:/C:\Users
                at java.net.URI$Parser.fail(URI.java:2848)
                at java.net.URI$Parser.checkChars(URI.java:3021)
                at java.net.URI$Parser.parseHierarchical(URI.java:3105)
                at java.net.URI$Parser.parse(URI.java:3053)
                at java.net.URI.&lt;init&gt;(URI.java:588)
                at java.net.URL.toURI(URL.java:946)
                at com.viavi.Main.makeURI(Main.java:23)
                at com.viavi.Main.main(Main.java:16)
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
                
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
                at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)


That is, it seems that registering the FsUrlStreamHandlerFactory disrupts the \
subsequent creation of Windows "file" URLs. I encountered this on 2.6.5, and have \
reproduced in 3.0.0 - alpha2.

I believe I have a fair understanding of the root of the issue. Is this of interest \
to anybody?

Thanks
Simon



</body></email><email><emailId>20170303214909</emailId><senderName>Steve Loughran</senderName><senderEmail>stevel@hortonworks.com</senderEmail><timestampReceived>2017-03-03 21:49:09-0400</timestampReceived><subject>Re: FsUrlStreamHandlerFactory and Windows file URLs</subject><body>

That stack trace is coming from the java code before it his Hadoop

&gt; On 3 Mar 2017, at 13:30, Simon Scott &lt;Simon.Scott@viavisolutions.com&gt; wrote:
&gt; 
&gt; Apologies if this an old topic, however the following test program:
&gt; 
&gt; package com.viavi;
&gt; 
&gt; import org.apache.hadoop.fs.FsUrlStreamHandlerFactory;
&gt; 
&gt; import java.io.File;
&gt; import java.net.MalformedURLException;
&gt; import java.net.URI;
&gt; import java.net.URISyntaxException;
&gt; import java.net.URL;
&gt; 
&gt; public class Main {
&gt; 
&gt; public static void main(String[] args) {
&gt; System.out.println("URI is " + makeURI());
&gt; URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());
&gt; System.out.println("URI is " + makeURI());
&gt; }
&gt; 
&gt; private static URI makeURI() {
&gt; try {
&gt; File file = new File("C:/Users");
&gt; final URL url = new URL("file:///" + file.getAbsolutePath());
&gt; return url.toURI();   ** HERE**


Ty creating the URI via File.toURI(); 




&gt; } catch (MalformedURLException x) {
&gt; x.printStackTrace();
&gt; return null;
&gt; } catch (URISyntaxException x) {
&gt; x.printStackTrace();
&gt; return null;
&gt; }
&gt; }
&gt; }
&gt; 
&gt; gives the following output:
&gt; 
&gt; URI is file:/C:/Users
&gt; URI is null
&gt; java.net.URISyntaxException: Illegal character in path at index 8: file:/C:\Users
&gt; at java.net.URI$Parser.fail(URI.java:2848)
&gt; at java.net.URI$Parser.checkChars(URI.java:3021)
&gt; at java.net.URI$Parser.parseHierarchical(URI.java:3105)
&gt; at java.net.URI$Parser.parse(URI.java:3053)
&gt; at java.net.URI.&lt;init&gt;(URI.java:588)
&gt; at java.net.URL.toURI(URL.java:946)
&gt; at com.viavi.Main.makeURI(Main.java:23)
&gt; at com.viavi.Main.main(Main.java:16)
&gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
&gt; at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
&gt; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
&gt;  at java.lang.reflect.Method.invoke(Method.java:498)
&gt; at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
&gt; 
&gt; That is, it seems that registering the FsUrlStreamHandlerFactory disrupts the \
&gt; subsequent creation of Windows "file" URLs. I encountered this on 2.6.5, and have \
&gt; reproduced in 3.0.0 - alpha2. 
&gt; I believe I have a fair understanding of the root of the issue. Is this of interest \
&gt; to anybody? 
&gt; Thanks
&gt; Simon
&gt; 


---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170306091043</emailId><senderName>Simon Scott</senderName><senderEmail>simon.scott@viavisolutions.com</senderEmail><timestampReceived>2017-03-06 09:10:43-0400</timestampReceived><subject>RE: FsUrlStreamHandlerFactory and Windows file URLs</subject><body>

Yes it is. But the registration of the FsUrlStreamHandlerFactory has surely \
influenced the JDK in some manner?

-----Original Message-----
From: Steve Loughran [mailto:stevel@hortonworks.com] 
Sent: 03 March 2017 21:49
Cc: common-dev@hadoop.apache.org
Subject: Re: FsUrlStreamHandlerFactory and Windows file URLs

That stack trace is coming from the java code before it his Hadoop

&gt; On 3 Mar 2017, at 13:30, Simon Scott &lt;Simon.Scott@viavisolutions.com&gt; wrote:
&gt; 
&gt; Apologies if this an old topic, however the following test program:
&gt; 
&gt; package com.viavi;
&gt; 
&gt; import org.apache.hadoop.fs.FsUrlStreamHandlerFactory;
&gt; 
&gt; import java.io.File;
&gt; import java.net.MalformedURLException; import java.net.URI; import 
&gt; java.net.URISyntaxException; import java.net.URL;
&gt; 
&gt; public class Main {
&gt; 
&gt; public static void main(String[] args) {
&gt; System.out.println("URI is " + makeURI());
&gt; URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());
&gt; System.out.println("URI is " + makeURI());
&gt; }
&gt; 
&gt; private static URI makeURI() {
&gt; try {
&gt; File file = new File("C:/Users");
&gt; final URL url = new URL("file:///" + file.getAbsolutePath());
&gt; return url.toURI();   ** HERE**


Ty creating the URI via File.toURI(); 




&gt; } catch (MalformedURLException x) {
&gt; x.printStackTrace();
&gt; return null;
&gt; } catch (URISyntaxException x) {
&gt; x.printStackTrace();
&gt; return null;
&gt; }
&gt; }
&gt; }
&gt; 
&gt; gives the following output:
&gt; 
&gt; URI is file:/C:/Users
&gt; URI is null
&gt; java.net.URISyntaxException: Illegal character in path at index 8: file:/C:\Users
&gt; at java.net.URI$Parser.fail(URI.java:2848)
&gt; at java.net.URI$Parser.checkChars(URI.java:3021)
&gt; at java.net.URI$Parser.parseHierarchical(URI.java:3105)
&gt; at java.net.URI$Parser.parse(URI.java:3053)
&gt; at java.net.URI.&lt;init&gt;(URI.java:588)
&gt; at java.net.URL.toURI(URL.java:946)
&gt; at com.viavi.Main.makeURI(Main.java:23)
&gt; at com.viavi.Main.main(Main.java:16)
&gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
&gt; at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
&gt; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
&gt;  at java.lang.reflect.Method.invoke(Method.java:498)
&gt; at 
&gt; com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
&gt; 
&gt; That is, it seems that registering the FsUrlStreamHandlerFactory disrupts the \
&gt; subsequent creation of Windows "file" URLs. I encountered this on 2.6.5, and have \
&gt; reproduced in 3.0.0 - alpha2. 
&gt; I believe I have a fair understanding of the root of the issue. Is this of interest \
&gt; to anybody? 
&gt; Thanks
&gt; Simon
&gt; 


---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170831220638</emailId><senderName>Ping Liu</senderName><senderEmail>pingpinganan@gmail.com</senderEmail><timestampReceived>2017-08-31 22:06:38-0400</timestampReceived><subject>native folder not found in hadoop-common build on Ubuntu</subject><body>


I built hadoop-common on Ubuntu in my VirtualBox.  But in target folder, I
didn't find "native" folder that is supposed to contain the generated JNI
header files for C.  On my Windows, native folder is found in target.

As I check the POM file, I found "native build only supported on Mac or
Unix".  Does this mean native is not supported on Linux?

Thanks!

Ping


</body></email><email><emailId>20170831223659</emailId><senderName>John Zhuge</senderName><senderEmail>john.zhuge@gmail.com</senderEmail><timestampReceived>2017-08-31 22:36:59-0400</timestampReceived><subject>Re: native folder not found in hadoop-common build on Ubuntu</subject><body>


Hi Ping,

Thanks for using Hadoop. Linux is Unix-like. Hadoop supports native code on
Linux. Please read BUILDING.txt in the root of the Hadoop source tree.

Could you provide the entire Maven command line when you built Hadoop?

On Thu, Aug 31, 2017 at 3:06 PM, Ping Liu &lt;pingpinganan@gmail.com&gt; wrote:

&gt; I built hadoop-common on Ubuntu in my VirtualBox.  But in target folder, I
&gt; didn't find "native" folder that is supposed to contain the generated JNI
&gt; header files for C.  On my Windows, native folder is found in target.
&gt;
&gt; As I check the POM file, I found "native build only supported on Mac or
&gt; Unix".  Does this mean native is not supported on Linux?
&gt;
&gt; Thanks!
&gt;
&gt; Ping
&gt;



-- 
John


</body></email><email><emailId>20170831225325</emailId><senderName>Ping Liu</senderName><senderEmail>pingpinganan@gmail.com</senderEmail><timestampReceived>2017-08-31 22:53:25-0400</timestampReceived><subject>Re: native folder not found in hadoop-common build on Ubuntu</subject><body>


Hi John,

Thank you for your quick response.

I used

mvn clean install -DskipTests

I just did a comparison with my Windows build result. winutils is missing
too.

So both "native" and "winutils" folders are not generated in target folder,
although it shows BUILD SUCCESS.

Thanks.

Ping







On Thu, Aug 31, 2017 at 3:36 PM, John Zhuge &lt;john.zhuge@gmail.com&gt; wrote:

&gt; Hi Ping,
&gt;
&gt; Thanks for using Hadoop. Linux is Unix-like. Hadoop supports native code
&gt; on Linux. Please read BUILDING.txt in the root of the Hadoop source tree.
&gt;
&gt; Could you provide the entire Maven command line when you built Hadoop?
&gt;
&gt; On Thu, Aug 31, 2017 at 3:06 PM, Ping Liu &lt;pingpinganan@gmail.com&gt; wrote:
&gt;
&gt;&gt; I built hadoop-common on Ubuntu in my VirtualBox.  But in target folder, I
&gt;&gt; didn't find "native" folder that is supposed to contain the generated JNI
&gt;&gt; header files for C.  On my Windows, native folder is found in target.
&gt;&gt;
&gt;&gt; As I check the POM file, I found "native build only supported on Mac or
&gt;&gt; Unix".  Does this mean native is not supported on Linux?
&gt;&gt;
&gt;&gt; Thanks!
&gt;&gt;
&gt;&gt; Ping
&gt;&gt;
&gt;
&gt;
&gt;
&gt; --
&gt; John
&gt;


</body></email><email><emailId>20170831230334</emailId><senderName>Ravi Prakash</senderName><senderEmail>ravihadoop@gmail.com</senderEmail><timestampReceived>2017-08-31 23:03:34-0400</timestampReceived><subject>Re: native folder not found in hadoop-common build on Ubuntu</subject><body>


Please use -Pnative profile

On Thu, Aug 31, 2017 at 3:53 PM, Ping Liu &lt;pingpinganan@gmail.com&gt; wrote:

&gt; Hi John,
&gt;
&gt; Thank you for your quick response.
&gt;
&gt; I used
&gt;
&gt; mvn clean install -DskipTests
&gt;
&gt; I just did a comparison with my Windows build result. winutils is missing
&gt; too.
&gt;
&gt; So both "native" and "winutils" folders are not generated in target folder,
&gt; although it shows BUILD SUCCESS.
&gt;
&gt; Thanks.
&gt;
&gt; Ping
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt; On Thu, Aug 31, 2017 at 3:36 PM, John Zhuge &lt;john.zhuge@gmail.com&gt; wrote:
&gt;
&gt; &gt; Hi Ping,
&gt; &gt;
&gt; &gt; Thanks for using Hadoop. Linux is Unix-like. Hadoop supports native code
&gt; &gt; on Linux. Please read BUILDING.txt in the root of the Hadoop source tree.
&gt; &gt;
&gt; &gt; Could you provide the entire Maven command line when you built Hadoop?
&gt; &gt;
&gt; &gt; On Thu, Aug 31, 2017 at 3:06 PM, Ping Liu &lt;pingpinganan@gmail.com&gt;
&gt; wrote:
&gt; &gt;
&gt; &gt;&gt; I built hadoop-common on Ubuntu in my VirtualBox.  But in target
&gt; folder, I
&gt; &gt;&gt; didn't find "native" folder that is supposed to contain the generated
&gt; JNI
&gt; &gt;&gt; header files for C.  On my Windows, native folder is found in target.
&gt; &gt;&gt;
&gt; &gt;&gt; As I check the POM file, I found "native build only supported on Mac or
&gt; &gt;&gt; Unix".  Does this mean native is not supported on Linux?
&gt; &gt;&gt;
&gt; &gt;&gt; Thanks!
&gt; &gt;&gt;
&gt; &gt;&gt; Ping
&gt; &gt;&gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt; --
&gt; &gt; John
&gt; &gt;
&gt;


</body></email><email><emailId>20170901020351</emailId><senderName>Allen Wittenauer</senderName><senderEmail>aw@effectivemachines.com</senderEmail><timestampReceived>2017-09-01 02:03:51-0400</timestampReceived><subject>Re: native folder not found in hadoop-common build on Ubuntu</subject><body>


Just to close the loop on this a bit ...

	Windows always triggers the 'native-win' profile because winutils is currently \
required to actually use Apache Hadoop on that platform.  On other platforms, the \
'native' profile is optional since their is enough support in the JDK to at least do \
all the basic of tasks.  (It's still HIGHLY recommended, however, that the native \
code get built though.)

	It'd probably be a good project for someone to see if modern JDKs (with or without \
additional dependencies) now have enough support to make winutils optional.  



&gt; On Aug 31, 2017, at 4:14 PM, Ping Liu &lt;pingpinganan@gmail.com&gt; wrote:
&gt; 
&gt; Hi Ravi, John,
&gt; 
&gt; Thanks!  Yeah, it's the first profile.  Now as I tried the build with
&gt; -Pnative, I saw the build failure.  It complains for cmake.
&gt; 
&gt; It's also a requirement specified in BUILDING.txt that John pointed out.
&gt; 
&gt; Thanks!!
&gt; 
&gt; Ping
&gt; 
&gt; 
&gt; 
&gt; On Thu, Aug 31, 2017 at 4:03 PM, Ravi Prakash &lt;ravihadoop@gmail.com&gt; wrote:
&gt; 
&gt; &gt; Please use -Pnative profile
&gt; &gt; 
&gt; &gt; On Thu, Aug 31, 2017 at 3:53 PM, Ping Liu &lt;pingpinganan@gmail.com&gt; wrote:
&gt; &gt; 
&gt; &gt; &gt; Hi John,
&gt; &gt; &gt; 
&gt; &gt; &gt; Thank you for your quick response.
&gt; &gt; &gt; 
&gt; &gt; &gt; I used
&gt; &gt; &gt; 
&gt; &gt; &gt; mvn clean install -DskipTests
&gt; &gt; &gt; 
&gt; &gt; &gt; I just did a comparison with my Windows build result. winutils is missing
&gt; &gt; &gt; too.
&gt; &gt; &gt; 
&gt; &gt; &gt; So both "native" and "winutils" folders are not generated in target
&gt; &gt; &gt; folder,
&gt; &gt; &gt; although it shows BUILD SUCCESS.
&gt; &gt; &gt; 
&gt; &gt; &gt; Thanks.
&gt; &gt; &gt; 
&gt; &gt; &gt; Ping
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; On Thu, Aug 31, 2017 at 3:36 PM, John Zhuge &lt;john.zhuge@gmail.com&gt; wrote:
&gt; &gt; &gt; 
&gt; &gt; &gt; &gt; Hi Ping,
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; Thanks for using Hadoop. Linux is Unix-like. Hadoop supports native code
&gt; &gt; &gt; &gt; on Linux. Please read BUILDING.txt in the root of the Hadoop source
&gt; &gt; &gt; tree.
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; Could you provide the entire Maven command line when you built Hadoop?
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; On Thu, Aug 31, 2017 at 3:06 PM, Ping Liu &lt;pingpinganan@gmail.com&gt;
&gt; &gt; &gt; wrote:
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; I built hadoop-common on Ubuntu in my VirtualBox.  But in target
&gt; &gt; &gt; folder, I
&gt; &gt; &gt; &gt; &gt; didn't find "native" folder that is supposed to contain the generated
&gt; &gt; &gt; JNI
&gt; &gt; &gt; &gt; &gt; header files for C.  On my Windows, native folder is found in target.
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; As I check the POM file, I found "native build only supported on Mac or
&gt; &gt; &gt; &gt; &gt; Unix".  Does this mean native is not supported on Linux?
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; Thanks!
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; &gt; Ping
&gt; &gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; --
&gt; &gt; &gt; &gt; John
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; 
&gt; &gt; 


---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170901133800</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-01 13:38:00-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14345) S3Guard: S3GuardTool to support provisioning existing metadata stor</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14345?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-14345.
-------------------------------------
    Resolution: Duplicate

&gt; S3Guard: S3GuardTool to support provisioning existing metadata store
&gt; --------------------------------------------------------------------
&gt; 
&gt; Key: HADOOP-14345
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14345
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3
&gt; Reporter: Mingliang Liu
&gt; Assignee: Steve Loughran
&gt; Priority: Minor
&gt; 
&gt; I don't know if this is considered a requested feature from S3Guard code, as the \
&gt; user can always provision the DDB tables via the cloud portal. Implementing should \
&gt; be straightforward. The {{DynamoDBMetadataStore}} has provision method to use.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170901134101</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-01 13:41:01-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14810) S3Guard: handle provisioning failure through backoff &amp; retry (&amp; met</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14810?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-14810.
-------------------------------------
    Resolution: Duplicate

Duplicate of HADOOP-13761; closing as such and pasting stack trace into that JIRA

&gt; S3Guard: handle provisioning failure through backoff &amp; retry (&amp; metrics)
&gt; ------------------------------------------------------------------------
&gt; 
&gt; Key: HADOOP-14810
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14810
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3
&gt; Affects Versions: HADOOP-13345
&gt; Reporter: Steve Loughran
&gt; Attachments: summary.txt
&gt; 
&gt; 
&gt; S3Guard can't handle overloaded tables.
&gt; I think we all though the API did: it doesn't; exceptions get raised and the caller \
&gt; is expected to handle it. This relates very much to the s3a-lambda invocation code \
&gt; in HADOOP-13786 to handle failures during commit, and the need for all the \
&gt; S3AFileSystem calls of the S3 APIs to handle transient failures like throttling, \
&gt; and again, needs some fault injection to verify the handling, metrics to count rate \
&gt; so it can be monitored  &amp; used to understand why work is underperforming.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170901135800</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-01 13:58:00-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14012) Handled dynamo exceptions in translateException</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14012?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-14012.
-------------------------------------
    Resolution: Duplicate

Marking as a duplicate of HADOOP-13761; that's where this work will be needed

&gt; Handled dynamo exceptions in translateException
&gt; -----------------------------------------------
&gt; 
&gt; Key: HADOOP-14012
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14012
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3
&gt; Affects Versions: HADOOP-13345
&gt; Reporter: Steve Loughran
&gt; 
&gt; {{translateException}} is good at translating S3 exceptions into meaningful IOEs. \
&gt; We need to make sure that dynamodb exceptions are also mapped appropriately.  \
&gt; Action plan # break things
&gt; # collect the stack traces
&gt; # translate them where possible
&gt; # discuss in troubleshooting section of s3guard



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170901140801</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-01 14:08:01-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14801) s3guard diff demand creates a new table</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14801?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-14801.
-------------------------------------
          Resolution: Duplicate
    Target Version/s: 3.1.0  (was: HADOOP-13345)

&gt; s3guard diff demand creates a new table
&gt; ---------------------------------------
&gt; 
&gt; Key: HADOOP-14801
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14801
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3
&gt; Affects Versions: HADOOP-13345
&gt; Reporter: Steve Loughran
&gt; Priority: Minor
&gt; 
&gt; ifr you call {{s3guard diff}} to diff a bucket and a table, it creates the table if \
&gt; not already there. I don't see that as being the right thing to do. {code}
&gt; hadoop s3guard diff $bucket
&gt; 2017-08-22 15:14:47,025 INFO s3guard.DynamoDBMetadataStore: Creating non-existent \
&gt; DynamoDB table hwdev-steve-ireland-new in region eu-west-1 2017-08-22 15:14:52,384 \
&gt; INFO s3guard.S3GuardTool: Metadata store DynamoDBMetadataStore{region=eu-west-1, \
&gt; tableName=hwdev-steve-ireland-new} is initialized. {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170901143200</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-01 14:32:00-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14800) eliminate double stack trace on some s3guard CLI failures</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14800?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-14800.
-------------------------------------
    Resolution: Duplicate

&gt; eliminate double stack trace on some s3guard CLI failures
&gt; ---------------------------------------------------------
&gt; 
&gt; Key: HADOOP-14800
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14800
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3
&gt; Affects Versions: 3.0.0-beta1
&gt; Reporter: Steve Loughran
&gt; Priority: Minor
&gt; Fix For: 3.1.0
&gt; 
&gt; 
&gt; {{s3guard destroy}] when there's no bucket ends up double-listing the stack trace, \
&gt; which is somewhat confusing



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170901145005</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-01 14:50:05-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-13345) S3Guard: Improved Consistency for S3A</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-13345?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-13345.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 3.0.0-beta1

Closing as fixed. I've added some release notes. Andrew: no bit in the site docs \
though, other than the bits off the s3 docs. Which, having just read, need a rework \
anyway, as things which are marked as "stabilising" can now be considered stable

&gt; S3Guard: Improved Consistency for S3A
&gt; -------------------------------------
&gt; 
&gt; Key: HADOOP-13345
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-13345
&gt; Project: Hadoop Common
&gt; Issue Type: New Feature
&gt; Components: fs/s3
&gt; Affects Versions: 2.8.1
&gt; Reporter: Chris Nauroth
&gt; Assignee: Chris Nauroth
&gt; Fix For: 3.0.0-beta1
&gt; 
&gt; Attachments: HADOOP-13345.prototype1.patch, s3c.001.patch, \
&gt; S3C-ConsistentListingonS3-Design.pdf, S3GuardImprovedConsistencyforS3A.pdf, \
&gt; S3GuardImprovedConsistencyforS3AV2.pdf 
&gt; 
&gt; This issue proposes S3Guard, a new feature of S3A, to provide an option for a \
&gt; stronger consistency model than what is currently offered.  The solution \
&gt; coordinates with a strongly consistent external store to resolve inconsistencies \
&gt; caused by the S3 eventual consistency model.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170901151110</emailId><senderName>Allen Wittenauer</senderName><senderEmail>aw@effectivemachines.com</senderEmail><timestampReceived>2017-09-01 15:11:10-0400</timestampReceived><subject>YARN javadoc failures  Re: [DISCUSS] Branches and versions for Hadoop 3</subject><body>


&gt; On Aug 28, 2017, at 9:58 AM, Allen Wittenauer &lt;aw@effectivemachines.com&gt; wrote:
&gt; 	The automation only goes so far.  At least while investigating Yetus bugs, I've \
&gt; seen more than enough blatant and purposeful ignored errors and warnings that I'm \
&gt; not convinced it will be effective. ("That javadoc compile failure didn't come from \
&gt; my patch!"  Um, yes, yes it did.) PR for features has greatly trumped code \
&gt; correctness for a few years now.


	I'm psychic.

	Looks like YARN-6877 is crashing JDK8 javadoc.  Maven stops processing and errors \
out before even giving a build error/success. Reverting the patch makes things work \
again. Anyway, Yetus caught it, warned about it continuously, but it was still \
committed.  

	
---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170901164801</emailId><senderName>Steve Loughran</senderName><senderEmail>stevel@hortonworks.com</senderEmail><timestampReceived>2017-09-01 16:48:01-0400</timestampReceived><subject>Re: [VOTE] Merge HADOOP-13345 (S3Guard feature branch)</subject><body>

[Attachment #2 (text/plain)]


On 25 Aug 2017, at 20:22, Aaron Fabbri \
&lt;fabbri@cloudera.com&lt;mailto:fabbri@cloudera.com&gt;&gt; wrote:

Thank you everyone for reviewing and voting on the S3Guard feature branch merge.

It looks like the Vote was a success. We have six binding +1's (Steve Loughran, Sean \
Mackrory, Mingliang Liu, Sanjay Radia, Kihwal Lee, and Lei (Eddy) Xu) and zero -1's.

I will coordinate w/ Steve L to get this committed to trunk.  I think we are going to \
bring it to branch-2 as well.

-AF




Update: this is now committed to trunk!


This was a major piece of work —and it's been a great time working with people.

Chris Nauroth, Aaron Fabbri, Mingliang Liu, Lei (Eddy) Xu, Sean Mackrory, &amp; others, \
as well as the effort of everyone who tested this, helped with the documentation, \
complained when it broke, etc.

Special mention: Thomas Demoor &amp; Ewan Higgs for explaining the low-level details of \
S3 protocols in a way that AWS themselves don't document.

It's gone in as one big patch &amp; not many small ones; we'd always planned it that way \
and had regularly merged trunk into the branch, its got regressions and fixes of them \
in. it'd be a mess. Now it's a single patch and you know who to complain to when it \
doesn't work. Sorry.


What next in S3A land, well, let me see

* HADOOP-14825 is where all the unfinished S3Guard work goes, with HADOOP-14220 being \
                some CLI improvements I've been adding based on recent use.
* HADOOP-13786 is my big "0-rename committer". It's been a regularly rebased branch \
atop the HADOOP-13345 branch, alongside an external downstream module to test the \
spark integration (we know the mapred v2 API stuff works, its only Spark &amp; Parquet \
                which doesn't play).
* With S3Guard in, you can now turn on listing inconsistency in the client; in \
HADOOP-13786 I've added more fault injection in the form of "service throttled" \
responses. S3A doesn't handle them yet, which needs to be fixed not just in the new \
commit operations (which do), but in every single FileSystem API call. Same for other \
failures. Hence HADOOP-14531&lt;https://issues.apache.org/jira/browse/HADOOP-14531&gt; .

As usual, people willing to code, document &amp; test welcome. Go on, download trunk, \
test with s3guard enabled: now is the time to complain that things don't work!

-Steve.



</body></email><email><emailId>20170901173800</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-01 17:38:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14826) reviewS3 docs prior to 3.0-beta-1</subject><body>

Steve Loughran created HADOOP-14826:
---------------------------------------

             Summary: reviewS3 docs prior to 3.0-beta-1
                 Key: HADOOP-14826
                 URL: https://issues.apache.org/jira/browse/HADOOP-14826
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: documentation, fs/s3
    Affects Versions: 3.0.0-beta1
            Reporter: Steve Loughran


The hadoop-aws docs need a review and update

* things marked as stabilizing (fast upload, .fadvise ..) can be considered stable
* move s3n docs off to the side
* add a "how to move from s3n to s3a" para



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170601135604</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-01 13:56:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14476) make InconsistentAmazonS3Client usable in downstream tests</subject><body>

Steve Loughran created HADOOP-14476:
---------------------------------------

             Summary: make InconsistentAmazonS3Client usable in downstream tests
                 Key: HADOOP-14476
                 URL: https://issues.apache.org/jira/browse/HADOOP-14476
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/s3, test
    Affects Versions: HADOOP-13345
            Reporter: Steve Loughran


It's important for downstream apps to be able to verify that s3guard works by making \
the AWS client inconsistent (so demonstrate problems), then turn s3guard on to verify \
that they go away. 

This can be done by exposing the {{InconsistentAmazonS3Client}}

# move the factory to the production source
# make delay configurable for when you want a really long delay
# have factory code log @ warn when a non-default factory is used.
# mention in s3a testing.md

I think we could look at the name of the option, {{fs.s3a.s3.client.factory.impl}} \
too. I'd like something which has "internal" in it, and without the duplication of \
s3a.s3



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170601214304</emailId><senderName>"BELUGA BEHR (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-01 21:43:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14477) FileSystem Simplify / Optimize listStatus Method</subject><body>

BELUGA BEHR created HADOOP-14477:
------------------------------------

             Summary: FileSystem Simplify / Optimize listStatus Method
                 Key: HADOOP-14477
                 URL: https://issues.apache.org/jira/browse/HADOOP-14477
             Project: Hadoop Common
          Issue Type: Improvement
    Affects Versions: 3.0.0-alpha3, 2.7.3
            Reporter: BELUGA BEHR
            Priority: Minor


{code:title=org.apache.hadoop.fs.FileSystem.listStatus(ArrayList&lt;FileStatus&gt;, Path, PathFilter)}
  /*
   * Filter files/directories in the given path using the user-supplied path
   * filter. Results are added to the given array &lt;code&gt;results&lt;/code&gt;.
   */
  private void listStatus(ArrayList&lt;FileStatus&gt; results, Path f,
      PathFilter filter) throws FileNotFoundException, IOException {
    FileStatus listing[] = listStatus(f);
    if (listing == null) {
      throw new IOException("Error accessing " + f);
    }

    for (int i = 0; i &lt; listing.length; i++) {
      if (filter.accept(listing[i].getPath())) {
        results.add(listing[i]);
      }
    }
  }
{code}

{code:title=org.apache.hadoop.fs.FileSystem.listStatus(Path, PathFilter)}
  public FileStatus[] listStatus(Path f, PathFilter filter) 
                                   throws FileNotFoundException, IOException {
    ArrayList&lt;FileStatus&gt; results = new ArrayList&lt;FileStatus&gt;();
    listStatus(results, f, filter);
    return results.toArray(new FileStatus[results.size()]);
  }
{code}

We can be smarter about this:

# Use enhanced for-loops
# Optimize for the case where there are zero files in a directory, save on object instantiation



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170602063005</emailId><senderName>"Rajesh Balamohan (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-02 06:30:05-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14478) Optimize NativeAzureFsInputStream for positional reads</subject><body>

Rajesh Balamohan created HADOOP-14478:
-----------------------------------------

             Summary: Optimize NativeAzureFsInputStream for positional reads
                 Key: HADOOP-14478
                 URL: https://issues.apache.org/jira/browse/HADOOP-14478
             Project: Hadoop Common
          Issue Type: Bug
          Components: fs/azure
            Reporter: Rajesh Balamohan


Azure's {{BlobbInputStream}} internally buffers 4 MB of data irrespective of the data \
length requested for. This would be beneficial for sequential reads. However, for \
positional reads (seek to specific location, read x number of bytes, seek back to \
original location) this may not be beneficial and might even download lot more data \
which are not used later.

It would be good to override {{readFully(long position, byte[] buffer, int offset, \
int length)}} for {{NativeAzureFsInputStream}} and make use of {{mark(readLimit)}} as \
a hint to Azure's BlobInputStream.

BlobInputStream reference: \
https://github.com/Azure/azure-storage-java/blob/master/microsoft-azure-storage/src/com/microsoft/azure/storage/blob/BlobInputStream.java#L448


BlobInputStream can consider this as a hint later to determine the amount of data to \
be read ahead. Changes to BlobInputStream would not be apart of this JIRA.






--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170602142804</emailId><senderName>"Ayappan (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-02 14:28:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14479) Erasurecode testcase failures with ISA-L</subject><body>

Ayappan created HADOOP-14479:
--------------------------------

             Summary: Erasurecode testcase failures with ISA-L 
                 Key: HADOOP-14479
                 URL: https://issues.apache.org/jira/browse/HADOOP-14479
             Project: Hadoop Common
          Issue Type: Bug
          Components: common
    Affects Versions: 3.0.0-alpha3
         Environment: x86_64 Ubuntu 16.04.02 LTS
            Reporter: Ayappan


I built hadoop with ISA-L support. I took the ISA-L code from \
https://github.com/01org/isa-l  (tag v2.18.0) and built it. While running the UTs , \
following three testcases are failing

1)TestHHXORErasureCoder

Tests run: 7, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 1.106 sec &lt;&lt;&lt; \
FAILURE! - in org.apache.hadoop.io.erasurecode.coder.TestHHXORErasureCoder \
testCodingDirectBuffer_10x4_erasing_p1(org.apache.hadoop.io.erasurecode.coder.TestHHXORErasureCoder) \
                Time elapsed: 0.029 sec  &lt;&lt;&lt; FAILURE!
java.lang.AssertionError: Decoding and comparing failed.
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.assertTrue(Assert.java:41)
        at org.apache.hadoop.io.erasurecode.TestCoderBase.compareAndVerify(TestCoderBase.java:170)
                
        at org.apache.hadoop.io.erasurecode.coder.TestErasureCoderBase.compareAndVerify(TestErasureCoderBase.java:141)
                
        at org.apache.hadoop.io.erasurecode.coder.TestErasureCoderBase.performTestCoding(TestErasureCoderBase.java:98)
                
        at org.apache.hadoop.io.erasurecode.coder.TestErasureCoderBase.testCoding(TestErasureCoderBase.java:69)
                
        at org.apache.hadoop.io.erasurecode.coder.TestHHXORErasureCoder.testCodingDirectBuffer_10x4_erasing_p1(TestHHXORErasureCoder.java:64)



2)TestRSErasureCoder

Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.591 sec - in \
org.apache.hadoop.io.erasurecode.coder.TestXORCoder Running \
org.apache.hadoop.io.erasurecode.coder.TestRSErasureCoder #
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f486a28a6e4, pid=8970, tid=0x00007f4850927700
#
# JRE version: OpenJDK Runtime Environment (8.0_121-b13) (build \
1.8.0_121-8u121-b13-0ubuntu1.16.04.2-b13) # Java VM: OpenJDK 64-Bit Server VM \
(25.121-b13 mixed mode linux-amd64 compressed oops) # Problematic frame:
# C  [libc.so.6+0x8e6e4]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, \
try "ulimit -c unlimited" before starting Java again #
# An error report file with more information is saved as:
# /home/ayappan/hadoop/hadoop-common-project/hadoop-common/hs_err_pid8970.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#

3)TestCodecRawCoderMapping

Running org.apache.hadoop.io.erasurecode.TestCodecRawCoderMapping
Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.559 sec &lt;&lt;&lt; \
FAILURE! - in org.apache.hadoop.io.erasurecode.TestCodecRawCoderMapping \
testRSDefaultRawCoder(org.apache.hadoop.io.erasurecode.TestCodecRawCoderMapping)  \
                Time elapsed: 0.015 sec  &lt;&lt;&lt; FAILURE!
java.lang.AssertionError: null
        at org.junit.Assert.fail(Assert.java:86)
        at org.junit.Assert.assertTrue(Assert.java:41)
        at org.junit.Assert.assertTrue(Assert.java:52)
        at org.apache.hadoop.io.erasurecode.TestCodecRawCoderMapping.testRSDefaultRawCoder(TestCodecRawCoderMapping.java:58)





--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170120223639</emailId><senderName>Andrew Wang</senderName><senderEmail>andrew.wang@cloudera.com</senderEmail><timestampReceived>2017-01-20 22:36:39-0400</timestampReceived><subject>[VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>


Hi all,

With heartfelt thanks to many contributors, the RC0 for 3.0.0-alpha2 is
ready.

3.0.0-alpha2 is the second alpha in the planned 3.0.0 release line leading
up to a 3.0.0 GA. It comprises 857 fixes, improvements, and new features
since alpha1 was released on September 3rd, 2016.

More information about the 3.0.0 release plan can be found here:

https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+3.0.0+release

The artifacts can be found here:

http://home.apache.org/~wang/3.0.0-alpha2-RC0/

This vote will run 5 days, ending on 01/25/2017 at 2PM pacific.

I ran basic validation with a local pseudo cluster and a Pi job. RAT output
was clean.

My +1 to start.

Thanks,
Andrew


</body></email><email><emailId>20170121024752</emailId><senderName>John Zhuge</senderName><senderEmail>jzhuge@cloudera.com</senderEmail><timestampReceived>2017-01-21 02:47:52-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>


Discovered 1 missing license using the script verify-license-notice.sh.

Missing LICENSE!
./../hadoop-dist/target/hadoop-3.0.0-alpha2/share/hadoop/client/hadoop-client-api-3.0.0-alpha2.jar
- Verified checksums and signatures of the tarballs
- Built source with Java 1.8.0_66 on Mac
- Deployed a pseudo cluster, passed the following sanity tests in both insecure
and SSL mode:

- basic dfs, distcp, ACL commands

- KMS and HttpFS tests

- MapReduce wordcount example
- balancer start/stop


John Zhuge
Software Engineer, Cloudera

On Fri, Jan 20, 2017 at 2:36 PM, Andrew Wang &lt;andrew.wang@cloudera.com&gt;
wrote:

&gt; Hi all,
&gt;
&gt; With heartfelt thanks to many contributors, the RC0 for 3.0.0-alpha2 is
&gt; ready.
&gt;
&gt; 3.0.0-alpha2 is the second alpha in the planned 3.0.0 release line leading
&gt; up to a 3.0.0 GA. It comprises 857 fixes, improvements, and new features
&gt; since alpha1 was released on September 3rd, 2016.
&gt;
&gt; More information about the 3.0.0 release plan can be found here:
&gt;
&gt; https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+3.0.0+release
&gt;
&gt; The artifacts can be found here:
&gt;
&gt; http://home.apache.org/~wang/3.0.0-alpha2-RC0/
&gt;
&gt; This vote will run 5 days, ending on 01/25/2017 at 2PM pacific.
&gt;
&gt; I ran basic validation with a local pseudo cluster and a Pi job. RAT output
&gt; was clean.
&gt;
&gt; My +1 to start.
&gt;
&gt; Thanks,
&gt; Andrew
&gt;


</body></email><email><emailId>20170121035646</emailId><senderName>Anu Engineer</senderName><senderEmail>aengineer@hortonworks.com</senderEmail><timestampReceived>2017-01-21 03:56:46-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>

SGkgQW5kcmV3LCANCg0KVGhhbmsgeW91IGZvciBhbGwgdGhlIGhhcmQgd29yay4gSSBhbSByZWFs
bHkgZXhjaXRlZCB0byBzZWUgdXMgbWFraW5nIHByb2dyZXNzIHRvd2FyZHMgYSAzLjAgcmVsZWFz
ZS4NCg0KKzEgKE5vbi1CaW5kaW5nKQ0KDQoxLiBEZXBsb3llZCB0aGUgZG93bmxvYWRlZCBiaXRz
IG9uIDQgbm9kZSBjbHVzdGVyIHdpdGggMSBOYW1lbm9kZSBhbmQgMyBkYXRhbm9kZXMuDQoyLiBW
ZXJpZmllZCBhbGwgbm9ybWFsIEhERlMgb3BlcmF0aW9ucyBsaWtlIGNyZWF0ZSBkaXJlY3Rvcnks
IGNyZWF0ZSBmaWxlICwgZGVsZXRlIGZpbGUgZXRjLg0KMy4gUmFuIE1hcCByZWR1Y2Ugam9icyAg
LSBQaSBhbmQgV29yZGNvdW50DQo1LiBWZXJpZmllZCBIYWRvb3AgdmVyc2lvbiBjb21tYW5kIG91
dHB1dCBpcyBjb3JyZWN0Lg0KDQpUaGFua3MNCkFudQ0KDQoNCk9uIDEvMjAvMTcsIDI6MzYgUE0s
ICJBbmRyZXcgV2FuZyIgPGFuZHJldy53YW5nQGNsb3VkZXJhLmNvbT4gd3JvdGU6DQoNCj5IaSBh
bGwsDQo+DQo+V2l0aCBoZWFydGZlbHQgdGhhbmtzIHRvIG1hbnkgY29udHJpYnV0b3JzLCB0aGUg
UkMwIGZvciAzLjAuMC1hbHBoYTIgaXMNCj5yZWFkeS4NCj4NCj4zLjAuMC1hbHBoYTIgaXMgdGhl
IHNlY29uZCBhbHBoYSBpbiB0aGUgcGxhbm5lZCAzLjAuMCByZWxlYXNlIGxpbmUgbGVhZGluZw0K
PnVwIHRvIGEgMy4wLjAgR0EuIEl0IGNvbXByaXNlcyA4NTcgZml4ZXMsIGltcHJvdmVtZW50cywg
YW5kIG5ldyBmZWF0dXJlcw0KPnNpbmNlIGFscGhhMSB3YXMgcmVsZWFzZWQgb24gU2VwdGVtYmVy
IDNyZCwgMjAxNi4NCj4NCj5Nb3JlIGluZm9ybWF0aW9uIGFib3V0IHRoZSAzLjAuMCByZWxlYXNl
IHBsYW4gY2FuIGJlIGZvdW5kIGhlcmU6DQo+DQo+aHR0cHM6Ly9jd2lraS5hcGFjaGUub3JnL2Nv
bmZsdWVuY2UvZGlzcGxheS9IQURPT1AvSGFkb29wKzMuMC4wK3JlbGVhc2UNCj4NCj5UaGUgYXJ0
aWZhY3RzIGNhbiBiZSBmb3VuZCBoZXJlOg0KPg0KPmh0dHA6Ly9ob21lLmFwYWNoZS5vcmcvfndh
bmcvMy4wLjAtYWxwaGEyLVJDMC8NCj4NCj5UaGlzIHZvdGUgd2lsbCBydW4gNSBkYXlzLCBlbmRp
bmcgb24gMDEvMjUvMjAxNyBhdCAyUE0gcGFjaWZpYy4NCj4NCj5JIHJhbiBiYXNpYyB2YWxpZGF0
aW9uIHdpdGggYSBsb2NhbCBwc2V1ZG8gY2x1c3RlciBhbmQgYSBQaSBqb2IuIFJBVCBvdXRwdXQN
Cj53YXMgY2xlYW4uDQo+DQo+TXkgKzEgdG8gc3RhcnQuDQo+DQo+VGhhbmtzLA0KPkFuZHJldw0K
DQo=
DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGNvbW1vbi1kZXYt
dW5zdWJzY3JpYmVAaGFkb29wLmFwYWNoZS5vcmcNCkZvciBhZGRpdGlvbmFsIGNvbW1hbmRz
LCBlLW1haWw6IGNvbW1vbi1kZXYtaGVscEBoYWRvb3AuYXBhY2hlLm9yZw0K
</body></email><email><emailId>20170121155013</emailId><senderName>Marton Elek</senderName><senderEmail>melek@hortonworks.com</senderEmail><timestampReceived>2017-01-21 15:50:13-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>

hadoop-3.0.0-alpha2.tar.gz is much more smaller than hadoop-3.0.0-alpha1.tar.gz. \
(246M vs 316M)

The big difference is the generated source documentation:

find -name src-html
./hadoop-2.7.3/share/doc/hadoop/api/src-html
./hadoop-2.7.3/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/src-html
./hadoop-3.0.0-alpha1/share/doc/hadoop/api/src-html
./hadoop-3.0.0-alpha1/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/src-html
./hadoop-3.0.0-alpha1/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/api/src-html
./hadoop-3.0.0-alpha1/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/api/src-html
                
./hadoop-3.0.0-alpha1/share/doc/hadoop/hadoop-project-dist/hadoop-common/api/src-html
(./hadoop-3.0.0-alpha-2 --&gt; no match)

I am just wondering if it's intentional or not as I can't find any related jira or \
mail thread (maybe I missed it)

Regards,
Marton

On 01/20/2017 11:36 PM, Andrew Wang wrote:
&gt; Hi all,
&gt; 
&gt; With heartfelt thanks to many contributors, the RC0 for 3.0.0-alpha2 is
&gt; ready.
&gt; 
&gt; 3.0.0-alpha2 is the second alpha in the planned 3.0.0 release line leading
&gt; up to a 3.0.0 GA. It comprises 857 fixes, improvements, and new features
&gt; since alpha1 was released on September 3rd, 2016.
&gt; 
&gt; More information about the 3.0.0 release plan can be found here:
&gt; 
&gt; https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+3.0.0+release
&gt; 
&gt; The artifacts can be found here:
&gt; 
&gt; http://home.apache.org/~wang/3.0.0-alpha2-RC0/
&gt; 
&gt; This vote will run 5 days, ending on 01/25/2017 at 2PM pacific.
&gt; 
&gt; I ran basic validation with a local pseudo cluster and a Pi job. RAT output
&gt; was clean.
&gt; 
&gt; My +1 to start.
&gt; 
&gt; Thanks,
&gt; Andrew
&gt; 

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170121184336</emailId><senderName>Akira Ajisaka</senderName><senderEmail>aajisaka@apache.org</senderEmail><timestampReceived>2017-01-21 18:43:36-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>

Hi Marton,

This is intentional. source htmls were removed.
https://issues.apache.org/jira/browse/HADOOP-13688

Regards,
Akira

On 2017/01/22 0:50, Marton Elek wrote:
&gt; hadoop-3.0.0-alpha2.tar.gz is much more smaller than hadoop-3.0.0-alpha1.tar.gz. \
&gt; (246M vs 316M) 
&gt; The big difference is the generated source documentation:
&gt; 
&gt; find -name src-html
&gt; ./hadoop-2.7.3/share/doc/hadoop/api/src-html
&gt; ./hadoop-2.7.3/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/src-html
&gt; ./hadoop-3.0.0-alpha1/share/doc/hadoop/api/src-html
&gt; ./hadoop-3.0.0-alpha1/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/src-html
&gt; ./hadoop-3.0.0-alpha1/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/api/src-html
&gt; ./hadoop-3.0.0-alpha1/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/api/src-html
&gt;                 
&gt; ./hadoop-3.0.0-alpha1/share/doc/hadoop/hadoop-project-dist/hadoop-common/api/src-html
&gt;  (./hadoop-3.0.0-alpha-2 --&gt; no match)
&gt; 
&gt; I am just wondering if it's intentional or not as I can't find any related jira or \
&gt; mail thread (maybe I missed it) 
&gt; Regards,
&gt; Marton
&gt; 
&gt; On 01/20/2017 11:36 PM, Andrew Wang wrote:
&gt; &gt; Hi all,
&gt; &gt; 
&gt; &gt; With heartfelt thanks to many contributors, the RC0 for 3.0.0-alpha2 is
&gt; &gt; ready.
&gt; &gt; 
&gt; &gt; 3.0.0-alpha2 is the second alpha in the planned 3.0.0 release line leading
&gt; &gt; up to a 3.0.0 GA. It comprises 857 fixes, improvements, and new features
&gt; &gt; since alpha1 was released on September 3rd, 2016.
&gt; &gt; 
&gt; &gt; More information about the 3.0.0 release plan can be found here:
&gt; &gt; 
&gt; &gt; https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+3.0.0+release
&gt; &gt; 
&gt; &gt; The artifacts can be found here:
&gt; &gt; 
&gt; &gt; http://home.apache.org/~wang/3.0.0-alpha2-RC0/
&gt; &gt; 
&gt; &gt; This vote will run 5 days, ending on 01/25/2017 at 2PM pacific.
&gt; &gt; 
&gt; &gt; I ran basic validation with a local pseudo cluster and a Pi job. RAT output
&gt; &gt; was clean.
&gt; &gt; 
&gt; &gt; My +1 to start.
&gt; &gt; 
&gt; &gt; Thanks,
&gt; &gt; Andrew
&gt; &gt; 
&gt; 
&gt; ---------------------------------------------------------------------
&gt; To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
&gt; For additional commands, e-mail: common-dev-help@hadoop.apache.org
&gt; 

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170122215126</emailId><senderName>Andrew Wang</senderName><senderEmail>andrew.wang@cloudera.com</senderEmail><timestampReceived>2017-01-22 21:51:26-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>


Thanks for the validation John. Do we feel the L&amp;N issue is enough to sink
the RC?

On Fri, Jan 20, 2017 at 6:47 PM, John Zhuge &lt;jzhuge@cloudera.com&gt; wrote:

&gt; Discovered 1 missing license using the script verify-license-notice.sh.
&gt;
&gt; Missing LICENSE! ./../hadoop-dist/target/hadoop-3.0.0-alpha2/share/
&gt; hadoop/client/hadoop-client-api-3.0.0-alpha2.jar
&gt; - Verified checksums and signatures of the tarballs
&gt; - Built source with Java 1.8.0_66 on Mac
&gt; - Deployed a pseudo cluster, passed the following sanity tests in both insecure
&gt; and SSL mode:
&gt;
&gt; - basic dfs, distcp, ACL commands
&gt;
&gt; - KMS and HttpFS tests
&gt;
&gt; - MapReduce wordcount example
&gt; - balancer start/stop
&gt;
&gt;
&gt; John Zhuge
&gt; Software Engineer, Cloudera
&gt;
&gt; On Fri, Jan 20, 2017 at 2:36 PM, Andrew Wang &lt;andrew.wang@cloudera.com&gt;
&gt; wrote:
&gt;
&gt;&gt; Hi all,
&gt;&gt;
&gt;&gt; With heartfelt thanks to many contributors, the RC0 for 3.0.0-alpha2 is
&gt;&gt; ready.
&gt;&gt;
&gt;&gt; 3.0.0-alpha2 is the second alpha in the planned 3.0.0 release line leading
&gt;&gt; up to a 3.0.0 GA. It comprises 857 fixes, improvements, and new features
&gt;&gt; since alpha1 was released on September 3rd, 2016.
&gt;&gt;
&gt;&gt; More information about the 3.0.0 release plan can be found here:
&gt;&gt;
&gt;&gt; https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+3.0.0+release
&gt;&gt;
&gt;&gt; The artifacts can be found here:
&gt;&gt;
&gt;&gt; http://home.apache.org/~wang/3.0.0-alpha2-RC0/
&gt;&gt;
&gt;&gt; This vote will run 5 days, ending on 01/25/2017 at 2PM pacific.
&gt;&gt;
&gt;&gt; I ran basic validation with a local pseudo cluster and a Pi job. RAT
&gt;&gt; output
&gt;&gt; was clean.
&gt;&gt;
&gt;&gt; My +1 to start.
&gt;&gt;
&gt;&gt; Thanks,
&gt;&gt; Andrew
&gt;&gt;
&gt;
&gt;


</body></email><email><emailId>20170123050529</emailId><senderName>Allen Wittenauer</senderName><senderEmail>aw@effectivemachines.com</senderEmail><timestampReceived>2017-01-23 05:05:29-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>





&gt; On Jan 20, 2017, at 2:36 PM, Andrew Wang &lt;andrew.wang@cloudera.com&gt; wrote:
&gt; 
&gt; http://home.apache.org/~wang/3.0.0-alpha2-RC0/

	There are quite a few JIRA issues that need release notes.


---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170123064545</emailId><senderName>Allen Wittenauer</senderName><senderEmail>aw@effectivemachines.com</senderEmail><timestampReceived>2017-01-23 06:45:45-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>


&gt; On Jan 22, 2017, at 9:05 PM, Allen Wittenauer &lt;aw@effectivemachines.com&gt; wrote:
&gt; 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; &gt; On Jan 20, 2017, at 2:36 PM, Andrew Wang &lt;andrew.wang@cloudera.com&gt; wrote:
&gt; &gt; 
&gt; &gt; http://home.apache.org/~wang/3.0.0-alpha2-RC0/
&gt; 
&gt; 	There are quite a few JIRA issues that need release notes.
&gt; 


	One other thing, before I forget... I'm not sure the hadoop-client-minicluster jar \
is getting built properly.  If you look inside, you'll find a real mishmash of \
things, including files and directories with the same names but different cases.  \
This means it won't extract properly on OS X.  (jar xf on that jar file literally \
                stack traces on my El Capitan machine. Neat!)
---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170123185144</emailId><senderName>Andrew Wang</senderName><senderEmail>andrew.wang@cloudera.com</senderEmail><timestampReceived>2017-01-23 18:51:44-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>


There are 5 JIRAs by my count that are missing release notes, which isn't
that bad but could of course be improved. Four of those I had already
checked earlier and forgot to follow up, and they were very minorly
incompatible (affecting private APIs) or mistakenly marked incompatible.

I'm not too concerned about the shaded minicluster since it's a new
feature, this is an alpha, and we have an IT test against the shaded
minicluster. Multiple files with the same name are actually also allowed by
the zip standard, so it's not clear if there is a functionality bug.

Could I get some additional PMC input on this vote? The most critical issue
in my mind is the missing LICENSE on that one new artifact. If we end up
spinning a new RC, I'll also handle the missing release notes that Allen
mentioned.

Thanks,
Andrew

On Sun, Jan 22, 2017 at 10:45 PM, Allen Wittenauer &lt;aw@effectivemachines.com
&gt; wrote:

&gt;
&gt; &gt; On Jan 22, 2017, at 9:05 PM, Allen Wittenauer &lt;aw@effectivemachines.com&gt;
&gt; wrote:
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;&gt; On Jan 20, 2017, at 2:36 PM, Andrew Wang &lt;andrew.wang@cloudera.com&gt;
&gt; wrote:
&gt; &gt;&gt;
&gt; &gt;&gt; http://home.apache.org/~wang/3.0.0-alpha2-RC0/
&gt; &gt;
&gt; &gt;       There are quite a few JIRA issues that need release notes.
&gt; &gt;
&gt;
&gt;
&gt;         One other thing, before I forget... I'm not sure the
&gt; hadoop-client-minicluster jar is getting built properly.  If you look
&gt; inside, you'll find a real mishmash of things, including files and
&gt; directories with the same names but different cases.  This means it won't
&gt; extract properly on OS X.  (jar xf on that jar file literally stack traces
&gt; on my El Capitan machine. Neat!)


</body></email><email><emailId>20170124045032</emailId><senderName>Chris Douglas</senderName><senderEmail>chris.douglas@gmail.com</senderEmail><timestampReceived>2017-01-24 04:50:32-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>

Thanks for all your work on this, Andrew. It's great to see the 3.x
series moving forward.

If you were willing to modify the release notes and add the LICENSE to
the jar, we don't need to reset the clock on the VOTE, IMO.

What's the issue with the minicluster jar [1]? I tried to reproduce,
but had no issues with 1.8.0_92-b14.

+1 Verified checksums, signature, built src tarball. -C

[1] hadoop-3.0.0-alpha2/share/hadoop/client/hadoop-client-minicluster-3.0.0-alpha2.jar


On Mon, Jan 23, 2017 at 10:51 AM, Andrew Wang &lt;andrew.wang@cloudera.com&gt; wrote:
&gt; There are 5 JIRAs by my count that are missing release notes, which isn't
&gt; that bad but could of course be improved. Four of those I had already
&gt; checked earlier and forgot to follow up, and they were very minorly
&gt; incompatible (affecting private APIs) or mistakenly marked incompatible.
&gt;
&gt; I'm not too concerned about the shaded minicluster since it's a new
&gt; feature, this is an alpha, and we have an IT test against the shaded
&gt; minicluster. Multiple files with the same name are actually also allowed by
&gt; the zip standard, so it's not clear if there is a functionality bug.
&gt;
&gt; Could I get some additional PMC input on this vote? The most critical issue
&gt; in my mind is the missing LICENSE on that one new artifact. If we end up
&gt; spinning a new RC, I'll also handle the missing release notes that Allen
&gt; mentioned.
&gt;
&gt; Thanks,
&gt; Andrew
&gt;
&gt; On Sun, Jan 22, 2017 at 10:45 PM, Allen Wittenauer &lt;aw@effectivemachines.com
&gt;&gt; wrote:
&gt;
&gt;&gt;
&gt;&gt; &gt; On Jan 22, 2017, at 9:05 PM, Allen Wittenauer &lt;aw@effectivemachines.com&gt;
&gt;&gt; wrote:
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;
&gt;&gt; &gt;&gt; On Jan 20, 2017, at 2:36 PM, Andrew Wang &lt;andrew.wang@cloudera.com&gt;
&gt;&gt; wrote:
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; http://home.apache.org/~wang/3.0.0-alpha2-RC0/
&gt;&gt; &gt;
&gt;&gt; &gt;       There are quite a few JIRA issues that need release notes.
&gt;&gt; &gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;         One other thing, before I forget... I'm not sure the
&gt;&gt; hadoop-client-minicluster jar is getting built properly.  If you look
&gt;&gt; inside, you'll find a real mishmash of things, including files and
&gt;&gt; directories with the same names but different cases.  This means it won't
&gt;&gt; extract properly on OS X.  (jar xf on that jar file literally stack traces
&gt;&gt; on my El Capitan machine. Neat!)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170124053202</emailId><senderName>Allen Wittenauer</senderName><senderEmail>aw@effectivemachines.com</senderEmail><timestampReceived>2017-01-24 05:32:02-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>


&gt; On Jan 23, 2017, at 8:50 PM, Chris Douglas &lt;chris.douglas@gmail.com&gt; wrote:
&gt; 
&gt; Thanks for all your work on this, Andrew. It's great to see the 3.x
&gt; series moving forward.
&gt; 
&gt; If you were willing to modify the release notes and add the LICENSE to
&gt; the jar, we don't need to reset the clock on the VOTE, IMO.

FWIW, I wrote a new version of the verify-license-files tool and attached it to \
HADOOP-13374.  This version actually verifies that the license and notice files in \
jars and wars matches the one in base of the (tarball) distribution.

ERROR: hadoop-client-api-3.0.0-alpha3-SNAPSHOT.jar: Missing a LICENSE file
ERROR: hadoop-client-api-3.0.0-alpha3-SNAPSHOT.jar: No valid NOTICE found

WARNING: hadoop-client-minicluster-3.0.0-alpha3-SNAPSHOT.jar: Found 5 LICENSE files \
                (0 were valid)
ERROR: hadoop-client-minicluster-3.0.0-alpha3-SNAPSHOT.jar: No valid LICENSE found
WARNING: hadoop-client-minicluster-3.0.0-alpha3-SNAPSHOT.jar: Found 3 NOTICE files (0 \
                were valid)
ERROR: hadoop-client-minicluster-3.0.0-alpha3-SNAPSHOT.jar: No valid NOTICE found

ERROR: hadoop-client-runtime-3.0.0-alpha3-SNAPSHOT.jar: No valid LICENSE found
ERROR: hadoop-client-runtime-3.0.0-alpha3-SNAPSHOT.jar: No valid NOTICE found

&gt; What's the issue with the minicluster jar [1]? I tried to reproduce,
&gt; but had no issues with 1.8.0_92-b14.

minicluster is kind of weird on filesystems that don't support mixed case, like OS \
X's default HFS+.

$  jar tf hadoop-client-minicluster-3.0.0-alpha3-SNAPSHOT.jar | grep -i license
LICENSE.txt
license/
license/LICENSE
license/LICENSE.dom-documentation.txt
license/LICENSE.dom-software.txt
license/LICENSE.sax.txt
license/NOTICE
license/README.dom.txt
license/README.sax.txt
LICENSE
Grizzly_THIRDPARTYLICENSEREADME.txt



The problem here is that there is a 'license' directory and a file called 'LICENSE'.  \
If this gets extracted by jar via jar xf, it will fail.  unzip can be made to extract \
it via an option like -o.  To make matters worse, none of these license files match \
the one in the generated tarball. :(



---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170124110223</emailId><senderName>Marton Elek</senderName><senderEmail>melek@hortonworks.com</senderEmail><timestampReceived>2017-01-24 11:02:23-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>

]&gt; 
&gt; minicluster is kind of weird on filesystems that don't support mixed case, like OS \
&gt; X's default HFS+. 
&gt; $  jar tf hadoop-client-minicluster-3.0.0-alpha3-SNAPSHOT.jar | grep -i license
&gt; LICENSE.txt
&gt; license/
&gt; license/LICENSE
&gt; license/LICENSE.dom-documentation.txt
&gt; license/LICENSE.dom-software.txt
&gt; license/LICENSE.sax.txt
&gt; license/NOTICE
&gt; license/README.dom.txt
&gt; license/README.sax.txt
&gt; LICENSE
&gt; Grizzly_THIRDPARTYLICENSEREADME.txt


I added a patch to https://issues.apache.org/jira/browse/HADOOP-14018 to add the \
missing META-INF/LICENSE.txt to the shaded files.

Question: what should be done with the other LICENSE files in the minicluster. Can we \
just exclude them (from legal point of view)?

Regards,
Marton

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170124222139</emailId><senderName>Chris Douglas</senderName><senderEmail>chris.douglas@gmail.com</senderEmail><timestampReceived>2017-01-24 22:21:39-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>

On Mon, Jan 23, 2017 at 9:32 PM, Allen Wittenauer
&lt;aw@effectivemachines.com&gt; wrote:
&gt; The problem here is that there is a 'license' directory and a file called=
 'LICENSE'.  If this gets extracted by jar via jar xf, it will fail.  unzip=
 can be made to extract it via an option like -o.  To make matters worse, n=
one of these license files match the one in the generated tarball. :(

Ah, got it. I didn't strip the trailing slash on directories. With
that, it looks like the "license" directory and "LICENSE" file are the
only conflict?

I've not followed the development of packaging LICENSE/NOTICE in the
jar files. AFAIK, it's sufficient that we have the top-level
LICENSE/NOTICE in the tarball. Unless there's a LEGAL thread to the
contrary, it's OK as-is.

Again, I don't think we need to restart the clock on the RC vote if
the release notes and LICENSE/NOTICE were fixed, but it's Andrew's
time and I don't think any of these are blockers for the release. -C

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170124235605</emailId><senderName>Eric Badger</senderName><senderEmail>ebadger@yahoo-inc.com.invalid</senderEmail><timestampReceived>2017-01-24 23:56:05-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>


+1 (non-binding)
- Verified signatures and md5- Built from source- Started single-node cluster on my \
mac- Ran some sleep jobs Eric 

    On Tuesday, January 24, 2017 4:32 PM, Yufei Gu &lt;flyrain000@gmail.com&gt; wrote:
 

 Hi Andrew,

Thanks for working on this.

+1 (Non-Binding)

1. Downloaded the binary and verified the md5.
2. Deployed it on 3 node cluster with 1 ResourceManager and 2 NodeManager.
3. Set YARN to use Fair Scheduler.
4. Ran MapReduce jobs Pi
5. Verified Hadoop version command output is correct.

Best,

Yufei

On Tue, Jan 24, 2017 at 3:02 AM, Marton Elek &lt;melek@hortonworks.com&gt; wrote:

&gt; ]&gt;
&gt; &gt; minicluster is kind of weird on filesystems that don't support mixed
&gt; case, like OS X's default HFS+.
&gt; &gt; 
&gt; &gt; $   jar tf hadoop-client-minicluster-3.0.0-alpha3-SNAPSHOT.jar | grep -i
&gt; license
&gt; &gt; LICENSE.txt
&gt; &gt; license/
&gt; &gt; license/LICENSE
&gt; &gt; license/LICENSE.dom-documentation.txt
&gt; &gt; license/LICENSE.dom-software.txt
&gt; &gt; license/LICENSE.sax.txt
&gt; &gt; license/NOTICE
&gt; &gt; license/README.dom.txt
&gt; &gt; license/README.sax.txt
&gt; &gt; LICENSE
&gt; &gt; Grizzly_THIRDPARTYLICENSEREADME.txt
&gt; 
&gt; 
&gt; I added a patch to https://issues.apache.org/jira/browse/HADOOP-14018 to
&gt; add the missing META-INF/LICENSE.txt to the shaded files.
&gt; 
&gt; Question: what should be done with the other LICENSE files in the
&gt; minicluster. Can we just exclude them (from legal point of view)?
&gt; 
&gt; Regards,
&gt; Marton
&gt; 
&gt; ---------------------------------------------------------------------
&gt; To unsubscribe, e-mail: yarn-dev-unsubscribe@hadoop.apache.org
&gt; For additional commands, e-mail: yarn-dev-help@hadoop.apache.org
&gt; 
&gt; 


   



</body></email><email><emailId>20170125073843</emailId><senderName>Yongjun Zhang</senderName><senderEmail>yjzhangal@apache.org</senderEmail><timestampReceived>2017-01-25 07:38:43-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>


Thanks Andrew much for the work here!

+1 (binding).

- Downloaded both binary and src tarballs
- Verified md5 checksum and signature for both
- Built from source tarball
- Deployed 2 pseudo clusters, one with the released tarball and the other
  with what I built from source, and did the following on both:
      - Run basic HDFS operations, snapshots and distcp jobs
      - Run pi job
      - Examined HDFS webui, YARN webui.

 Best,

 --Yongjun


On Tue, Jan 24, 2017 at 3:56 PM, Eric Badger &lt;ebadger@yahoo-inc.com.invalid&gt;
wrote:

&gt; +1 (non-binding)
&gt; - Verified signatures and md5- Built from source- Started single-node
&gt; cluster on my mac- Ran some sleep jobs
&gt; Eric
&gt;
&gt;     On Tuesday, January 24, 2017 4:32 PM, Yufei Gu &lt;flyrain000@gmail.com&gt;
&gt; wrote:
&gt;
&gt;
&gt;  Hi Andrew,
&gt;
&gt; Thanks for working on this.
&gt;
&gt; +1 (Non-Binding)
&gt;
&gt; 1. Downloaded the binary and verified the md5.
&gt; 2. Deployed it on 3 node cluster with 1 ResourceManager and 2 NodeManager.
&gt; 3. Set YARN to use Fair Scheduler.
&gt; 4. Ran MapReduce jobs Pi
&gt; 5. Verified Hadoop version command output is correct.
&gt;
&gt; Best,
&gt;
&gt; Yufei
&gt;
&gt; On Tue, Jan 24, 2017 at 3:02 AM, Marton Elek &lt;melek@hortonworks.com&gt;
&gt; wrote:
&gt;
&gt; &gt; ]&gt;
&gt; &gt; &gt; minicluster is kind of weird on filesystems that don't support mixed
&gt; &gt; case, like OS X's default HFS+.
&gt; &gt; &gt;
&gt; &gt; &gt; $  jar tf hadoop-client-minicluster-3.0.0-alpha3-SNAPSHOT.jar | grep
&gt; -i
&gt; &gt; license
&gt; &gt; &gt; LICENSE.txt
&gt; &gt; &gt; license/
&gt; &gt; &gt; license/LICENSE
&gt; &gt; &gt; license/LICENSE.dom-documentation.txt
&gt; &gt; &gt; license/LICENSE.dom-software.txt
&gt; &gt; &gt; license/LICENSE.sax.txt
&gt; &gt; &gt; license/NOTICE
&gt; &gt; &gt; license/README.dom.txt
&gt; &gt; &gt; license/README.sax.txt
&gt; &gt; &gt; LICENSE
&gt; &gt; &gt; Grizzly_THIRDPARTYLICENSEREADME.txt
&gt; &gt;
&gt; &gt;
&gt; &gt; I added a patch to https://issues.apache.org/jira/browse/HADOOP-14018 to
&gt; &gt; add the missing META-INF/LICENSE.txt to the shaded files.
&gt; &gt;
&gt; &gt; Question: what should be done with the other LICENSE files in the
&gt; &gt; minicluster. Can we just exclude them (from legal point of view)?
&gt; &gt;
&gt; &gt; Regards,
&gt; &gt; Marton
&gt; &gt;
&gt; &gt; ---------------------------------------------------------------------
&gt; &gt; To unsubscribe, e-mail: yarn-dev-unsubscribe@hadoop.apache.org
&gt; &gt; For additional commands, e-mail: yarn-dev-help@hadoop.apache.org
&gt; &gt;
&gt; &gt;
&gt;
&gt;
&gt;


</body></email><email><emailId>20170125092059</emailId><senderName>Marton Elek</senderName><senderEmail>melek@hortonworks.com</senderEmail><timestampReceived>2017-01-25 09:20:59-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>


Hi,

I also did a quick smoketest with the provided 3.0.0-alpha2 binaries:

TLDR; It works well

Environment:
 * 5 hosts, docker based hadoop cluster, every component in separated container (5 \
                datanode/5 nodemanager/...)
 * Components are:
   * Hdfs/Yarn cluster (upgraded 2.7.3 to 3.0.0-alpha2 using the binary package for \
                vote)
   * Zeppelin 0.6.2/0.7.0-RC2
   * Spark 2.0.2/2.1.0
   * HBase 1.2.4 + zookeeper
   * + additional docker containers for configuration management and monitoring
* No HA, no kerberos, no wire encryption

 * HDFS cluster upgraded successfully from 2.7.3 (with about 200G data)
 * Imported 100G data to HBase successfully
 * Started Spark jobs to process 1G json from HDFS (using spark-master/slave \
cluster). It worked even when I used the Zeppelin 0.6.2 + Spark 2.0.2 (with old \
hadoop client included). Obviously the old version can't use the new Yarn cluster as \
                the token file format has been changed.
 * I upgraded my setup to use Zeppelin 0.7.0-RC2/Spark 2.1.0(distribution without \
hadoop)/hadoop 3.0.0-alpha2. It also worked well: processed the same json files from \
HDFS with spark jobs (from zeppelin) using yarn cluster (master: yarn deploy-mode: \
                cluster)
 * Started spark jobs (with spark submit, master: yarn) to count records from the \
                hbase database: OK
 * Started example Mapreduce jobs from distribution over yarn. It was OK but only \
with specific configuration (see bellow)

So my overall impression that it works very well (at least with my 'smalldata')

Some notes (none of them are blocking):

1. To run the example mapreduce jobs I defined HADOOP_MAPRED_HOME at command line:
./bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-alpha2.jar pi \
-Dyarn.app.mapreduce.am.env="HADOOP_MAPRED_HOME={{HADOOP_COMMON_HOME}}" \
-Dmapreduce.admin.user.env="HADOOP_MAPRED_HOME={{HADOOP_COMMON_HOME}}" 10 10

And in the yarn-site:

yarn.nodemanager.env-whitelist: \
JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,MAPRED_HOME_DIR


I don't know the exact reason for the change, but the 2.7.3 was more userfriendly as \
the example could be run without specific configuration.

For the same reason I didn't start hbase mapreduce job with hbase command line app \
(There could be some option for hbase to define MAPRED_HOME_DIR as well, but by \
default I got ClassNotFoundException for one of the MR class)

2. For the records: The logging and htrace classes are excluded from the shaded \
hadoop client jar so I added it manually one by one to the spark (spark 2.1.0 \
distribution without hadoop):

RUN wget `cat url` -O spark.tar.gz &amp;&amp; tar zxf spark.tar.gz &amp;&amp; rm spark.tar.gz &amp;&amp; mv \
spark* spark RUN cp /opt/hadoop/share/hadoop/client/hadoop-client-api-3.0.0-alpha2.jar \
/opt/spark/jars RUN cp \
/opt/hadoop/share/hadoop/client/hadoop-client-runtime-3.0.0-alpha2.jar \
/opt/spark/jars ADD https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar \
/opt/spark/jars ADD https://repo1.maven.org/maven2/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar \
/opt/spark/jars ADD https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.10/slf4j-api-1.7.10.jar \
/opt/spark/jars/ ADD \
https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar /opt/spark/jars

With this jars files spark 2.1.0 works well with the alpha2 version of HDFS and YARN.

3. The messages "Upgrade in progress. Not yet finalized." wasn't disappeared from the \
namenode webui but the cluster works well.

Most probably I missed to do something, but it's a little bit confusing.

(I checked the REST call, it is the jmx bean who reports that it was not yet \
finalized, the code of the webpage seems to be ok.)

Regards
Marton

On Jan 25, 2017, at 8:38 AM, Yongjun Zhang \
&lt;yjzhangal@apache.org&lt;mailto:yjzhangal@apache.org&gt;&gt; wrote:

Thanks Andrew much for the work here!

+1 (binding).

- Downloaded both binary and src tarballs
- Verified md5 checksum and signature for both
- Built from source tarball
- Deployed 2 pseudo clusters, one with the released tarball and the other
 with what I built from source, and did the following on both:
     - Run basic HDFS operations, snapshots and distcp jobs
     - Run pi job
     - Examined HDFS webui, YARN webui.

Best,

--Yongjun


On Tue, Jan 24, 2017 at 3:56 PM, Eric Badger \
&lt;ebadger@yahoo-inc.com.invalid&lt;mailto:ebadger@yahoo-inc.com.invalid&gt;&gt; wrote:

+1 (non-binding)
- Verified signatures and md5- Built from source- Started single-node
cluster on my mac- Ran some sleep jobs
Eric

   On Tuesday, January 24, 2017 4:32 PM, Yufei Gu \
&lt;flyrain000@gmail.com&lt;mailto:flyrain000@gmail.com&gt;&gt; wrote:


Hi Andrew,

Thanks for working on this.

+1 (Non-Binding)

1. Downloaded the binary and verified the md5.
2. Deployed it on 3 node cluster with 1 ResourceManager and 2 NodeManager.
3. Set YARN to use Fair Scheduler.
4. Ran MapReduce jobs Pi
5. Verified Hadoop version command output is correct.

Best,

Yufei

On Tue, Jan 24, 2017 at 3:02 AM, Marton Elek \
&lt;melek@hortonworks.com&lt;mailto:melek@hortonworks.com&gt;&gt; wrote:

]&gt;
minicluster is kind of weird on filesystems that don't support mixed
case, like OS X's default HFS+.

$  jar tf hadoop-client-minicluster-3.0.0-alpha3-SNAPSHOT.jar | grep
-i
license
LICENSE.txt
license/
license/LICENSE
license/LICENSE.dom-documentation.txt
license/LICENSE.dom-software.txt
license/LICENSE.sax.txt
license/NOTICE
license/README.dom.txt
license/README.sax.txt
LICENSE
Grizzly_THIRDPARTYLICENSEREADME.txt


I added a patch to https://issues.apache.org/jira/browse/HADOOP-14018 to
add the missing META-INF/LICENSE.txt to the shaded files.

Question: what should be done with the other LICENSE files in the
minicluster. Can we just exclude them (from legal point of view)?

Regards,
Marton

---------------------------------------------------------------------
To unsubscribe, e-mail: \
yarn-dev-unsubscribe@hadoop.apache.org&lt;mailto:yarn-dev-unsubscribe@hadoop.apache.org&gt; \
For additional commands, e-mail: \
yarn-dev-help@hadoop.apache.org&lt;mailto:yarn-dev-help@hadoop.apache.org&gt;



</body></email><email><emailId>20170125160703</emailId><senderName>Kuhu Shukla</senderName><senderEmail>kshukla@yahoo-inc.com.invalid</senderEmail><timestampReceived>2017-01-25 16:07:03-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>


+1 (non-binding)* Built from source* Deployed on a pseudo-distributed cluster (MAC)* \
Ran wordcount and sleep jobs.  

    On Wednesday, January 25, 2017 3:21 AM, Marton Elek &lt;melek@hortonworks.com&gt; \
wrote:  

 Hi,

I also did a quick smoketest with the provided 3.0.0-alpha2 binaries:

TLDR; It works well

Environment:
 * 5 hosts, docker based hadoop cluster, every component in separated container (5 \
                datanode/5 nodemanager/...)
 * Components are:
   * Hdfs/Yarn cluster (upgraded 2.7.3 to 3.0.0-alpha2 using the binary package for \
                vote)
   * Zeppelin 0.6.2/0.7.0-RC2
   * Spark 2.0.2/2.1.0
   * HBase 1.2.4 + zookeeper
   * + additional docker containers for configuration management and monitoring
* No HA, no kerberos, no wire encryption

 * HDFS cluster upgraded successfully from 2.7.3 (with about 200G data)
 * Imported 100G data to HBase successfully
 * Started Spark jobs to process 1G json from HDFS (using spark-master/slave \
cluster). It worked even when I used the Zeppelin 0.6.2 + Spark 2.0.2 (with old \
hadoop client included). Obviously the old version can't use the new Yarn cluster as \
                the token file format has been changed.
 * I upgraded my setup to use Zeppelin 0.7.0-RC2/Spark 2.1.0(distribution without \
hadoop)/hadoop 3.0.0-alpha2. It also worked well: processed the same json files from \
HDFS with spark jobs (from zeppelin) using yarn cluster (master: yarn deploy-mode: \
                cluster)
 * Started spark jobs (with spark submit, master: yarn) to count records from the \
                hbase database: OK
 * Started example Mapreduce jobs from distribution over yarn. It was OK but only \
with specific configuration (see bellow)

So my overall impression that it works very well (at least with my 'smalldata')

Some notes (none of them are blocking):

1. To run the example mapreduce jobs I defined HADOOP_MAPRED_HOME at command line:
./bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-alpha2.jar pi \
-Dyarn.app.mapreduce.am.env="HADOOP_MAPRED_HOME={{HADOOP_COMMON_HOME}}" \
-Dmapreduce.admin.user.env="HADOOP_MAPRED_HOME={{HADOOP_COMMON_HOME}}" 10 10

And in the yarn-site:

yarn.nodemanager.env-whitelist: \
JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,MAPRED_HOME_DIR


I don't know the exact reason for the change, but the 2.7.3 was more userfriendly as \
the example could be run without specific configuration.

For the same reason I didn't start hbase mapreduce job with hbase command line app \
(There could be some option for hbase to define MAPRED_HOME_DIR as well, but by \
default I got ClassNotFoundException for one of the MR class)

2. For the records: The logging and htrace classes are excluded from the shaded \
hadoop client jar so I added it manually one by one to the spark (spark 2.1.0 \
distribution without hadoop):

RUN wget `cat url` -O spark.tar.gz &amp;&amp; tar zxf spark.tar.gz &amp;&amp; rm spark.tar.gz &amp;&amp; mv \
spark* spark RUN cp /opt/hadoop/share/hadoop/client/hadoop-client-api-3.0.0-alpha2.jar \
/opt/spark/jars RUN cp \
/opt/hadoop/share/hadoop/client/hadoop-client-runtime-3.0.0-alpha2.jar \
/opt/spark/jars ADD https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar \
/opt/spark/jars ADD https://repo1.maven.org/maven2/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar \
/opt/spark/jars ADD https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.10/slf4j-api-1.7.10.jar \
/opt/spark/jars/ ADD \
https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar /opt/spark/jars

With this jars files spark 2.1.0 works well with the alpha2 version of HDFS and YARN.

3. The messages "Upgrade in progress. Not yet finalized." wasn't disappeared from the \
namenode webui but the cluster works well.

Most probably I missed to do something, but it's a little bit confusing.

(I checked the REST call, it is the jmx bean who reports that it was not yet \
finalized, the code of the webpage seems to be ok.)

Regards
Marton

On Jan 25, 2017, at 8:38 AM, Yongjun Zhang \
&lt;yjzhangal@apache.org&lt;mailto:yjzhangal@apache.org&gt;&gt; wrote:

Thanks Andrew much for the work here!

+1 (binding).

- Downloaded both binary and src tarballs
- Verified md5 checksum and signature for both
- Built from source tarball
- Deployed 2 pseudo clusters, one with the released tarball and the other
 with what I built from source, and did the following on both:
      - Run basic HDFS operations, snapshots and distcp jobs
      - Run pi job
      - Examined HDFS webui, YARN webui.

Best,

--Yongjun


On Tue, Jan 24, 2017 at 3:56 PM, Eric Badger \
&lt;ebadger@yahoo-inc.com.invalid&lt;mailto:ebadger@yahoo-inc.com.invalid&gt;&gt; wrote:

+1 (non-binding)
- Verified signatures and md5- Built from source- Started single-node
cluster on my mac- Ran some sleep jobs
Eric

   On Tuesday, January 24, 2017 4:32 PM, Yufei Gu \
&lt;flyrain000@gmail.com&lt;mailto:flyrain000@gmail.com&gt;&gt; wrote:


Hi Andrew,

Thanks for working on this.

+1 (Non-Binding)

1. Downloaded the binary and verified the md5.
2. Deployed it on 3 node cluster with 1 ResourceManager and 2 NodeManager.
3. Set YARN to use Fair Scheduler.
4. Ran MapReduce jobs Pi
5. Verified Hadoop version command output is correct.

Best,

Yufei

On Tue, Jan 24, 2017 at 3:02 AM, Marton Elek \
&lt;melek@hortonworks.com&lt;mailto:melek@hortonworks.com&gt;&gt; wrote:

]&gt;
minicluster is kind of weird on filesystems that don't support mixed
case, like OS X's default HFS+.

$   jar tf hadoop-client-minicluster-3.0.0-alpha3-SNAPSHOT.jar | grep
-i
license
LICENSE.txt
license/
license/LICENSE
license/LICENSE.dom-documentation.txt
license/LICENSE.dom-software.txt
license/LICENSE.sax.txt
license/NOTICE
license/README.dom.txt
license/README.sax.txt
LICENSE
Grizzly_THIRDPARTYLICENSEREADME.txt


I added a patch to https://issues.apache.org/jira/browse/HADOOP-14018 to
add the missing META-INF/LICENSE.txt to the shaded files.

Question: what should be done with the other LICENSE files in the
minicluster. Can we just exclude them (from legal point of view)?

Regards,
Marton

---------------------------------------------------------------------
To unsubscribe, e-mail: \
yarn-dev-unsubscribe@hadoop.apache.org&lt;mailto:yarn-dev-unsubscribe@hadoop.apache.org&gt; \
For additional commands, e-mail: \
yarn-dev-help@hadoop.apache.org&lt;mailto:yarn-dev-help@hadoop.apache.org&gt;







   



</body></email><email><emailId>20170125174126</emailId><senderName>Zhihai Xu</senderName><senderEmail>zhihaixu2012@gmail.com</senderEmail><timestampReceived>2017-01-25 17:41:26-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>


Thanks Andrew for creating release Hadoop 3.0.0-alpha2 RC0
+1 ( non-binding)

--Downloaded source and built from it.
--Deployed on a pseudo-distributed cluster.
--Ran sample MR jobs and tested with basics HDFS operations.
--Did a sanity check for RM and NM UI.

Best,
zhihai

On Wed, Jan 25, 2017 at 8:07 AM, Kuhu Shukla &lt;kshukla@yahoo-inc.com.invalid&gt;
wrote:

&gt; +1 (non-binding)* Built from source* Deployed on a pseudo-distributed
&gt; cluster (MAC)* Ran wordcount and sleep jobs.
&gt;
&gt;
&gt;     On Wednesday, January 25, 2017 3:21 AM, Marton Elek &lt;
&gt; melek@hortonworks.com&gt; wrote:
&gt;
&gt;
&gt;  Hi,
&gt;
&gt; I also did a quick smoketest with the provided 3.0.0-alpha2 binaries:
&gt;
&gt; TLDR; It works well
&gt;
&gt; Environment:
&gt;  * 5 hosts, docker based hadoop cluster, every component in separated
&gt; container (5 datanode/5 nodemanager/...)
&gt;  * Components are:
&gt;   * Hdfs/Yarn cluster (upgraded 2.7.3 to 3.0.0-alpha2 using the binary
&gt; package for vote)
&gt;   * Zeppelin 0.6.2/0.7.0-RC2
&gt;   * Spark 2.0.2/2.1.0
&gt;   * HBase 1.2.4 + zookeeper
&gt;   * + additional docker containers for configuration management and
&gt; monitoring
&gt; * No HA, no kerberos, no wire encryption
&gt;
&gt;  * HDFS cluster upgraded successfully from 2.7.3 (with about 200G data)
&gt;  * Imported 100G data to HBase successfully
&gt;  * Started Spark jobs to process 1G json from HDFS (using
&gt; spark-master/slave cluster). It worked even when I used the Zeppelin 0.6.2
&gt; + Spark 2.0.2 (with old hadoop client included). Obviously the old version
&gt; can't use the new Yarn cluster as the token file format has been changed.
&gt;  * I upgraded my setup to use Zeppelin 0.7.0-RC2/Spark 2.1.0(distribution
&gt; without hadoop)/hadoop 3.0.0-alpha2. It also worked well: processed the
&gt; same json files from HDFS with spark jobs (from zeppelin) using yarn
&gt; cluster (master: yarn deploy-mode: cluster)
&gt;  * Started spark jobs (with spark submit, master: yarn) to count records
&gt; from the hbase database: OK
&gt;  * Started example Mapreduce jobs from distribution over yarn. It was OK
&gt; but only with specific configuration (see bellow)
&gt;
&gt; So my overall impression that it works very well (at least with my
&gt; 'smalldata')
&gt;
&gt; Some notes (none of them are blocking):
&gt;
&gt; 1. To run the example mapreduce jobs I defined HADOOP_MAPRED_HOME at
&gt; command line:
&gt; ./bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-alpha2.jar
&gt; pi -Dyarn.app.mapreduce.am.env="HADOOP_MAPRED_HOME={{HADOOP_COMMON_HOME}}"
&gt; -Dmapreduce.admin.user.env="HADOOP_MAPRED_HOME={{HADOOP_COMMON_HOME}}" 10
&gt; 10
&gt;
&gt; And in the yarn-site:
&gt;
&gt; yarn.nodemanager.env-whitelist: JAVA_HOME,HADOOP_COMMON_HOME,
&gt; HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_
&gt; DISTCACHE,HADOOP_YARN_HOME,MAPRED_HOME_DIR
&gt;
&gt; I don't know the exact reason for the change, but the 2.7.3 was more
&gt; userfriendly as the example could be run without specific configuration.
&gt;
&gt; For the same reason I didn't start hbase mapreduce job with hbase command
&gt; line app (There could be some option for hbase to define MAPRED_HOME_DIR as
&gt; well, but by default I got ClassNotFoundException for one of the MR class)
&gt;
&gt; 2. For the records: The logging and htrace classes are excluded from the
&gt; shaded hadoop client jar so I added it manually one by one to the spark
&gt; (spark 2.1.0 distribution without hadoop):
&gt;
&gt; RUN wget `cat url` -O spark.tar.gz &amp;&amp; tar zxf spark.tar.gz &amp;&amp; rm
&gt; spark.tar.gz &amp;&amp; mv spark* spark
&gt; RUN cp /opt/hadoop/share/hadoop/client/hadoop-client-api-3.0.0-alpha2.jar
&gt; /opt/spark/jars
&gt; RUN cp /opt/hadoop/share/hadoop/client/hadoop-client-runtime-3.0.0-alpha2.jar
&gt; /opt/spark/jars
&gt; ADD https://repo1.maven.org/maven2/org/slf4j/slf4j-
&gt; log4j12/1.7.10/slf4j-log4j12-1.7.10.jar /opt/spark/jars
&gt; ADD https://repo1.maven.org/maven2/org/apache/htrace/
&gt; htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar
&gt; /opt/spark/jars
&gt; ADD https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.
&gt; 7.10/slf4j-api-1.7.10.jar /opt/spark/jars/
&gt; ADD https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar
&gt; /opt/spark/jars
&gt;
&gt; With this jars files spark 2.1.0 works well with the alpha2 version of
&gt; HDFS and YARN.
&gt;
&gt; 3. The messages "Upgrade in progress. Not yet finalized." wasn't
&gt; disappeared from the namenode webui but the cluster works well.
&gt;
&gt; Most probably I missed to do something, but it's a little bit confusing.
&gt;
&gt; (I checked the REST call, it is the jmx bean who reports that it was not
&gt; yet finalized, the code of the webpage seems to be ok.)
&gt;
&gt; Regards
&gt; Marton
&gt;
&gt; On Jan 25, 2017, at 8:38 AM, Yongjun Zhang &lt;yjzhangal@apache.org&lt;mailto:y
&gt; jzhangal@apache.org&gt;&gt; wrote:
&gt;
&gt; Thanks Andrew much for the work here!
&gt;
&gt; +1 (binding).
&gt;
&gt; - Downloaded both binary and src tarballs
&gt; - Verified md5 checksum and signature for both
&gt; - Built from source tarball
&gt; - Deployed 2 pseudo clusters, one with the released tarball and the other
&gt;  with what I built from source, and did the following on both:
&gt;     - Run basic HDFS operations, snapshots and distcp jobs
&gt;     - Run pi job
&gt;     - Examined HDFS webui, YARN webui.
&gt;
&gt; Best,
&gt;
&gt; --Yongjun
&gt;
&gt;
&gt; On Tue, Jan 24, 2017 at 3:56 PM, Eric Badger &lt;ebadger@yahoo-inc.com.invalid
&gt; &lt;mailto:ebadger@yahoo-inc.com.invalid&gt;&gt;
&gt; wrote:
&gt;
&gt; +1 (non-binding)
&gt; - Verified signatures and md5- Built from source- Started single-node
&gt; cluster on my mac- Ran some sleep jobs
&gt; Eric
&gt;
&gt;   On Tuesday, January 24, 2017 4:32 PM, Yufei Gu &lt;flyrain000@gmail.com
&gt; &lt;mailto:flyrain000@gmail.com&gt;&gt;
&gt; wrote:
&gt;
&gt;
&gt; Hi Andrew,
&gt;
&gt; Thanks for working on this.
&gt;
&gt; +1 (Non-Binding)
&gt;
&gt; 1. Downloaded the binary and verified the md5.
&gt; 2. Deployed it on 3 node cluster with 1 ResourceManager and 2 NodeManager.
&gt; 3. Set YARN to use Fair Scheduler.
&gt; 4. Ran MapReduce jobs Pi
&gt; 5. Verified Hadoop version command output is correct.
&gt;
&gt; Best,
&gt;
&gt; Yufei
&gt;
&gt; On Tue, Jan 24, 2017 at 3:02 AM, Marton Elek &lt;melek@hortonworks.com
&gt; &lt;mailto:melek@hortonworks.com&gt;&gt;
&gt; wrote:
&gt;
&gt; ]&gt;
&gt; minicluster is kind of weird on filesystems that don't support mixed
&gt; case, like OS X's default HFS+.
&gt;
&gt; $  jar tf hadoop-client-minicluster-3.0.0-alpha3-SNAPSHOT.jar | grep
&gt; -i
&gt; license
&gt; LICENSE.txt
&gt; license/
&gt; license/LICENSE
&gt; license/LICENSE.dom-documentation.txt
&gt; license/LICENSE.dom-software.txt
&gt; license/LICENSE.sax.txt
&gt; license/NOTICE
&gt; license/README.dom.txt
&gt; license/README.sax.txt
&gt; LICENSE
&gt; Grizzly_THIRDPARTYLICENSEREADME.txt
&gt;
&gt;
&gt; I added a patch to https://issues.apache.org/jira/browse/HADOOP-14018 to
&gt; add the missing META-INF/LICENSE.txt to the shaded files.
&gt;
&gt; Question: what should be done with the other LICENSE files in the
&gt; minicluster. Can we just exclude them (from legal point of view)?
&gt;
&gt; Regards,
&gt; Marton
&gt;
&gt; ---------------------------------------------------------------------
&gt; To unsubscribe, e-mail: yarn-dev-unsubscribe@hadoop.apache.org&lt;mailto:
&gt; yarn-dev-unsubscribe@hadoop.apache.org&gt;
&gt; For additional commands, e-mail: yarn-dev-help@hadoop.apache.org&lt;mailto:
&gt; yarn-dev-help@hadoop.apache.org&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;


</body></email><email><emailId>20170125185359</emailId><senderName>Xiao Chen</senderName><senderEmail>xiao@cloudera.com</senderEmail><timestampReceived>2017-01-25 18:53:59-0400</timestampReceived><subject>Re: [VOTE] Release Apache Hadoop 3.0.0-alpha2 RC0</subject><body>


Thanks Andrew and the community to work out the alpha2 RC!

+1 (non-binding)

   - Built the source tarball
   - Tested on a pseudo-distributed cluster, basic HDFS operations/sample
   pi job over HDFS encryption zone work.
   - Sanity checked NN and KMS webui
   - Sanity checked NN/DN/KMS logs.


-Xiao

On Wed, Jan 25, 2017 at 9:41 AM, Zhihai Xu &lt;zhihaixu2012@gmail.com&gt; wrote:

&gt; Thanks Andrew for creating release Hadoop 3.0.0-alpha2 RC0
&gt; +1 ( non-binding)
&gt;
&gt; --Downloaded source and built from it.
&gt; --Deployed on a pseudo-distributed cluster.
&gt; --Ran sample MR jobs and tested with basics HDFS operations.
&gt; --Did a sanity check for RM and NM UI.
&gt;
&gt; Best,
&gt; zhihai
&gt;
&gt; On Wed, Jan 25, 2017 at 8:07 AM, Kuhu Shukla &lt;kshukla@yahoo-inc.com.invalid
&gt; &gt;
&gt; wrote:
&gt;
&gt; &gt; +1 (non-binding)* Built from source* Deployed on a pseudo-distributed
&gt; &gt; cluster (MAC)* Ran wordcount and sleep jobs.
&gt; &gt;
&gt; &gt;
&gt; &gt;     On Wednesday, January 25, 2017 3:21 AM, Marton Elek &lt;
&gt; &gt; melek@hortonworks.com&gt; wrote:
&gt; &gt;
&gt; &gt;
&gt; &gt;  Hi,
&gt; &gt;
&gt; &gt; I also did a quick smoketest with the provided 3.0.0-alpha2 binaries:
&gt; &gt;
&gt; &gt; TLDR; It works well
&gt; &gt;
&gt; &gt; Environment:
&gt; &gt;  * 5 hosts, docker based hadoop cluster, every component in separated
&gt; &gt; container (5 datanode/5 nodemanager/...)
&gt; &gt;  * Components are:
&gt; &gt;   * Hdfs/Yarn cluster (upgraded 2.7.3 to 3.0.0-alpha2 using the binary
&gt; &gt; package for vote)
&gt; &gt;   * Zeppelin 0.6.2/0.7.0-RC2
&gt; &gt;   * Spark 2.0.2/2.1.0
&gt; &gt;   * HBase 1.2.4 + zookeeper
&gt; &gt;   * + additional docker containers for configuration management and
&gt; &gt; monitoring
&gt; &gt; * No HA, no kerberos, no wire encryption
&gt; &gt;
&gt; &gt;  * HDFS cluster upgraded successfully from 2.7.3 (with about 200G data)
&gt; &gt;  * Imported 100G data to HBase successfully
&gt; &gt;  * Started Spark jobs to process 1G json from HDFS (using
&gt; &gt; spark-master/slave cluster). It worked even when I used the Zeppelin
&gt; 0.6.2
&gt; &gt; + Spark 2.0.2 (with old hadoop client included). Obviously the old
&gt; version
&gt; &gt; can't use the new Yarn cluster as the token file format has been changed.
&gt; &gt;  * I upgraded my setup to use Zeppelin 0.7.0-RC2/Spark 2.1.0(distribution
&gt; &gt; without hadoop)/hadoop 3.0.0-alpha2. It also worked well: processed the
&gt; &gt; same json files from HDFS with spark jobs (from zeppelin) using yarn
&gt; &gt; cluster (master: yarn deploy-mode: cluster)
&gt; &gt;  * Started spark jobs (with spark submit, master: yarn) to count records
&gt; &gt; from the hbase database: OK
&gt; &gt;  * Started example Mapreduce jobs from distribution over yarn. It was OK
&gt; &gt; but only with specific configuration (see bellow)
&gt; &gt;
&gt; &gt; So my overall impression that it works very well (at least with my
&gt; &gt; 'smalldata')
&gt; &gt;
&gt; &gt; Some notes (none of them are blocking):
&gt; &gt;
&gt; &gt; 1. To run the example mapreduce jobs I defined HADOOP_MAPRED_HOME at
&gt; &gt; command line:
&gt; &gt; ./bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-
&gt; alpha2.jar
&gt; &gt; pi -Dyarn.app.mapreduce.am.env="HADOOP_MAPRED_HOME={{HADOOP_
&gt; COMMON_HOME}}"
&gt; &gt; -Dmapreduce.admin.user.env="HADOOP_MAPRED_HOME={{HADOOP_COMMON_HOME}}"
&gt; 10
&gt; &gt; 10
&gt; &gt;
&gt; &gt; And in the yarn-site:
&gt; &gt;
&gt; &gt; yarn.nodemanager.env-whitelist: JAVA_HOME,HADOOP_COMMON_HOME,
&gt; &gt; HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_
&gt; &gt; DISTCACHE,HADOOP_YARN_HOME,MAPRED_HOME_DIR
&gt; &gt;
&gt; &gt; I don't know the exact reason for the change, but the 2.7.3 was more
&gt; &gt; userfriendly as the example could be run without specific configuration.
&gt; &gt;
&gt; &gt; For the same reason I didn't start hbase mapreduce job with hbase command
&gt; &gt; line app (There could be some option for hbase to define MAPRED_HOME_DIR
&gt; as
&gt; &gt; well, but by default I got ClassNotFoundException for one of the MR
&gt; class)
&gt; &gt;
&gt; &gt; 2. For the records: The logging and htrace classes are excluded from the
&gt; &gt; shaded hadoop client jar so I added it manually one by one to the spark
&gt; &gt; (spark 2.1.0 distribution without hadoop):
&gt; &gt;
&gt; &gt; RUN wget `cat url` -O spark.tar.gz &amp;&amp; tar zxf spark.tar.gz &amp;&amp; rm
&gt; &gt; spark.tar.gz &amp;&amp; mv spark* spark
&gt; &gt; RUN cp /opt/hadoop/share/hadoop/client/hadoop-client-api-3.0.
&gt; 0-alpha2.jar
&gt; &gt; /opt/spark/jars
&gt; &gt; RUN cp /opt/hadoop/share/hadoop/client/hadoop-client-runtime-
&gt; 3.0.0-alpha2.jar
&gt; &gt; /opt/spark/jars
&gt; &gt; ADD https://repo1.maven.org/maven2/org/slf4j/slf4j-
&gt; &gt; log4j12/1.7.10/slf4j-log4j12-1.7.10.jar /opt/spark/jars
&gt; &gt; ADD https://repo1.maven.org/maven2/org/apache/htrace/
&gt; &gt; htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar
&gt; &gt; /opt/spark/jars
&gt; &gt; ADD https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.
&gt; &gt; 7.10/slf4j-api-1.7.10.jar /opt/spark/jars/
&gt; &gt; ADD https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar
&gt; &gt; /opt/spark/jars
&gt; &gt;
&gt; &gt; With this jars files spark 2.1.0 works well with the alpha2 version of
&gt; &gt; HDFS and YARN.
&gt; &gt;
&gt; &gt; 3. The messages "Upgrade in progress. Not yet finalized." wasn't
&gt; &gt; disappeared from the namenode webui but the cluster works well.
&gt; &gt;
&gt; &gt; Most probably I missed to do something, but it's a little bit confusing.
&gt; &gt;
&gt; &gt; (I checked the REST call, it is the jmx bean who reports that it was not
&gt; &gt; yet finalized, the code of the webpage seems to be ok.)
&gt; &gt;
&gt; &gt; Regards
&gt; &gt; Marton
&gt; &gt;
&gt; &gt; On Jan 25, 2017, at 8:38 AM, Yongjun Zhang &lt;yjzhangal@apache.org&lt;mailto:
&gt; y
&gt; &gt; jzhangal@apache.org&gt;&gt; wrote:
&gt; &gt;
&gt; &gt; Thanks Andrew much for the work here!
&gt; &gt;
&gt; &gt; +1 (binding).
&gt; &gt;
&gt; &gt; - Downloaded both binary and src tarballs
&gt; &gt; - Verified md5 checksum and signature for both
&gt; &gt; - Built from source tarball
&gt; &gt; - Deployed 2 pseudo clusters, one with the released tarball and the other
&gt; &gt;  with what I built from source, and did the following on both:
&gt; &gt;     - Run basic HDFS operations, snapshots and distcp jobs
&gt; &gt;     - Run pi job
&gt; &gt;     - Examined HDFS webui, YARN webui.
&gt; &gt;
&gt; &gt; Best,
&gt; &gt;
&gt; &gt; --Yongjun
&gt; &gt;
&gt; &gt;
&gt; &gt; On Tue, Jan 24, 2017 at 3:56 PM, Eric Badger
&gt; &lt;ebadger@yahoo-inc.com.invalid
&gt; &gt; &lt;mailto:ebadger@yahoo-inc.com.invalid&gt;&gt;
&gt; &gt; wrote:
&gt; &gt;
&gt; &gt; +1 (non-binding)
&gt; &gt; - Verified signatures and md5- Built from source- Started single-node
&gt; &gt; cluster on my mac- Ran some sleep jobs
&gt; &gt; Eric
&gt; &gt;
&gt; &gt;   On Tuesday, January 24, 2017 4:32 PM, Yufei Gu &lt;flyrain000@gmail.com
&gt; &gt; &lt;mailto:flyrain000@gmail.com&gt;&gt;
&gt; &gt; wrote:
&gt; &gt;
&gt; &gt;
&gt; &gt; Hi Andrew,
&gt; &gt;
&gt; &gt; Thanks for working on this.
&gt; &gt;
&gt; &gt; +1 (Non-Binding)
&gt; &gt;
&gt; &gt; 1. Downloaded the binary and verified the md5.
&gt; &gt; 2. Deployed it on 3 node cluster with 1 ResourceManager and 2
&gt; NodeManager.
&gt; &gt; 3. Set YARN to use Fair Scheduler.
&gt; &gt; 4. Ran MapReduce jobs Pi
&gt; &gt; 5. Verified Hadoop version command output is correct.
&gt; &gt;
&gt; &gt; Best,
&gt; &gt;
&gt; &gt; Yufei
&gt; &gt;
&gt; &gt; On Tue, Jan 24, 2017 at 3:02 AM, Marton Elek &lt;melek@hortonworks.com
&gt; &gt; &lt;mailto:melek@hortonworks.com&gt;&gt;
&gt; &gt; wrote:
&gt; &gt;
&gt; &gt; ]&gt;
&gt; &gt; minicluster is kind of weird on filesystems that don't support mixed
&gt; &gt; case, like OS X's default HFS+.
&gt; &gt;
&gt; &gt; $  jar tf hadoop-client-minicluster-3.0.0-alpha3-SNAPSHOT.jar | grep
&gt; &gt; -i
&gt; &gt; license
&gt; &gt; LICENSE.txt
&gt; &gt; license/
&gt; &gt; license/LICENSE
&gt; &gt; license/LICENSE.dom-documentation.txt
&gt; &gt; license/LICENSE.dom-software.txt
&gt; &gt; license/LICENSE.sax.txt
&gt; &gt; license/NOTICE
&gt; &gt; license/README.dom.txt
&gt; &gt; license/README.sax.txt
&gt; &gt; LICENSE
&gt; &gt; Grizzly_THIRDPARTYLICENSEREADME.txt
&gt; &gt;
&gt; &gt;
&gt; &gt; I added a patch to https://issues.apache.org/jira/browse/HADOOP-14018 to
&gt; &gt; add the missing META-INF/LICENSE.txt to the shaded files.
&gt; &gt;
&gt; &gt; Question: what should be done with the other LICENSE files in the
&gt; &gt; minicluster. Can we just exclude them (from legal point of view)?
&gt; &gt;
&gt; &gt; Regards,
&gt; &gt; Marton
&gt; &gt;
&gt; &gt; ---------------------------------------------------------------------
&gt; &gt; To unsubscribe, e-mail: yarn-dev-unsubscribe@hadoop.apache.org&lt;mailto:
&gt; &gt; yarn-dev-unsubscribe@hadoop.apache.org&gt;
&gt; &gt; For additional commands, e-mail: yarn-dev-help@hadoop.apache.org&lt;mailto:
&gt; &gt; yarn-dev-help@hadoop.apache.org&gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt;


</body></email><email><emailId>20170403173744</emailId><senderName>Esteban Gutierrez</senderName><senderEmail>esteban@cloudera.com</senderEmail><timestampReceived>2017-04-03 17:37:44-0400</timestampReceived><subject>Re: [DISCUSS] Changing the default class path for clients</subject><body>


I agreed with Andrew too. Users have relied for years on `hadoop classpath`
for their script to launch jobs or other tools, perhaps no the best idea to
change the behavior without providing a proper deprecation path.

thanks!
esteban.

--
Cloudera, Inc.


On Mon, Apr 3, 2017 at 10:26 AM, Andrew Wang &lt;andrew.wang@cloudera.com&gt;
wrote:

&gt; What's the current contract for `hadoop classpath`? Would it be safer to
&gt; introduce `hadoop userclasspath` or similar for this behavior?
&gt;
&gt; I'm betting that changing `hadoop classpath` will lead to some breakages,
&gt; so I'd prefer to make this new behavior opt-in.
&gt;
&gt; Best,
&gt; Andrew
&gt;
&gt; On Mon, Apr 3, 2017 at 9:04 AM, Allen Wittenauer &lt;aw@effectivemachines.com
&gt; &gt;
&gt; wrote:
&gt;
&gt; &gt;
&gt; &gt;         This morning I had a bit of a shower thought:
&gt; &gt;
&gt; &gt;         With the new shaded hadoop client in 3.0, is there any reason the
&gt; &gt; default classpath should remain the full blown jar list?  e.g., shouldn't
&gt; &gt; ‘hadoop classpath' just return configuration, user supplied bits (e.g.,
&gt; &gt; HADOOP_USER_CLASSPATH, etc), HADOOP_OPTIONAL_TOOLS, and
&gt; &gt; hadoop-client-runtime? We'd obviously have to add some plumbing for
&gt; daemons
&gt; &gt; and the capability for the user to get the full list, but that should be
&gt; &gt; trivial.
&gt; &gt;
&gt; &gt;         Thoughts?
&gt; &gt; ---------------------------------------------------------------------
&gt; &gt; To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
&gt; &gt; For additional commands, e-mail: common-dev-help@hadoop.apache.org
&gt; &gt;
&gt; &gt;
&gt;


</body></email><email><emailId>20170403180844</emailId><senderName>Allen Wittenauer</senderName><senderEmail>aw@effectivemachines.com</senderEmail><timestampReceived>2017-04-03 18:08:44-0400</timestampReceived><subject>Re: [DISCUSS] Changing the default class path for clients</subject><body>


1.0.4:

	"Prints the class path needed to get the Hadoop jar and the required libraries."

 2.8.0 and 3.0.0:

	"Prints the class path needed to get the Hadoop jar and the required libraries. If \
called without arguments, then prints the classpath set up by the command scripts, \
which is likely to contain wildcards in the classpath entries."

	I would take that to mean "what gives me all the public APIs?"  Which, by \
definition, should all be in hadoop-client-runtime (with the possible exception of \
the DistributedFileSystem Quota APIs, since for some reason those are marked public.) \


Let me ask it a different way:

	Why should ‘yarn jar', ‘mapred jar', ‘hadoop distcp', ‘hadoop fs', etc, etc, \
etc, have anything but hadoop-client-runtime as the provided jar? Yes, some things \
might break, but given this is 3.0, some changes should be expected anyway. Given the \
definition above "needed to get the Hadoop jar and the required libraries"  switching \
this over seems correct.  


&gt; On Apr 3, 2017, at 10:37 AM, Esteban Gutierrez &lt;esteban@cloudera.com&gt; wrote:
&gt; 
&gt; 
&gt; I agreed with Andrew too. Users have relied for years on `hadoop classpath` for \
&gt; their script to launch jobs or other tools, perhaps no the best idea to change the \
&gt; behavior without providing a proper deprecation path. 
&gt; thanks!
&gt; esteban.
&gt; 
&gt; --
&gt; Cloudera, Inc.
&gt; 
&gt; 
&gt; On Mon, Apr 3, 2017 at 10:26 AM, Andrew Wang &lt;andrew.wang@cloudera.com&gt; wrote:
&gt; What's the current contract for `hadoop classpath`? Would it be safer to
&gt; introduce `hadoop userclasspath` or similar for this behavior?
&gt; 
&gt; I'm betting that changing `hadoop classpath` will lead to some breakages,
&gt; so I'd prefer to make this new behavior opt-in.
&gt; 
&gt; Best,
&gt; Andrew
&gt; 
&gt; On Mon, Apr 3, 2017 at 9:04 AM, Allen Wittenauer &lt;aw@effectivemachines.com&gt;
&gt; wrote:
&gt; 
&gt; &gt; 
&gt; &gt; This morning I had a bit of a shower thought:
&gt; &gt; 
&gt; &gt; With the new shaded hadoop client in 3.0, is there any reason the
&gt; &gt; default classpath should remain the full blown jar list?  e.g., shouldn't
&gt; &gt; ‘hadoop classpath' just return configuration, user supplied bits (e.g.,
&gt; &gt; HADOOP_USER_CLASSPATH, etc), HADOOP_OPTIONAL_TOOLS, and
&gt; &gt; hadoop-client-runtime? We'd obviously have to add some plumbing for daemons
&gt; &gt; and the capability for the user to get the full list, but that should be
&gt; &gt; trivial.
&gt; &gt; 
&gt; &gt; Thoughts?
&gt; &gt; ---------------------------------------------------------------------
&gt; &gt; To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
&gt; &gt; For additional commands, e-mail: common-dev-help@hadoop.apache.org
&gt; &gt; 
&gt; &gt; 
&gt; 


---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170901202100</emailId><senderName>"Erik Krogen (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-01 20:21:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14827) Allow StopWatch to accept a Timer parameter for tests</subject><body>

Erik Krogen created HADOOP-14827:
------------------------------------

             Summary: Allow StopWatch to accept a Timer parameter for tests
                 Key: HADOOP-14827
                 URL: https://issues.apache.org/jira/browse/HADOOP-14827
             Project: Hadoop Common
          Issue Type: Improvement
          Components: common, test
            Reporter: Erik Krogen
            Assignee: Erik Krogen
            Priority: Minor


{{StopWatch}} should accept a {{Timer}} parameter rather than directly using {{Time}} \
so that its behavior can be controlled during tests.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170901204500</emailId><senderName>"Jonathan Hung (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-01 20:45:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14828) RetryUpToMaximumTimeWithFixedSleep is not bounded by maximum time</subject><body>

Jonathan Hung created HADOOP-14828:
--------------------------------------

             Summary: RetryUpToMaximumTimeWithFixedSleep is not bounded by maximum \
time  Key: HADOOP-14828
                 URL: https://issues.apache.org/jira/browse/HADOOP-14828
             Project: Hadoop Common
          Issue Type: Bug
            Reporter: Jonathan Hung


In RetryPolicies.java, RetryUpToMaximumTimeWithFixedSleep is converted to a \
RetryUpToMaximumCountWithFixedSleep, whose count is the maxTime / sleepTime: \
{noformat}    public RetryUpToMaximumTimeWithFixedSleep(long maxTime, long sleepTime, \
TimeUnit timeUnit) {  super((int) (maxTime / sleepTime), sleepTime, timeUnit);
      this.maxTime = maxTime;
      this.timeUnit = timeUnit;
    }
{noformat}
But if retries take a long time, then the maxTime passed to the \
RetryUpToMaximumTimeWithFixedSleep is exceeded.

As an example, while doing NM restarts, we saw an issue where the NMProxy creates a \
retry policy which specifies a maximum wait time of 15 minutes and a 10 sec interval \
(which is converted to a MaximumCount policy with 15 min / 10 sec = 90 tries). But \
each NMProxy retry policy invokes o.a.h.ipc.Client's retry policy: {noformat}      if \
(connectionRetryPolicy == null) {  final int max = conf.getInt(
            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,
            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_DEFAULT);
        final int retryInterval = conf.getInt(
            CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_RETRY_INTERVAL_KEY,
            CommonConfigurationKeysPublic
                .IPC_CLIENT_CONNECT_RETRY_INTERVAL_DEFAULT);

        connectionRetryPolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(
            max, retryInterval, TimeUnit.MILLISECONDS);
      }{noformat}
So the time it takes the NMProxy to fail is actually (90 retries) * (10 sec NMProxy \
interval + o.a.h.ipc.Client retry time). In the default case, ipc client retries 10 \
times with a 1 sec interval, meaning the time it takes for NMProxy to fail is (90)(10 \
sec + 10 sec) = 30 min instead of the 15 min specified by NMProxy configuration.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170901215329</emailId><senderName>Andrew Wang</senderName><senderEmail>andrew.wang@cloudera.com</senderEmail><timestampReceived>2017-09-01 21:53:29-0400</timestampReceived><subject>2017-09-01 Hadoop 3 release status update</subject><body>


https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+3+release+status+updates

2017-09-01

We're two weeks out from beta1, focus is on blocker burndown.

Highlights:

   - S3Guard merged!
   - TSv2 alpha2 merged!
   - branch-3.0 has been cut after discussion on dev lists.

Red flags:

   - 10 blockers on the dashboard, closed and bumped some but new ones
   appeared.
   - Still need to land YARN native services and fix some S3Guard doc
   issues for beta1.
   - Rolling upgrade JIRAs for YARN and HDFS are not making any visible
   progress

Previously tracked beta1 blockers that have been resolved:

   - HADOOP-13363 &lt;https://issues.apache.org/jira/browse/HADOOP-13363&gt; (Upgrade
   to protobuf 3): I dropped this from beta1 since it's simply not going to
   happen in time.
   - YARN-7076 &lt;https://issues.apache.org/jira/browse/YARN-7076&gt;: This was
   quickly resolved! Thanks Jian, Junping, Jason for the action.
   - YARN-7094 &lt;https://issues.apache.org/jira/browse/YARN-7094&gt; (Document
   that server-side graceful decom is currently not recommended): Patch
   committed!

beta1 blockers:

   - HADOOP-14826 &lt;https://issues.apache.org/jira/browse/HADOOP-14826&gt; (review
   S3 docs prior to 3.0.0-beta1): New blocker with S3Guard merged. Should just
   be a quick doc update.
   - HADOOP-14284 &lt;https://issues.apache.org/jira/browse/HADOOP-14284&gt; (Shade
   Guava everywhere): Agreement to shade yarn-client at at HADOOP-14771.
   Shading hadoop-hdfs is still being discussed?
   - HADOOP-14771 &lt;https://issues.apache.org/jira/browse/HADOOP-14771&gt;
(hadoop-client
   does not include hadoop-yarn-client): Patch up, needs review, waiting on
   Busbey
   - YARN-5536 &lt;https://issues.apache.org/jira/browse/YARN-5536&gt; (Multiple
   format support (JSON, etc.) for exclude node file in NM graceful
   decommission with timeout): We're waiting on input from Junping.
   - MAPREDUCE-6941 (The default setting doesn't work for MapReduce job):
   Ray thinks this is a Won't Fix, waiting on Junping to confirm.
   - HADOOP-14238 (Rechecking Guava's object is not exposed to user-facing
   API): This relates to HADOOP-14771, I left a JIRA comment.

beta1 features:

   - Erasure coding
      - There are three must-dos. Two have patches, one might not be a
      must-do.
      - HDFS-11882 has been revved and reviewed, seems close
      - HDFS-11467 and HDFS-7859 are related, Sammi/Eddy/Kai are
      discussing, Sammi thinks we can still make beta1.
   - Addressing incompatible changes (YARN-6142 and HDFS-11096)
      - Sean has HDFS rolling upgrade scripts up, waiting on Ray to add
      some YARN/MR coverage too.
      - Need to do a final runthrough of the JACC reports for YARN and HDFS.
   - Classpath isolation (HADOOP-11656)
      - Sean has retriaged the subtasks and has been posting patches.
   - Compat guide (HADOOP-13714
   &lt;https://issues.apache.org/jira/browse/HADOOP-13714&gt;)
      - New patch is up, but needs review. Daniel asked Chris Douglas and
      Steve Loughran.
   - YARN native services
      - Jian sent out the merge vote
   - TSv2 alpha 2
   - This was merged, no problems thus far [image: (smile)]

GA features:

   - Resource profiles (Wangda Tan)
      - Merge vote was sent out. Since branch-3.0 has been cut, this can be
      merged to trunk (3.1.0) and then backported once we've completed testing.
   - HDFS router-based federation (Chris Douglas)
   - This is like YARN federation, very separate and doesn't add new APIs,
      run in production at MSFT.
      - If it passes Cloudera internal integration testing, I'm fine
      putting this in for GA.
   - API-based scheduler configuration (Jonathan Hung)
      - Jonathan mentioned that his main goal is to get this in for 2.9.0,
      which seems likely to go out after 3.0.0 GA since there hasn't been any
      serious release planning yet. Jonathan said that delaying this
until 3.1.0
      is fine.


</body></email><email><emailId>20170902012200</emailId><senderName>"Yuliya Feldman (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-02 01:22:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14829) Path should support colon</subject><body>

Yuliya Feldman created HADOOP-14829:
---------------------------------------

             Summary: Path should support colon
                 Key: HADOOP-14829
                 URL: https://issues.apache.org/jira/browse/HADOOP-14829
             Project: Hadoop Common
          Issue Type: Improvement
          Components: common
            Reporter: Yuliya Feldman


Object storages today support colon (:) in names, while Hadoop Path does not support it.
This JIRA is to allow Path to support colon in URI



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170902023100</emailId><senderName>"Allen Wittenauer (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-02 02:31:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14830) Write some non-Docker-based instructions on how to build on Mac OS X</subject><body>

Allen Wittenauer created HADOOP-14830:
-----------------------------------------

             Summary: Write some non-Docker-based instructions on how to build on Mac OS X
                 Key: HADOOP-14830
                 URL: https://issues.apache.org/jira/browse/HADOOP-14830
             Project: Hadoop Common
          Issue Type: Improvement
          Components: build, documentation
    Affects Versions: 3.0.0-beta1
            Reporter: Allen Wittenauer
            Assignee: Allen Wittenauer


We should have some decent documentation on how to build on OS X, now that almost everything works again.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170902121700</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-02 12:17:00-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14829) Path should support colon</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14829?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-14829.
-------------------------------------
    Resolution: Duplicate

Closing as dupe; remember to do a search for the issue (indeed, look for links off \
issues you have found) before creating new ones, as it only complicates life.

&gt; Path should support colon
&gt; -------------------------
&gt; 
&gt; Key: HADOOP-14829
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14829
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: common
&gt; Reporter: Yuliya Feldman
&gt; 
&gt; Object storages today support colon ( : ) in names, while Hadoop Path does not \
&gt; support it. This JIRA is to allow Path to support colon in URI



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170904143503</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-04 14:35:03-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-13654) S3A create() to support asynchronous check of dest &amp; parent paths</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-13654?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-13654.
-------------------------------------
    Resolution: Won't Fix

&gt; S3A create() to support asynchronous check of dest &amp; parent paths
&gt; -----------------------------------------------------------------
&gt; 
&gt; Key: HADOOP-13654
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-13654
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3
&gt; Affects Versions: 2.7.3
&gt; Reporter: Steve Loughran
&gt; 
&gt; One source of delays in S3A is the need to check if a destination path exists in \
&gt; create; this makes sure the operation isn't trying to overwrite a directory. #. \
&gt; This is slow, 1-4 HTTPS requests # The code doesn't seem to check the entire parent \
&gt; path to make sure there isn't a file as a parent (which raises the question: \
&gt; shouldn't we have a contract test for this?) # Even with the create overwrite=false \
&gt; check, the fact that the new object isn't created until the output stream is \
&gt; close()'d, means that the check has race conditions. Instead of doing a synchronous \
&gt; check in create(), we could do an asynchronous check of the parent directory tree. \
&gt; If any error surfaced, this could be cached and then thrown on the next call to: \
&gt; write(), flush() or close(); that is, the failure of a create due to path problems \
&gt; would not surface immediately on the create() call, *but before any writes were \
&gt; committed*. The full directory tree can/should be checked, and is results \
&gt; remembered. This would allow for the post-commit cleanup to issue delete() requests \
&gt; purely for those paths (if any) which referred to directories. As well as the need \
&gt; to use the AWS thread pool, there's a bit of complexity with cancelling multipart \
&gt; uploads: the output stream needs to know that the request failed, and that the \
&gt; multipart should be aborted. If the complexity of the asynchronous calls can be \
&gt; coped with, *and client code happy to accept errors in the any IO call to the \
&gt; output stream*, then the initial overhead at file creation could be skipped.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170904152100</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-04 15:21:00-0400</timestampReceived><subject>=?utf-8?Q?[jira]_[Created]_(HADOOP-14831)_=C3=9Cber-j?= =?utf-8?Q?ira:_S3a_phase_IV:_Hadoop_3.1_feat</subject><body>

Steve Loughran created HADOOP-14831:
---------------------------------------

             Summary: =C3=9Cber-jira: S3a phase IV: Hadoop 3.1 features
                 Key: HADOOP-14831
                 URL: https://issues.apache.org/jira/browse/HADOOP-14831
             Project: Hadoop Common
          Issue Type: Bug
          Components: fs/s3
    Affects Versions: 3.1.0
            Reporter: Steve Loughran


All the S3/S3A features targeting Hadoop 3.1



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170905034600</emailId><senderName>"John Zhuge (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 03:46:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14832) listing s3a bucket without credentials gives Interrupted error</subject><body>

John Zhuge created HADOOP-14832:
-----------------------------------

             Summary: listing s3a bucket without credentials gives Interrupted error
                 Key: HADOOP-14832
                 URL: https://issues.apache.org/jira/browse/HADOOP-14832
             Project: Hadoop Common
          Issue Type: Improvement
          Components: fs/s3
    Affects Versions: 3.0.0-beta1
            Reporter: John Zhuge
            Priority: Minor


In trunk pseudo distributed mode, without setting s3a credentials, listing an s3a \
bucket only gives "Interrupted" error : {noformat}
$ hadoop fs -ls s3a://bucket/
ls: Interrupted
{noformat}

In comparison, branch-2 gives a much better error message:
{noformat}
(branch-2)$ hadoop_env hadoop fs -ls s3a://bucket/
ls: doesBucketExist on hdfs-cce: com.amazonaws.AmazonClientException: No AWS \
Credentials provided by BasicAWSCredentialsProvider \
EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : \
com.amazonaws.SdkClientException: Unable to load credentials from service endpoint \
{noformat}




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905085200</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 08:52:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14833) Move s3a user:secret auth out of the main chain of auth mechanisms</subject><body>

Steve Loughran created HADOOP-14833:
---------------------------------------

             Summary: Move s3a user:secret auth out of the main chain of auth \
mechanisms  Key: HADOOP-14833
                 URL: https://issues.apache.org/jira/browse/HADOOP-14833
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/s3
    Affects Versions: 3.0.0-beta1
            Reporter: Steve Loughran
            Priority: Minor


Remove the s3a://user:secret@host auth mechanism from S3a

I think we could consider retain it as an explicit credential provider you can ask \
for, so that people who cannot move off it (yet) can reconfigure their system, but \
unless you do that, it stops working. 

We could add a dummy credential handler which recognises the user:secret pattern &amp; \
then tells the user "no longer supported, sorry, here's how to migrate", &amp; add that \
to the default chain after everything else.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905085600</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 08:56:00-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-13757) Remove verifyBuckets overhead in S3AFileSystem::initialize()</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-13757?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-13757.
-------------------------------------
    Resolution: Won't Fix

I'm going to make this a wontfix, with s3guard &amp; c logging in has become more \
complex. sorry

&gt; Remove verifyBuckets overhead in S3AFileSystem::initialize()
&gt; ------------------------------------------------------------
&gt; 
&gt; Key: HADOOP-13757
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-13757
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3
&gt; Affects Versions: 3.0.0-beta1
&gt; Reporter: Rajesh Balamohan
&gt; Priority: Minor
&gt; 
&gt; {{S3AFileSystem.initialize()}} invokes verifyBuckets, but in case the bucket does \
&gt; not exist and gets a 403 error message, it ends up returning {{true}} for \
&gt; {{s3.doesBucketExists(bucketName}}.  In that aspect,  verifyBuckets() is an \
&gt; unnecessary call during initialization. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905090018</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 09:00:18-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-13950) S3A create(path, overwrite=true) need only check for path being a d</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-13950?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-13950.
-------------------------------------
    Resolution: Duplicate

&gt; S3A create(path, overwrite=true) need only check for path being a dir, not a file
&gt; ---------------------------------------------------------------------------------
&gt; 
&gt; Key: HADOOP-13950
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-13950
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3
&gt; Affects Versions: 2.7.3
&gt; Reporter: Steve Loughran
&gt; Priority: Minor
&gt; 
&gt; When you create a file with overwrite=true, you don't care that a path resolves to \
&gt; a file, only that there isn't a directory at the destination. S3A can use this, \
&gt; bypass the {{GET path}} and only do a {{GET path + "/"}} and LIST path. That way: \
&gt; one HTTPS request saved, and no negative caching of the path to confuse followup \
&gt; checks. That is: better performance and consistency



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905090300</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 09:03:00-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14605) transient list consistency failure in ITestS3AContractRootDir</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14605?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-14605.
-------------------------------------
    Resolution: Duplicate

closing as a duplicate of HADOOP-13271. It's a slightly different stack, but I \
suspect the same cause: observed listing inconsistency

&gt; transient list consistency failure in ITestS3AContractRootDir
&gt; -------------------------------------------------------------
&gt; 
&gt; Key: HADOOP-14605
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14605
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3, test
&gt; Affects Versions: 3.0.0-alpha3
&gt; Environment: s3guard disabled
&gt; Reporter: Steve Loughran
&gt; Priority: Minor
&gt; 
&gt; Test against s3 ireland just failed with a deleted-path-still-found exception; \
&gt; clearly a consistency event of one form or another. This was on a \
&gt; {{fileSystem.getFileStatus(path);}} call; a HEAD against the object. Assume that a \
&gt; tombstone marker hadn't made it to the shard the request went to. Test will need to \
&gt; move to an eventually()



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905090900</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 09:09:00-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14736) S3AInputStream to implement an efficient skip() call through seekin</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14736?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-14736.
-------------------------------------
    Resolution: Duplicate

&gt; S3AInputStream to implement an efficient skip() call through seeking
&gt; --------------------------------------------------------------------
&gt; 
&gt; Key: HADOOP-14736
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14736
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3
&gt; Reporter: Steve Loughran
&gt; Priority: Minor
&gt; 
&gt; {{S3AInputStream}} implements skip() naively through the byte class: Reading and \
&gt; discarding all data. Efficient on classic "sequential" reads, provided the forward \
&gt; skip is &lt;1MB. For larger skip values or on random IO, seek() should be used. After \
&gt; some range checks/handling past-EOF skips to seek (EOF-1), a seek() should handle \
&gt;                 the skip file.
&gt; *there are no FS contract tests for skip semantics*



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905091401</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 09:14:01-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-13630) split up AWS index.md</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-13630?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-13630.
-------------------------------------
    Resolution: Duplicate

&gt; split up AWS index.md
&gt; ---------------------
&gt; 
&gt; Key: HADOOP-13630
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-13630
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: documentation, fs/s3
&gt; Affects Versions: 2.9.0
&gt; Reporter: Steve Loughran
&gt; 
&gt; The AWS index.md file is too big, too much written by developers as we go along, \
&gt; not for end users. I propose splitting it into its own docs
&gt; * Intro
&gt; * S3A
&gt; * S3N
&gt; * S3 (branch-2 only, obviously)
&gt; * testing
&gt; * maybe in future: something on effective coding against object stores,
&gt; though that could go toplevel, as it applies to all
&gt; I propose waiting for HADOOP-13560 to be in, as that changes the docs.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905091500</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 09:15:00-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14826) review S3 docs prior to 3.0-beta-1</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14826?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-14826.
-------------------------------------
    Resolution: Duplicate

&gt; review S3 docs prior to 3.0-beta-1
&gt; ----------------------------------
&gt; 
&gt; Key: HADOOP-14826
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14826
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: documentation, fs/s3
&gt; Affects Versions: 3.0.0-beta1
&gt; Reporter: Steve Loughran
&gt; Assignee: Steve Loughran
&gt; Priority: Blocker
&gt; 
&gt; The hadoop-aws docs need a review and update
&gt; * things marked as stabilizing (fast upload, .fadvise ..) can be considered stable
&gt; * move s3n docs off to the side
&gt; * add a "how to move from s3n to s3a" para



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905113200</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 11:32:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14834) Make default output stream of S3a the block output stream</subject><body>

Steve Loughran created HADOOP-14834:
---------------------------------------

             Summary: Make default output stream of S3a the block output stream
                 Key: HADOOP-14834
                 URL: https://issues.apache.org/jira/browse/HADOOP-14834
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/s3
    Affects Versions: 3.0.0-beta1
            Reporter: Steve Loughran
            Priority: Minor


The S3A Block output stream is working well and much better than the original stream \
in terms of: scale, performance, instrumentation, robustness

Proposed: switch this to be the default, as a precursor to removing it later \
HADOOP-14746



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905113801</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 11:38:01-0400</timestampReceived><subject>[jira] [Reopened] (HADOOP-14736) S3AInputStream to implement an efficient skip() call through seekin</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14736?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran reopened HADOOP-14736:
-------------------------------------

&gt; S3AInputStream to implement an efficient skip() call through seeking
&gt; --------------------------------------------------------------------
&gt; 
&gt; Key: HADOOP-14736
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14736
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3
&gt; Reporter: Steve Loughran
&gt; Priority: Minor
&gt; 
&gt; {{S3AInputStream}} implements skip() naively through the byte class: Reading and \
&gt; discarding all data. Efficient on classic "sequential" reads, provided the forward \
&gt; skip is &lt;1MB. For larger skip values or on random IO, seek() should be used. After \
&gt; some range checks/handling past-EOF skips to seek (EOF-1), a seek() should handle \
&gt;                 the skip file.
&gt; *there are no FS contract tests for skip semantics*



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905160600</emailId><senderName>"Allen Wittenauer (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 16:06:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14835) mvn site build throws SAX errors</subject><body>

Allen Wittenauer created HADOOP-14835:
-----------------------------------------

             Summary: mvn site build throws SAX errors
                 Key: HADOOP-14835
                 URL: https://issues.apache.org/jira/browse/HADOOP-14835
             Project: Hadoop Common
          Issue Type: Bug
          Components: build, site
    Affects Versions: 3.0.0-beta1
            Reporter: Allen Wittenauer
            Priority: Critical




Running mvn  install site site:stage -DskipTests -Pdist,src -Preleasedocs,docs \
results in a stack trace when run on a fresh .m2 directory.  It appears to be coming \
from the jdiff doclets in the annotations code.





--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905161600</emailId><senderName>"Allen Wittenauer (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 16:16:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14836) multiple versions of maven-clean-plugin in use</subject><body>

Allen Wittenauer created HADOOP-14836:
-----------------------------------------

             Summary: multiple versions of maven-clean-plugin in use
                 Key: HADOOP-14836
                 URL: https://issues.apache.org/jira/browse/HADOOP-14836
             Project: Hadoop Common
          Issue Type: Bug
          Components: build
    Affects Versions: 3.0.0-beta1
            Reporter: Allen Wittenauer


hadoop-yarn-ui re-declares maven-clean-plugin with 3.0 while the rest of the source \
tree uses 2.5.  This should get synced up.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905162500</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 16:25:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14837) Handle S3A "glacier" data</subject><body>

Steve Loughran created HADOOP-14837:
---------------------------------------

             Summary: Handle S3A "glacier" data
                 Key: HADOOP-14837
                 URL: https://issues.apache.org/jira/browse/HADOOP-14837
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/s3
    Affects Versions: 3.0.0-beta1
            Reporter: Steve Loughran


SPARK-21797 covers how if you have AWS S3 set to copy some files to glacier, they \
appear in the listing but GETs fail, and so does everything else

We should think about how best to handle this.

# report better
# if listings can identify files which are glaciated then maybe we could have an \
option to filter them out # test &amp; see what happens




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905171000</emailId><senderName>"Jonathan Hung (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 17:10:00-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14828) RetryUpToMaximumTimeWithFixedSleep is not bounded by maximum time</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14828?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Jonathan Hung resolved HADOOP-14828.
------------------------------------
    Resolution: Duplicate

&gt; RetryUpToMaximumTimeWithFixedSleep is not bounded by maximum time
&gt; -----------------------------------------------------------------
&gt; 
&gt; Key: HADOOP-14828
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14828
&gt; Project: Hadoop Common
&gt; Issue Type: Bug
&gt; Reporter: Jonathan Hung
&gt; 
&gt; In RetryPolicies.java, RetryUpToMaximumTimeWithFixedSleep is converted to a \
&gt; RetryUpToMaximumCountWithFixedSleep, whose count is the maxTime / sleepTime: \
&gt; {noformat}    public RetryUpToMaximumTimeWithFixedSleep(long maxTime, long \
&gt; sleepTime, TimeUnit timeUnit) {
&gt; super((int) (maxTime / sleepTime), sleepTime, timeUnit);
&gt; this.maxTime = maxTime;
&gt; this.timeUnit = timeUnit;
&gt; }
&gt; {noformat}
&gt; But if retries take a long time, then the maxTime passed to the \
&gt; RetryUpToMaximumTimeWithFixedSleep is exceeded. As an example, while doing NM \
&gt; restarts, we saw an issue where the NMProxy creates a retry policy which specifies \
&gt; a maximum wait time of 15 minutes and a 10 sec interval (which is converted to a \
&gt; MaximumCount policy with 15 min / 10 sec = 90 tries). But each NMProxy retry policy \
&gt; invokes o.a.h.ipc.Client's retry policy: {noformat}      if (connectionRetryPolicy \
&gt; == null) { final int max = conf.getInt(
&gt; CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,
&gt; CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_DEFAULT);
&gt; final int retryInterval = conf.getInt(
&gt; CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_RETRY_INTERVAL_KEY,
&gt; CommonConfigurationKeysPublic
&gt; .IPC_CLIENT_CONNECT_RETRY_INTERVAL_DEFAULT);
&gt; connectionRetryPolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(
&gt; max, retryInterval, TimeUnit.MILLISECONDS);
&gt; }{noformat}
&gt; So the time it takes the NMProxy to fail is actually (90 retries) * (10 sec NMProxy \
&gt; interval + o.a.h.ipc.Client retry time). In the default case, ipc client retries 10 \
&gt; times with a 1 sec interval, meaning the time it takes for NMProxy to fail is \
&gt; (90)(10 sec + 10 sec) = 30 min instead of the 15 min specified by NMProxy \
&gt; configuration.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905174501</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 17:45:01-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14838) backport S3guard to branch-2</subject><body>

Steve Loughran created HADOOP-14838:
---------------------------------------

             Summary: backport S3guard to branch-2
                 Key: HADOOP-14838
                 URL: https://issues.apache.org/jira/browse/HADOOP-14838
             Project: Hadoop Common
          Issue Type: New Feature
          Components: fs/s3
    Affects Versions: 2.9.0
            Reporter: Steve Loughran
            Assignee: Steve Loughran


Backport S3Guard to branch-2

this consists of
* classpath updates (AWS SDK, ...)
* hadoop bin classpath and command setup
* java 7 compatibility
* testing

The last patch of HADOOP-13998 brought the java code down to java 7 &amp; has already \
been tested/merged with branch-2; all that's left is the packaging, bin/hadoop and \
review



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905203300</emailId><senderName>"Subru Krishnan (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 20:33:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14840) Tool to estimate resource requirements of an application pipeline ba</subject><body>

Subru Krishnan created HADOOP-14840:
---------------------------------------

             Summary: Tool to estimate resource requirements of an application \
pipeline based on prior executions  Key: HADOOP-14840
                 URL: https://issues.apache.org/jira/browse/HADOOP-14840
             Project: Hadoop Common
          Issue Type: New Feature
          Components: tools
            Reporter: Subru Krishnan


We have been working on providing SLAs for job execution on Hadoop. At high level \
this involves 2 parts: deriving the resource requirements of a job and guaranteeing \
the estimated resources at runtime. The {{YARN ReservationSystem}} \
(YARN-1051/YARN-2572/YARN-5326) enable the latter and in this JIRA, we propose to add \
a tool to Hadoop to predict the  resource requirements of a job based on past \
executions of the job. The system (aka *Morpheus*) deep dive can be found in our \
OSDI'16 paper [here|https://www.usenix.org/conference/osdi16/technical-sessions/presentation/jyothi].




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905205900</emailId><senderName>"Xiao Chen (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 20:59:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14841) Add KMS Client retry to handle 'No content to map' EOFExceptions</subject><body>

Xiao Chen created HADOOP-14841:
----------------------------------

             Summary: Add KMS Client retry to handle 'No content to map' \
EOFExceptions  Key: HADOOP-14841
                 URL: https://issues.apache.org/jira/browse/HADOOP-14841
             Project: Hadoop Common
          Issue Type: Improvement
          Components: kms
    Affects Versions: 2.6.0
            Reporter: Xiao Chen
            Assignee: Xiao Chen


We have seen quite some occurrences when the KMS server is stressed, some of the \
requests would end up getting a 500 return code, with this in the server log: \
{noformat} 2017-08-31 06:45:33,021 WARN org.apache.hadoop.crypto.key.kms.server.KMS: \
User impala/HOSTNAME@REALM (auth:KERBEROS) request POST \
https://HOSTNAME:16000/kms/v1/keyversion/MNHDKEdWtZWM4vPb0p2bw544vdSRB2gy7APAQURcZns/_eek?eek_op=decrypt \
                caused exception.
java.io.EOFException: No content to map to Object due to end of input
        at org.codehaus.jackson.map.ObjectMapper._initForReading(ObjectMapper.java:2444)
                
        at org.codehaus.jackson.map.ObjectMapper._readMapAndClose(ObjectMapper.java:2396)
                
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:1648)
        at org.apache.hadoop.crypto.key.kms.server.KMSJSONReader.readFrom(KMSJSONReader.java:54)
                
        at com.sun.jersey.spi.container.ContainerRequest.getEntity(ContainerRequest.java:474)
                
        at com.sun.jersey.server.impl.model.method.dispatch.EntityParamDispatchProvider$EntityInjectable.getValue(EntityParamDispatchProvider.java:123)
                
        at com.sun.jersey.server.impl.inject.InjectableValuesProvider.getInjectableValues(InjectableValuesProvider.java:46)
  at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchPr \
ovider$EntityParamInInvoker.getParams(AbstractResourceMethodDispatchProvider.java:153)
  at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchPr \
                ovider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:203)
                
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
                
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
                
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
                
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
                
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
                
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
                
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
                
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
                
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
                
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
                
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
                
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
                
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)
  at javax.servlet.http.HttpServlet.service(HttpServlet.java:723)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)
                
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
                
        at org.apache.hadoop.crypto.key.kms.server.KMSMDCFilter.doFilter(KMSMDCFilter.java:84)
                
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
                
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
                
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:631)
                
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:301)
                
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:579)
                
        at org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter.doFilter(KMSAuthenticationFilter.java:130)
                
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
                
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
                
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)
                
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
                
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)
                
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
                
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)
                
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)
                
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:859)
        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:610)
                
        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:503)
        at java.lang.Thread.run(Thread.java:745)
{noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905212700</emailId><senderName>"Andrew Wang (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 21:27:00-0400</timestampReceived><subject>[jira] [Reopened] (HADOOP-13998) Merge initial S3guard release into trunk</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-13998?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Andrew Wang reopened HADOOP-13998:
----------------------------------

Re-opening to resolve as "Complete" or something, since this code change was \
attributed to the parent JIRA HADOOP-13345 in the commit message.

&gt; Merge initial S3guard release into trunk
&gt; ----------------------------------------
&gt; 
&gt; Key: HADOOP-13998
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-13998
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3
&gt; Affects Versions: 3.0.0-beta1
&gt; Reporter: Steve Loughran
&gt; Assignee: Steve Loughran
&gt; Fix For: 3.0.0-beta1
&gt; 
&gt; Attachments: HADOOP-13998-001.patch, HADOOP-13998-002.patch, \
&gt; HADOOP-13998-003.patch, HADOOP-13998-004.patch, HADOOP-13998-005.patch 
&gt; 
&gt; JIRA to link in all the things we think are needed for a preview/merge into trunk



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>201709052127000</emailId><senderName>"Andrew Wang (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 21:27:00-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-13998) Merge initial S3guard release into trunk</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-13998?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Andrew Wang resolved HADOOP-13998.
----------------------------------
    Resolution: Done

Re-resolving per above.

&gt; Merge initial S3guard release into trunk
&gt; ----------------------------------------
&gt; 
&gt; Key: HADOOP-13998
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-13998
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3
&gt; Affects Versions: 3.0.0-beta1
&gt; Reporter: Steve Loughran
&gt; Assignee: Steve Loughran
&gt; Fix For: 3.0.0-beta1
&gt; 
&gt; Attachments: HADOOP-13998-001.patch, HADOOP-13998-002.patch, \
&gt; HADOOP-13998-003.patch, HADOOP-13998-004.patch, HADOOP-13998-005.patch 
&gt; 
&gt; JIRA to link in all the things we think are needed for a preview/merge into trunk



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170905213700</emailId><senderName>"Junping Du (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-09-05 21:37:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14842) Hadoop 2.8.2 release build process get stuck due to java issue</subject><body>

Junping Du created HADOOP-14842:
-----------------------------------

             Summary: Hadoop 2.8.2 release build process get stuck due to java issue
                 Key: HADOOP-14842
                 URL: https://issues.apache.org/jira/browse/HADOOP-14842
             Project: Hadoop Common
          Issue Type: Bug
          Components: build
            Reporter: Junping Du
            Priority: Blocker


In my latest 2.8.2 release build (via docker) get failed, and following errors \
received:  
{noformat}
"/usr/bin/mvn -Dmaven.repo.local=/maven -pl hadoop-maven-plugins -am clean install
Error: JAVA_HOME is not defined correctly. We cannot execute \
/usr/lib/jvm/java-7-oracle/bin/java" {noformat}

This looks like related to HADOOP-14474. However, reverting that patch doesn't work \
here because build progress will get failed earlier in java download/installation - \
may be as mentioned in HADOOP-14474, some java 7 download address get changed by \
Oracle.  Hard coding my local JAVA_HOME to create-release or Dockerfile doesn't work \
here although it show correct java home. My suspect so far is we still need to \
download java 7 from somewhere to make build progress continue in docker building \
process, but haven't got clue to go through this.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170801072100</emailId><senderName>"Wenxin He (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 07:21:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14706) Adding a helper method to determine whether a log is Log4j implement</subject><body>

Wenxin He created HADOOP-14706:
----------------------------------

             Summary: Adding a helper method to determine whether a log is Log4j \
implement  Key: HADOOP-14706
                 URL: https://issues.apache.org/jira/browse/HADOOP-14706
             Project: Hadoop Common
          Issue Type: Improvement
          Components: util
            Reporter: Wenxin He
            Assignee: Wenxin He
            Priority: Minor


Base on the comments in YARN-6873, we'd like to add a helper method to determine \
whether a log is Log4j implement. Using this helper method, we don't have to care \
about it's org.apache.commons.logging or org.slf4j.Logger used in our system.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170801092300</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 09:23:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14707) AbstractContractDistCpTest to test attr preservation with -p, verify</subject><body>

Steve Loughran created HADOOP-14707:
---------------------------------------

             Summary: AbstractContractDistCpTest to test attr preservation with -p, \
verify blobstores downgrade  Key: HADOOP-14707
                 URL: https://issues.apache.org/jira/browse/HADOOP-14707
             Project: Hadoop Common
          Issue Type: Improvement
          Components: fs, fs/azure, fs/s3, test, tools/distcp
    Affects Versions: 2.9.0
            Reporter: Steve Loughran
            Priority: Minor


It *may* be that trying to use {{distcp -p}} with S3a triggers a stack trace 
{code}
java.lang.UnsupportedOperationException: S3AFileSystem doesn't support getXAttrs 
at org.apache.hadoop.fs.FileSystem.getXAttrs(FileSystem.java:2559) 
at org.apache.hadoop.tools.util.DistCpUtils.toCopyListingFileStatus(DistCpUtils.java:322) \
 {code}

Add a test to {{AbstractContractDistCpTest}} to verify that this is handled better. \
What is "handle better" here? Either ignore the option or fail with "don't do that" \
text



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170801093800</emailId><senderName>"Lantao Jin (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 09:38:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14708) FsckServlet can not create SaslRpcClient with auth KERBEROS_SSL</subject><body>

Lantao Jin created HADOOP-14708:
-----------------------------------

             Summary: FsckServlet can not create SaslRpcClient with auth KERBEROS_SSL
                 Key: HADOOP-14708
                 URL: https://issues.apache.org/jira/browse/HADOOP-14708
             Project: Hadoop Common
          Issue Type: Bug
          Components: security
    Affects Versions: 3.0.0-alpha3, 2.8.1, 2.7.3
            Reporter: Lantao Jin


FSCK started by xx (auth:KERBEROS_SSL) failed with exception msg "fsck encountered \
internal errors!"

FSCK use FSCKServlet to submit RPC to NameNode, it use {{KERBEROS_SSL}} as its \
{{AuthenticationMethod}} in {{JspHelper.java}} {code}
  /** Same as getUGI(context, request, conf, KERBEROS_SSL, true). */
  public static UserGroupInformation getUGI(ServletContext context,
      HttpServletRequest request, Configuration conf) throws IOException {
    return getUGI(context, request, conf, AuthenticationMethod.KERBEROS_SSL, true);
  }
{code}

But when setup SaslConnection with server, KERBEROS_SSL will failed to create \
SaslClient instance. See {{SaslRpcClient.java}} {code}
private SaslClient createSaslClient(SaslAuth authType)
      throws SaslException, IOException {
      ....
      case KERBEROS: {
        if (ugi.getRealAuthenticationMethod().getAuthMethod() !=
            AuthMethod.KERBEROS) {
          return null; // client isn't using kerberos
        }
{code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170801131600</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 13:16:00-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14675) Path object disallows access to S3 objects with // in their names</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14675?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-14675.
-------------------------------------
    Resolution: Not A Problem

&gt; Path object disallows access to S3 objects with // in their names
&gt; -----------------------------------------------------------------
&gt; 
&gt; Key: HADOOP-14675
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14675
&gt; Project: Hadoop Common
&gt; Issue Type: Bug
&gt; Components: fs/s3
&gt; Affects Versions: 2.7.1
&gt; Environment: Plain vanila Hadoop 2.7.
&gt; Reporter: Andi
&gt; 
&gt; Seems that there's explicit code to disallow FS paths with //.
&gt; In our environment this means that direct access using url's like \
&gt; s3a://bucket/data//part1 are inaccessible. Why does this code exist? Can it be \
&gt; removed? https://github.com/apache/hadoop/blob/6fefb8f4a486b1d8071a7071e9e96f0c62997 \
&gt; 52d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Path.java#L289-L289
&gt; 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170801153800</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 15:38:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14709) fix checkstyle warnings in ContractTestUtils</subject><body>

Steve Loughran created HADOOP-14709:
---------------------------------------

             Summary: fix checkstyle warnings in ContractTestUtils
                 Key: HADOOP-14709
                 URL: https://issues.apache.org/jira/browse/HADOOP-14709
             Project: Hadoop Common
          Issue Type: Improvement
          Components: test
    Affects Versions: 2.8.1
            Reporter: Steve Loughran
            Assignee: Thomas Marquardt
            Priority: Minor


{{ContractTestUtils}} is generating a lot of minor checkstyle complaints which make \
patching against the file noisier. Clean up

(based on work in HADOOP-14660)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170801163201</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 16:32:01-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-12669) clean up temp dirs in hadoop-project-dist/pom.xml</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-12669?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Steve Loughran resolved HADOOP-12669.
-------------------------------------
    Resolution: Won't Fix

&gt; clean up temp dirs in  hadoop-project-dist/pom.xml
&gt; --------------------------------------------------
&gt; 
&gt; Key: HADOOP-12669
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-12669
&gt; Project: Hadoop Common
&gt; Issue Type: Improvement
&gt; Components: build
&gt; Affects Versions: 2.8.0
&gt; Reporter: Steve Loughran
&gt; 
&gt; Andrew Wang noted in HDFS-9263 that there are various tmp dir definitions in \
&gt; {{hadoop-project-dist/pom.xml}} which are creating data in the wrong place: clean \
&gt; them up



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170801164800</emailId><senderName>"John Zhuge (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 16:48:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14710) Uber-JIRA: Support AWS Snowball</subject><body>

John Zhuge created HADOOP-14710:
-----------------------------------

             Summary: Uber-JIRA: Support AWS Snowball
                 Key: HADOOP-14710
                 URL: https://issues.apache.org/jira/browse/HADOOP-14710
             Project: Hadoop Common
          Issue Type: Bug
          Components: fs/s3
    Affects Versions: 2.8.0
            Reporter: John Zhuge
            Assignee: John Zhuge


Support data transfer between Hadoop and [AWS \
Snowball|http://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html].





--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170801165600</emailId><senderName>"John Zhuge (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 16:56:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14711) Test data transfer between Hadoop and AWS Snowball</subject><body>

John Zhuge created HADOOP-14711:
-----------------------------------

             Summary: Test data transfer between Hadoop and AWS Snowball
                 Key: HADOOP-14711
                 URL: https://issues.apache.org/jira/browse/HADOOP-14711
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/s3, test
    Affects Versions: 2.8.0
            Reporter: John Zhuge


Test data transfer between Hadoop and AWS Snowball:
* fs -cp
* DistCp
* Scale tests



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170801165900</emailId><senderName>"John Zhuge (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 16:59:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14712) Document support for AWS Snowball</subject><body>

John Zhuge created HADOOP-14712:
-----------------------------------

             Summary: Document support for AWS Snowball
                 Key: HADOOP-14712
                 URL: https://issues.apache.org/jira/browse/HADOOP-14712
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: documentation, fs/s3
    Affects Versions: 2.8.0
         Environment: Document Hadoop support for AWS Snowball:
* Commands and parameters
* Performance tuning
* Caveats
* Troubleshooting
            Reporter: John Zhuge






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170801181900</emailId><senderName>"Jason Lowe (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 18:19:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14713) Audit for durations that should be measured via Time.monotonicNow</subject><body>

Jason Lowe created HADOOP-14713:
-----------------------------------

             Summary: Audit for durations that should be measured via \
Time.monotonicNow  Key: HADOOP-14713
                 URL: https://issues.apache.org/jira/browse/HADOOP-14713
             Project: Hadoop Common
          Issue Type: Improvement
            Reporter: Jason Lowe


Whenever we are measuring a time delta or duration in the same process, the \
timestamps probably should be using Time.monotonicNow rather than Time.now or \
System.currentTimeMillis.  The latter two are directly reading the system clock which \
can move faster or slower than actual time if the system is undergoing a time \
adjustment (e.g.: adjtime or admin sets a new system time).

We should go through the code base and identify places where the code is using the \
system clock but really should be using monotonic time.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170801202302</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 20:23:02-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14714) handle InternalError in bulk object delete through retries</subject><body>

Steve Loughran created HADOOP-14714:
---------------------------------------

             Summary: handle InternalError in bulk object delete through retries
                 Key: HADOOP-14714
                 URL: https://issues.apache.org/jira/browse/HADOOP-14714
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/s3
    Affects Versions: 2.8.0
            Reporter: Steve Loughran


There's some more detail appearing on HADOOP-11572 about the errors seen here; sounds \
like its large fileset related (or just probability working against you). Most \
importantly: retries may make it go away. 

Proposed: implement a retry policy.

Issue: delete is not idempotent, not if someone else adds things.




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170801204500</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 20:45:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14715) failure of test org.apache.hadoop.fs.azure.TestWasbRemoteCallHelper</subject><body>

Steve Loughran created HADOOP-14715:
---------------------------------------

             Summary: failure of test \
org.apache.hadoop.fs.azure.TestWasbRemoteCallHelper  Key: HADOOP-14715
                 URL: https://issues.apache.org/jira/browse/HADOOP-14715
             Project: Hadoop Common
          Issue Type: Sub-task
    Affects Versions: 3.0.0-beta1
            Reporter: Steve Loughran
            Priority: Minor


{{org.apache.hadoop.fs.azure.TestWasbRemoteCallHelper.testWhenOneInstanceIsDown}} is \
failing for me on trunk



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170801214100</emailId><senderName>"Chen He (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 21:41:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14716) SwiftNativeFileSystem should not eat the exception when rename</subject><body>

Chen He created HADOOP-14716:
--------------------------------

             Summary: SwiftNativeFileSystem should not eat the exception when rename
                 Key: HADOOP-14716
                 URL: https://issues.apache.org/jira/browse/HADOOP-14716
             Project: Hadoop Common
          Issue Type: Bug
          Components: tools
    Affects Versions: 3.0.0-alpha3, 2.8.1
            Reporter: Chen He
            Assignee: Chen He
            Priority: Minor


Currently, if "rename" will eat excpetions and return "false" in \
SwiftNativeFileSystem. It is not easy for user to find root cause about why rename \
failed. It has to, at least, write out some logs instead of directly eats these \
exceptions.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170801223000</emailId><senderName>"Sean Mackrory (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 22:30:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14717) Add StreamCapabilities support to s3a</subject><body>

Sean Mackrory created HADOOP-14717:
--------------------------------------

             Summary: Add StreamCapabilities support to s3a
                 Key: HADOOP-14717
                 URL: https://issues.apache.org/jira/browse/HADOOP-14717
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/s3
            Reporter: Sean Mackrory
            Assignee: Sean Mackrory






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170801225000</emailId><senderName>"John Zhuge (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 22:50:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14718) Add StreamCapabilities support to ADLS</subject><body>

John Zhuge created HADOOP-14718:
-----------------------------------

             Summary: Add StreamCapabilities support to ADLS
                 Key: HADOOP-14718
                 URL: https://issues.apache.org/jira/browse/HADOOP-14718
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/adl
    Affects Versions: 3.0.0-alpha4
            Reporter: John Zhuge






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170801225100</emailId><senderName>"John Zhuge (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 22:51:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14719) Add StreamCapabilities support to WASB</subject><body>

John Zhuge created HADOOP-14719:
-----------------------------------

             Summary: Add StreamCapabilities support to WASB
                 Key: HADOOP-14719
                 URL: https://issues.apache.org/jira/browse/HADOOP-14719
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/azure
    Affects Versions: 3.0.0-alpha4
            Reporter: John Zhuge






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170801225200</emailId><senderName>"John Zhuge (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 22:52:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14720) Add StreamCapabilities support to Swift</subject><body>

John Zhuge created HADOOP-14720:
-----------------------------------

             Summary: Add StreamCapabilities support to Swift
                 Key: HADOOP-14720
                 URL: https://issues.apache.org/jira/browse/HADOOP-14720
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/swift
    Affects Versions: 3.0.0-alpha4
            Reporter: John Zhuge






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170801225300</emailId><senderName>"John Zhuge (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-01 22:53:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14721) Add StreamCapabilities support to Aliyun OSS</subject><body>

John Zhuge created HADOOP-14721:
-----------------------------------

             Summary: Add StreamCapabilities support to Aliyun OSS
                 Key: HADOOP-14721
                 URL: https://issues.apache.org/jira/browse/HADOOP-14721
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/oss
    Affects Versions: 3.0.0-alpha4
            Reporter: John Zhuge






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170802052300</emailId><senderName>"Thomas Marquardt (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-02 05:23:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14722) Azure: BlockBlobInputStream position incorrect after seek</subject><body>

Thomas Marquardt created HADOOP-14722:
-----------------------------------------

             Summary: Azure: BlockBlobInputStream position incorrect after seek
                 Key: HADOOP-14722
                 URL: https://issues.apache.org/jira/browse/HADOOP-14722
             Project: Hadoop Common
          Issue Type: Bug
          Components: fs/azure
            Reporter: Thomas Marquardt
            Assignee: Thomas Marquardt


The seek, skip, and getPos methods of BlockBlobInputStream do not correctly account \
for the stream's  internal buffer.  This results in invalid stream positions. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170802123201</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-02 12:32:01-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14723) reinstate URI parameter in AWSCredentialProvider constructors</subject><body>

Steve Loughran created HADOOP-14723:
---------------------------------------

             Summary: reinstate URI parameter in AWSCredentialProvider constructors
                 Key: HADOOP-14723
                 URL: https://issues.apache.org/jira/browse/HADOOP-14723
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/s3
    Affects Versions: 2.9.0
            Reporter: Steve Loughran
            Assignee: Steve Loughran


I need to revert HADOOP-14135 "Remove URI parameter in AWSCredentialProvider \
constructors", as knowing the bucket in use is needed for

* HADOOP-14507: per bucket secrets in JCEKS files
* HADOOP-14556: delegation tokens in S3A

these providers need the URI as it needs to it to decide which keys to scan for/what \
token to look up.

I know we pulled it out to allow us to talk to DDB without needing a FS URI, but for \
these specific cases, it is needed —we just won't be able to use the specific auth \
providers to talk to AWS except to an S3 bucket. 

Rather than just revert the patch, I propose waiting for s3guard phase I to be merged \
in to trunk, then do it, with the JCEKS auth mech being set up to skip looking for a \
per-bucket secret and key if it doesn't know its bucket name.




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170802180600</emailId><senderName>"Allen Wittenauer (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-02 18:06:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14724) Get a daily QBT run for Windows</subject><body>

Allen Wittenauer created HADOOP-14724:
-----------------------------------------

             Summary: Get a daily QBT run for Windows
                 Key: HADOOP-14724
                 URL: https://issues.apache.org/jira/browse/HADOOP-14724
             Project: Hadoop Common
          Issue Type: Test
          Components: test
    Affects Versions: 3.0.0-beta1
            Reporter: Allen Wittenauer
            Assignee: Allen Wittenauer


We used to have a Windows as part of our testing infrastructure.  Let's get it back \
up and running now that the ASF has some boxes.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170802182901</emailId><senderName>"Allen Wittenauer (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-02 18:29:01-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14725) hadoop-aws parallel tests do not work under Windows</subject><body>

Allen Wittenauer created HADOOP-14725:
-----------------------------------------

             Summary: hadoop-aws parallel tests do not work under Windows
                 Key: HADOOP-14725
                 URL: https://issues.apache.org/jira/browse/HADOOP-14725
             Project: Hadoop Common
          Issue Type: Test
          Components: test
    Affects Versions: 3.0.0-beta1
            Reporter: Allen Wittenauer
            Priority: Minor






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170802204701</emailId><senderName>"Allen Wittenauer (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-02 20:47:01-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14725) hadoop-aws parallel tests do not work under Windows</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14725?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Allen Wittenauer resolved HADOOP-14725.
---------------------------------------
    Resolution: Duplicate

&gt; hadoop-aws parallel tests do not work under Windows
&gt; ---------------------------------------------------
&gt; 
&gt; Key: HADOOP-14725
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14725
&gt; Project: Hadoop Common
&gt; Issue Type: Test
&gt; Components: test
&gt; Affects Versions: 3.0.0-beta1
&gt; Reporter: Allen Wittenauer
&gt; Priority: Minor
&gt; 




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170802212600</emailId><senderName>"Chris Douglas (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-02 21:26:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14726) Remove FileStatus#isDir</subject><body>

Chris Douglas created HADOOP-14726:
--------------------------------------

             Summary: Remove FileStatus#isDir
                 Key: HADOOP-14726
                 URL: https://issues.apache.org/jira/browse/HADOOP-14726
             Project: Hadoop Common
          Issue Type: Task
          Components: fs
            Reporter: Chris Douglas
            Priority: Minor


FileStatus#isDir was deprecated in 0.21 (HADOOP-6585).



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170802215702</emailId><senderName>"Xiao Chen (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-02 21:57:02-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14727) Socket not closed properly when reading Configurations with BlockRea</subject><body>

Xiao Chen created HADOOP-14727:
----------------------------------

             Summary: Socket not closed properly when reading Configurations with \
BlockReaderRemote  Key: HADOOP-14727
                 URL: https://issues.apache.org/jira/browse/HADOOP-14727
             Project: Hadoop Common
          Issue Type: Bug
          Components: conf
    Affects Versions: 3.0.0-alpha4, 2.9.0
            Reporter: Xiao Chen
            Priority: Blocker


This is caught by Cloudera's internal testing over the alpha3 release.

We got report that some hosts ran out of FDs. Triaging that, found out both oozie \
server and Yarn JobHistoryServer have tons of sockets on {{CLOSE_WAIT}} state.

[~haibochen] helped then narrow down a consistent reproduction by simply visiting the \
JHS webui, and clicking through a job and its logs.

I then look at the {{BlockReaderRemote}} and related code. After adding a debug log \
whenever a {{Peer}} is created/closed/in/out {{PeerCache}}, it looks like all the \
{{CLOSE_WAIT}} sockets are created from this call stack: {noformat}
2017-08-02 13:58:59,901 INFO org.apache.hadoop.hdfs.client.impl.BlockReaderFactory: \
____ associated peer \
NioInetPeer(Socket[addr=/10.17.196.28,port=20002,localport=42512]) with blockreader \
                org.apache.hadoop.hdfs.client.impl.BlockReaderRemote@717ce109
java.lang.Exception: test
        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:745)
                
        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)
                
        at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:636)
                
        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:749)
                
        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:807)
        at java.io.DataInputStream.read(DataInputStream.java:149)
        at com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)
                
        at com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)
                
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)
                
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:647)
        at com.ctc.wstx.stax.WstxInputFactory.createXMLStreamReader(WstxInputFactory.java:366)
                
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2649)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2697)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2662)
                
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2545)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1076)
        at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1126)
        at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1344)
        at org.apache.hadoop.mapreduce.counters.Limits.init(Limits.java:45)
        at org.apache.hadoop.mapreduce.counters.Limits.reset(Limits.java:130)
        at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.loadFullHistoryData(CompletedJob.java:363)
                
        at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.&lt;init&gt;(CompletedJob.java:105)
                
        at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.loadJob(HistoryFileManager.java:473)
                
        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.loadJob(CachedHistoryStorage.java:180)
                
        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.access$000(CachedHistoryStorage.java:52)
                
        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1.load(CachedHistoryStorage.java:103)
                
        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1.load(CachedHistoryStorage.java:100)
                
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)
                
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)
                
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)
        at com.google.common.cache.LocalCache.get(LocalCache.java:3965)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)
        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4829)
                
        at com.google.common.cache.LocalCache$LocalManualCache.getUnchecked(LocalCache.java:4834)
                
        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getFullJob(CachedHistoryStorage.java:193)
                
        at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getJob(JobHistory.java:220)
        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.requireJob(AppController.java:416)
                
        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:277)
                
        at org.apache.hadoop.mapreduce.v2.hs.webapp.HsController.attempts(HsController.java:152)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
                
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:162)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
        at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)
                
        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)
                
        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)
                
        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)
                
        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)
                
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)
                
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)
                
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)
                
        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)
                
        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)
                
        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)
        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)
        at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)
        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
                
        at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)
                
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
                
        at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
                
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
                
        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1552)
                
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
                
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
                
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
                
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
                
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
                
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
                
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
                
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
                
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
                
        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
                
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
  at org.eclipse.jetty.server.Server.handle(Server.java:534)
        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
                
        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
                
        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
                
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
                
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
                
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
                
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
                
        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
  at java.lang.Thread.run(Thread.java:748)
{noformat}

I was able to further confirm this theory by backing out the 4 recent commits to \
                {{Configuration}} on alpha3 and no longer seeing {{CLOSE_WAIT}} \
                sockets.
- HADOOP-14501. 
- HADOOP-14399. 
- HADOOP-14216. Addendum 
- HADOOP-14216. 

It's not clear to me who's responsible to close the InputStream though.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170803085000</emailId><senderName>"Akira Ajisaka (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-08-03 08:50:00-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14729) Upgrade JUnit 3 TestCase to JUnit 4</subject><body>

Akira Ajisaka created HADOOP-14729:
--------------------------------------

             Summary: Upgrade JUnit 3 TestCase to JUnit 4
                 Key: HADOOP-14729
                 URL: https://issues.apache.org/jira/browse/HADOOP-14729
             Project: Hadoop Common
          Issue Type: Test
            Reporter: Akira Ajisaka


There are still test classes that extend from junit.framework.TestCase in \
hadoop-common. Upgrade them to JUnit4.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170602165904</emailId><senderName>"Xiao Chen (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-02 16:59:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14480) Remove Oracle JDK usage in Dockerfile</subject><body>

Xiao Chen created HADOOP-14480:
----------------------------------

             Summary: Remove Oracle JDK usage in Dockerfile
                 Key: HADOOP-14480
                 URL: https://issues.apache.org/jira/browse/HADOOP-14480
             Project: Hadoop Common
          Issue Type: Improvement
            Reporter: Xiao Chen






--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170602170504</emailId><senderName>"Wei-Chiu Chuang (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-02 17:05:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14481) Print stack trace when native bzip2 library does not load</subject><body>

Wei-Chiu Chuang created HADOOP-14481:
----------------------------------------

             Summary: Print stack trace when native bzip2 library does not load
                 Key: HADOOP-14481
                 URL: https://issues.apache.org/jira/browse/HADOOP-14481
             Project: Hadoop Common
          Issue Type: Improvement
          Components: io
            Reporter: Wei-Chiu Chuang
            Assignee: Wei-Chiu Chuang
            Priority: Minor


When I run hadoop checknative on my machine, it was not able to load system bzip2 \
library and printed the following message.

17/06/02 09:25:42 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 \
library system-native, will use pure-Java version

Reviewing the relevant code, it fails because of an exception. However, that \
exception is not logged. We should print the stacktrace, at least at debug log level.

{code:title=Bzip2Factory#isNativeBzip2Loaded()}
try {
          // Initialize the native library.
          Bzip2Compressor.initSymbols(libname);
          Bzip2Decompressor.initSymbols(libname);
          nativeBzip2Loaded = true;
          LOG.info("Successfully loaded &amp; initialized native-bzip2 library " +
                   libname);
        } catch (Throwable t) {
          LOG.warn("Failed to load/initialize native-bzip2 library " + 
                   libname + ", will use pure-Java version");
        }
{code}



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170602183704</emailId><senderName>"Wei-Chiu Chuang (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-02 18:37:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14482) Update BUILDING.txt to include the correct steps to install zstd lib</subject><body>

Wei-Chiu Chuang created HADOOP-14482:
----------------------------------------

             Summary: Update BUILDING.txt to include the correct steps to install \
zstd library  Key: HADOOP-14482
                 URL: https://issues.apache.org/jira/browse/HADOOP-14482
             Project: Hadoop Common
          Issue Type: Improvement
          Components: io
    Affects Versions: 3.0.0-alpha2
            Reporter: Wei-Chiu Chuang
            Priority: Minor


The current BUILDING.txt includes the following steps for installing zstd library:
    $ sudo apt-get install zstd

This is incorrect. On my Ubuntu machine, zstd is not a library
{quote}
apt-cache search zstd
libzstd-dev - fast lossless compression algorithm -- development files
libzstd0 - fast lossless compression algorithm
zstd - fast lossless compression algorithm -- CLI tool
{quote}
In fact, to build Hadoop with ZStandard library, I have to install libzstd-dev.
I will also need to install the runtime to use it. libzstd0 is the older version. \
libzstd1 is for zstd 1.x. It's not clear to me if libzstd0 is compatible. CentOS does \
have libzstd1 though.

{quote}
  * Use -Dzstd.prefix to specify a nonstandard location for the libzstd
    header files and library files. You do not need this option if you have
    installed zstandard using a package manager.

  * Use -Dzstd.lib to specify a nonstandard location for the libzstd library
    files.  Similarly to zstd.prefix, you do not need this option if you have
    installed using a package manager.
{quote}
At least for CentOS, the library installed by rpm was not located and I had to \
specify -Dzstd.prefix to get it installed.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170602193704</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-02 19:37:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14483) increase default value of fs.s3a.multipart.size to 128M</subject><body>

Steve Loughran created HADOOP-14483:
---------------------------------------

             Summary: increase default value of fs.s3a.multipart.size to 128M
                 Key: HADOOP-14483
                 URL: https://issues.apache.org/jira/browse/HADOOP-14483
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/s3
    Affects Versions: 2.8.0
            Reporter: Steve Loughran
            Priority: Minor


increment the default value of {{fs.s3a.multipart.size}} from "100M" to "128M".

Why? AWS S3 throttles clients making too many requests; going to a larger size will \
reduce this. Also: document the issue



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170602195904</emailId><senderName>"Sean Mackrory (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-02 19:59:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14484) Ensure deleted parent directory tombstones are overwritten when impl</subject><body>

Sean Mackrory created HADOOP-14484:
--------------------------------------

             Summary: Ensure deleted parent directory tombstones are overwritten when \
implicitly recreated  Key: HADOOP-14484
                 URL: https://issues.apache.org/jira/browse/HADOOP-14484
             Project: Hadoop Common
          Issue Type: Sub-task
            Reporter: Sean Mackrory


As discussed on HADOOP-13998, there may be a test missing (and possibly broken \
metadata store implementations) for the case where a directory is deleted but is \
later implicitly recreated by creating a file inside it, where the tombstone is not \
overwritten. In such a case, listing the parent directory would result in an error.

This may also be happening because of HADOOP-14457, but we should add a test for this \
other possibility anyway and fix it if it fails with any implementations.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170603165604</emailId><senderName>"wenxin he (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-03 16:56:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14485) Redundant 'final' modifier in try-with-resources statement</subject><body>

wenxin he created HADOOP-14485:
----------------------------------

             Summary: Redundant 'final' modifier in try-with-resources statement
                 Key: HADOOP-14485
                 URL: https://issues.apache.org/jira/browse/HADOOP-14485
             Project: Hadoop Common
          Issue Type: Improvement
    Affects Versions: 3.0.0-alpha4
            Reporter: wenxin he
            Priority: Minor


Redundant 'final' modifier in the try-with-resources statement. Any variable declared \
in the try-with-resources statement is implicitly modified with final.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170605091404</emailId><senderName>"Sonia Garudi (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-05 09:14:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14486) TestSFTPFileSystem#testGetAccessTime test failure</subject><body>

Sonia Garudi created HADOOP-14486:
-------------------------------------

             Summary: TestSFTPFileSystem#testGetAccessTime test failure
                 Key: HADOOP-14486
                 URL: https://issues.apache.org/jira/browse/HADOOP-14486
             Project: Hadoop Common
          Issue Type: Bug
    Affects Versions: 3.0.0-alpha4
         Environment: Ubuntu 14.04 
x86, ppc64le
$ java -version
openjdk version "1.8.0_111"
OpenJDK Runtime Environment (build 1.8.0_111-8u111-b14-3~14.04.1-b14)
OpenJDK 64-Bit Server VM (build 25.111-b14, mixed mode)
            Reporter: Sonia Garudi


The TestSFTPFileSystem#testGetAccessTime test fails consistently with the error below:

{code}
java.lang.AssertionError: expected:&lt;1496496040072&gt; but was:&lt;1496496040000&gt;
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.fs.sftp.TestSFTPFileSystem.testGetAccessTime(TestSFTPFileSystem.java:319)
{code}



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170605140605</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-05 14:06:05-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14487) DirListingMetadata precondition failure messages to include path at </subject><body>

Steve Loughran created HADOOP-14487:
---------------------------------------

             Summary: DirListingMetadata precondition failure messages to include \
path at fault  Key: HADOOP-14487
                 URL: https://issues.apache.org/jira/browse/HADOOP-14487
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/s3
    Affects Versions: HADOOP-13345
            Reporter: Steve Loughran
            Priority: Minor


I've done something wrong in my code and getting "" childPath must be a child of \
path", which is all very well, but it doesn't include paths.

The precondition checks all need to include the relevant path info for users to start \
working out what has gone wrong.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170605143104</emailId><senderName>"Sean Mackrory (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-05 14:31:04-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14484) Ensure deleted parent directory tombstones are overwritten when imp</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14484?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Sean Mackrory resolved HADOOP-14484.
------------------------------------
    Resolution: Duplicate

Resolving this as a duplicate, since I did end up doing it as part of the first \
patch, and it makes sense to continue to do so.

&gt; Ensure deleted parent directory tombstones are overwritten when implicitly \
&gt;                 recreated
&gt; ------------------------------------------------------------------------------------
&gt;  
&gt; Key: HADOOP-14484
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14484
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3
&gt; Reporter: Sean Mackrory
&gt; Assignee: Sean Mackrory
&gt; 
&gt; As discussed on HADOOP-13998, there may be a test missing (and possibly broken \
&gt; metadata store implementations) for the case where a directory is deleted but is \
&gt; later implicitly recreated by creating a file inside it, where the tombstone is not \
&gt; overwritten. In such a case, listing the parent directory would result in an error. \
&gt; This may also be happening because of HADOOP-14457, but we should add a test for \
&gt; this other possibility anyway and fix it if it fails with any implementations.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170605143904</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-05 14:39:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14488) s34guard localdynamo listStatus fails after renaming file into direc</subject><body>

Steve Loughran created HADOOP-14488:
---------------------------------------

             Summary: s34guard localdynamo listStatus fails after renaming file into directory
                 Key: HADOOP-14488
                 URL: https://issues.apache.org/jira/browse/HADOOP-14488
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/s3
            Reporter: Steve Loughran


Running scala integration test with inconsistent s3 client &amp; local DDB enabled

{code}
fs.rename("work/task-00/part-00", work)
fs.listStatus(work)
{code}

The list status work fails with a message about the childStatus not being a child of the parent. 

Hypothesis: rename isn't updating the child path entry



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170605154404</emailId><senderName>"Sean Mackrory (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-05 15:44:04-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14489) ITestS3GuardConcurrentOps requires explicit DynamoDB table name to b</subject><body>

Sean Mackrory created HADOOP-14489:
--------------------------------------

             Summary: ITestS3GuardConcurrentOps requires explicit DynamoDB table name \
to be configured  Key: HADOOP-14489
                 URL: https://issues.apache.org/jira/browse/HADOOP-14489
             Project: Hadoop Common
          Issue Type: Bug
            Reporter: Sean Mackrory
            Assignee: Sean Mackrory


testConcurrentTableCreations fails with this: \
{quote}java.lang.IllegalArgumentException: No DynamoDB table name configured!{quote}

I don't think that's necessary - should be able to shuffle stuff around and either \
use the bucket name by default (like other DynamoDB tests would) or use the table \
name that's configured later in the test.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170605214104</emailId><senderName>"Sean Mackrory (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-05 21:41:04-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14489) ITestS3GuardConcurrentOps requires explicit DynamoDB table name to </subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14489?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Sean Mackrory resolved HADOOP-14489.
------------------------------------
    Resolution: Fixed

Resolving as a duplicate. Thanks [~liuml07]

&gt; ITestS3GuardConcurrentOps requires explicit DynamoDB table name to be configured
&gt; --------------------------------------------------------------------------------
&gt; 
&gt; Key: HADOOP-14489
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14489
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Reporter: Sean Mackrory
&gt; Assignee: Sean Mackrory
&gt; 
&gt; testConcurrentTableCreations fails with this: \
&gt; {quote}java.lang.IllegalArgumentException: No DynamoDB table name \
&gt; configured!{quote} I don't think that's necessary - should be able to shuffle stuff \
&gt; around and either use the bucket name by default (like other DynamoDB tests would) \
&gt; or use the table name that's configured later in the test.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170606003312</emailId><senderName>"Mingliang Liu (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-06 00:33:12-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14490) Upgrade azure-storage sdk version</subject><body>

Mingliang Liu created HADOOP-14490:
--------------------------------------

             Summary: Upgrade azure-storage sdk version
                 Key: HADOOP-14490
                 URL: https://issues.apache.org/jira/browse/HADOOP-14490
             Project: Hadoop Common
          Issue Type: Improvement
            Reporter: Mingliang Liu


As required by [HADOOP-14478], we're expecting the {{BlobInputStream}} to support \
advanced {{readFully()}} by taking hints of mark. This can only be done by means of \
sdk version bump.

cc: [~rajesh.balamohan].



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170606012412</emailId><senderName>"Mingliang Liu (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-06 01:24:12-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14491) Azure has messed doc structure</subject><body>

Mingliang Liu created HADOOP-14491:
--------------------------------------

             Summary: Azure has messed doc structure
                 Key: HADOOP-14491
                 URL: https://issues.apache.org/jira/browse/HADOOP-14491
             Project: Hadoop Common
          Issue Type: Improvement
          Components: documentation, fs/azure
            Reporter: Mingliang Liu
            Assignee: Mingliang Liu


# The _WASB Secure mode and configuration_ and _Authorization Support in WASB_ \
sections are missing in the navigation # _Authorization Support in WASB_ should be \
header level 3 instead of level 2  # Some of the code format is not specified
# Sample code indent not unified.

Let's use the auto-generated navigation instead of manually updating it, just as \
other documents.




--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170606022818</emailId><senderName>"Lantao Jin (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-06 02:28:18-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14492) RpcDetailedMetrics and NameNodeMetrics use different rate metrics ab</subject><body>

Lantao Jin created HADOOP-14492:
-----------------------------------

             Summary: RpcDetailedMetrics and NameNodeMetrics use different rate \
metrics abstraction cause the Xavgtime confused  Key: HADOOP-14492
                 URL: https://issues.apache.org/jira/browse/HADOOP-14492
             Project: Hadoop Common
          Issue Type: Bug
          Components: metrics
    Affects Versions: 2.8.0, 2.7.4
            Reporter: Lantao Jin
            Priority: Minor


For performance purpose, \
[HADOOP-13782|https://issues.apache.org/jira/browse/HADOOP-13782] change the metrics \
behaviour in {{RpcDetailedMetrics}}. In 2.7.4:
{code}
public class RpcDetailedMetrics {

  @Metric MutableRatesWithAggregation rates;
{code}
In old version:
{code}
public class RpcDetailedMetrics {

  @Metric MutableRates rates;
{code}

But {{NameNodeMetrics}} still use {{MutableRate}} whatever in the new or old version:
{code}
public class NameNodeMetrics {
  @Metric("Block report") MutableRate blockReport;
{code}

It causes the metrics in JMX very different between them.
{quote}
{
name: "Hadoop:service=NameNode,name=RpcDetailedActivityForPort8030",
modelerType: "RpcDetailedActivityForPort8030",
tag.port: "8030",
tag.Context: "rpcdetailed",
...
BlockReportNumOps: 237634,
BlockReportAvgTime: 1382,
...
}
{
name: "Hadoop:service=NameNode,name=NameNodeActivity",
modelerType: "NameNodeActivity",
tag.ProcessName: "NameNode",
...
BlockReportNumOps: 2592932,
BlockReportAvgTime: 19.258064516129032,
...
}
{quote}
In the old version. They are correct.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170606053718</emailId><senderName>"Sathishkumar Manimoorthy (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-06 05:37:18-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14493) YARN distributed shell application fails, when RM failed over or Res</subject><body>

Sathishkumar Manimoorthy created HADOOP-14493:
-------------------------------------------------

             Summary: YARN distributed shell application fails, when RM failed over \
or Restarts  Key: HADOOP-14493
                 URL: https://issues.apache.org/jira/browse/HADOOP-14493
             Project: Hadoop Common
          Issue Type: Bug
    Affects Versions: 2.7.0
            Reporter: Sathishkumar Manimoorthy
            Priority: Minor


YARN Distributed shell application fails when doing RM failover or RM restarts.

Exception trace:

17/05/30 11:57:38 DEBUG security.UserGroupInformation: PrivilegedAction as:mapr \
(auth:SIMPLE) from:org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster.renameScriptFile(ApplicationMaster.java:1032)
 17/05/30 11:57:38 DEBUG security.UserGroupInformation: PrivilegedActionException \
as:mapr (auth:SIMPLE) cause:java.io.IOException: Invalid source or target 17/05/30 \
11:57:38 ERROR distributedshell.ApplicationMaster: Not able to add suffix (.bat/.sh) \
                to the shell script filename
java.io.IOException: Invalid source or target
	at com.mapr.fs.MapRFileSystem.rename(MapRFileSystem.java:1132)
	at org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$2.run(ApplicationMaster.java:1036)
  at org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$2.run(ApplicationMaster.java:1032)
  at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1595)
  at org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster.renameScriptFile(ApplicationMaster.java:1032)
  at org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster.access$1400(ApplicationMaster.java:167)
  at org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$LaunchContainerRunnable.run(ApplicationMaster.java:953)
  at java.lang.Thread.run(Thread.java:748)

DS application trying to lo launch the additional container and it is failing to \
rename the path Execscript.sh as it was already renamed by the previous containers in \
filesystem path.

I will upload the logs and path details soon.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170606143618</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-06 14:36:18-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14494) ITestJets3tNativeS3FileSystemContract tests NPEs in teardown if stor</subject><body>

Steve Loughran created HADOOP-14494:
---------------------------------------

             Summary: ITestJets3tNativeS3FileSystemContract tests NPEs in teardown if \
store undefined  Key: HADOOP-14494
                 URL: https://issues.apache.org/jira/browse/HADOOP-14494
             Project: Hadoop Common
          Issue Type: Bug
          Components: fs/s3, test
    Affects Versions: 3.0.0-alpha3
            Reporter: Steve Loughran
            Assignee: Steve Loughran
            Priority: Minor


the move to junit 4 causes the {{.ITestJets3tNativeS3FileSystemContract} tests to NPE \
in teardown if you don't actually declare an s3n test bucket.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170606175118</emailId><senderName>"Lei (Eddy) Xu (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-06 17:51:18-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14495) Add set options interface to FSDataOutputStreamBuilder</subject><body>

Lei (Eddy) Xu created HADOOP-14495:
--------------------------------------

             Summary: Add set options interface to FSDataOutputStreamBuilder 
                 Key: HADOOP-14495
                 URL: https://issues.apache.org/jira/browse/HADOOP-14495
             Project: Hadoop Common
          Issue Type: Sub-task
    Affects Versions: 3.0.0-alpha3
            Reporter: Lei (Eddy) Xu
            Assignee: Lei (Eddy) Xu






--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170606194618</emailId><senderName>"Yongjun Zhang (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-06 19:46:18-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14496) Logs for KMS delegation token lifecycle</subject><body>

Yongjun Zhang created HADOOP-14496:
--------------------------------------

             Summary: Logs for KMS delegation token lifecycle
                 Key: HADOOP-14496
                 URL: https://issues.apache.org/jira/browse/HADOOP-14496
             Project: Hadoop Common
          Issue Type: Improvement
            Reporter: Yongjun Zhang






--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170606194918</emailId><senderName>"Yongjun Zhang (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-06 19:49:18-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14496) Logs for KMS delegation token lifecycle</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14496?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Yongjun Zhang resolved HADOOP-14496.
------------------------------------
    Resolution: Duplicate

&gt; Logs for KMS delegation token lifecycle
&gt; ---------------------------------------
&gt; 
&gt; Key: HADOOP-14496
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14496
&gt; Project: Hadoop Common
&gt; Issue Type: Improvement
&gt; Reporter: Yongjun Zhang
&gt; 




--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170606203018</emailId><senderName>"Mingliang Liu (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-06 20:30:18-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14498) HADOOP_OPTIONAL_TOOLS not parsed correctly</subject><body>

Mingliang Liu created HADOOP-14498:
--------------------------------------

             Summary: HADOOP_OPTIONAL_TOOLS not parsed correctly
                 Key: HADOOP-14498
                 URL: https://issues.apache.org/jira/browse/HADOOP-14498
             Project: Hadoop Common
          Issue Type: Bug
          Components: build
    Affects Versions: 3.0.0-alpha1
            Reporter: Mingliang Liu



# This will make hadoop-azure not show up in the hadoop classpath, though both \
hadoop-aws and hadoop-azure-datalake are in the classpath.{code:title=hadoop-env.sh} \
export HADOOP_OPTIONAL_TOOLS="hadoop-azure,hadoop-aws,hadoop-azure-datalake" {code}
# And if we put only hadoop-azure and hadoop-aws, both of them are shown in the \
classpath. {code:title=hadoop-env.sh}
export HADOOP_OPTIONAL_TOOLS="hadoop-azure,hadoop-aws"
{code}

This makes me guess that, while parsing the {{HADOOP_OPTIONAL_TOOLS}}, we make some \
assumptions that hadoop tool modules have a single "-" in names, and the \
_hadoop-azure-datalake_ overrides the _hadoop-azure_. Or any other assumptions about \
the {{${project.artifactId\}}}?

Ping [~aw].



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170606214318</emailId><senderName>"Sean Mackrory (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-06 21:43:18-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14499) Findbufs warning in LocalMetadataStore</subject><body>

Sean Mackrory created HADOOP-14499:
--------------------------------------

             Summary: Findbufs warning in LocalMetadataStore
                 Key: HADOOP-14499
                 URL: https://issues.apache.org/jira/browse/HADOOP-14499
             Project: Hadoop Common
          Issue Type: Sub-task
            Reporter: Sean Mackrory
            Assignee: Sean Mackrory


First saw this raised by Yetus on HADOOP-14433:
{code}
Bug type UC_USELESS_OBJECT (click for details)
In class org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
In method org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore.prune(long)
Value ancestors
Type java.util.LinkedList
At LocalMetadataStore.java:[line 300]
{code}



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170606221918</emailId><senderName>"Xiao Chen (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-06 22:19:18-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-13474) Add more details in the log when a token is expired</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-13474?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Xiao Chen resolved HADOOP-13474.
--------------------------------
    Resolution: Won't Fix

With more understanding around this area, I think this jira is not necessary.
This is because AuthenticationFilter is usually passing the authentication further \
down to the authentication handler, and that's where we should log more. Will cover \
that in HADOOP-13174, so closing this one.

&gt; Add more details in the log when a token is expired
&gt; ---------------------------------------------------
&gt; 
&gt; Key: HADOOP-13474
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-13474
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: security
&gt; Affects Versions: 2.6.0
&gt; Reporter: Xiao Chen
&gt; Assignee: Xiao Chen
&gt; Attachments: HADOOP-13474.01.patch
&gt; 
&gt; 
&gt; Currently when there's an expired token, we see this from the log:
&gt; {noformat}
&gt; 2016-08-06 07:13:20,807 WARN \
&gt; org.apache.hadoop.security.authentication.server.AuthenticationFilter: \
&gt; AuthenticationToken ignored: AuthenticationToken expired 2016-08-06 09:55:48,665 \
&gt; WARN org.apache.hadoop.security.authentication.server.AuthenticationFilter: \
&gt; AuthenticationToken ignored: AuthenticationToken expired 2016-08-06 10:01:41,452 \
&gt; WARN org.apache.hadoop.security.authentication.server.AuthenticationFilter: \
&gt; AuthenticationToken ignored: AuthenticationToken expired {noformat}
&gt; We should log a better \
&gt; [message|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-au \
&gt; th/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationFilter.java#L456], \
&gt; to include more details (e.g. token type, username, tokenid) for trouble-shooting \
&gt; purpose. I don't think the additional information exposed will lead to any security \
&gt; concern, since the token is expired anyways.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170606223418</emailId><senderName>"Mingliang Liu (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-06 22:34:18-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14500) Azure: TestFileSystemOperationExceptionHandling{,MultiThreaded} fail</subject><body>

Mingliang Liu created HADOOP-14500:
--------------------------------------

             Summary: Azure: TestFileSystemOperationExceptionHandling{,MultiThreaded} \
fails  Key: HADOOP-14500
                 URL: https://issues.apache.org/jira/browse/HADOOP-14500
             Project: Hadoop Common
          Issue Type: Bug
          Components: fs/azure, test
            Reporter: Mingliang Liu


The following test fails:
{code}
TestFileSystemOperationExceptionHandling.testSingleThreadBlockBlobSeekScenario \
Expected exception: java.io.FileNotFoundExceptionTestFileSystemOperationsExceptionHandlingMultiThreaded.testMultiThreadBlockBlobSeekScenario \
Expected exception: java.io.FileNotFoundException {code}

I did early analysis and found [HADOOP-14478] maybe the reason. I think we can fix \
the test itself here.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170606234018</emailId><senderName>"Andrew Wang (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-06 23:40:18-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14501) aalto-xml cannot handle some odd XML features</subject><body>

Andrew Wang created HADOOP-14501:
------------------------------------

             Summary: aalto-xml cannot handle some odd XML features
                 Key: HADOOP-14501
                 URL: https://issues.apache.org/jira/browse/HADOOP-14501
             Project: Hadoop Common
          Issue Type: Bug
          Components: conf
    Affects Versions: 2.9.0, 3.0.0-alpha4
            Reporter: Andrew Wang
            Priority: Blocker


[~hgadre] tried testing solr with a Hadoop 3 client. He saw various test case \
failures due to what look like functionality gaps in the new aalto-xml stax \
implementation pulled in by HADOOP-14216:

{noformat}
   [junit4]    &gt; Throwable #1: com.fasterxml.aalto.WFCException: Illegal XML \
                character ('ü' (code 252))
....
   [junit4]    &gt; Caused by: com.fasterxml.aalto.WFCException: General entity \
reference () encountered in entity expanding mode: operation not (yet) \
                implemented
...
   [junit4]    &gt; Throwable #1: org.apache.solr.common.SolrException: General entity \
reference () encountered in entity expanding mode: operation not (yet) \
implemented {noformat}

These were from the following test case executions:

{noformat}
NOTE: reproduce with: ant test  -Dtestcase=DocumentAnalysisRequestHandlerTest \
-Dtests.method=testCharsetOutsideDocument -Dtests.seed=2F739D88D9C723CA \
-Dtests.slow=true -Dtests.locale=und -Dtests.timezone=Atlantic/Faeroe \
                -Dtests.asserts=true -Dtests.file.encoding=US-ASCII
NOTE: reproduce with: ant test  -Dtestcase=MBeansHandlerTest \
-Dtests.method=testXMLDiffWithExternalEntity -Dtests.seed=2F739D88D9C723CA \
-Dtests.slow=true -Dtests.locale=en-US -Dtests.timezone=US/Aleutian \
                -Dtests.asserts=true -Dtests.file.encoding=US-ASCII
NOTE: reproduce with: ant test  -Dtestcase=XmlUpdateRequestHandlerTest \
-Dtests.method=testExternalEntities -Dtests.seed=2F739D88D9C723CA -Dtests.slow=true \
-Dtests.locale=hr -Dtests.timezone=America/Barbados -Dtests.asserts=true \
                -Dtests.file.encoding=US-ASCII
NOTE: reproduce with: ant test  -Dtestcase=XmlUpdateRequestHandlerTest \
-Dtests.method=testNamedEntity -Dtests.seed=2F739D88D9C723CA -Dtests.slow=true \
-Dtests.locale=hr -Dtests.timezone=America/Barbados -Dtests.asserts=true \
-Dtests.file.encoding=US-ASCII {noformat}



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170607182418</emailId><senderName>"Erik Krogen (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-07 18:24:18-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14502) Confusion/name conflict between NameNodeActivity#BlockReportNumOps a</subject><body>

Erik Krogen created HADOOP-14502:
------------------------------------

             Summary: Confusion/name conflict between \
NameNodeActivity#BlockReportNumOps and RpcDetailedActivity#BlockReportNumOps  Key: \
HADOOP-14502  URL: https://issues.apache.org/jira/browse/HADOOP-14502
             Project: Hadoop Common
          Issue Type: Improvement
          Components: metrics
            Reporter: Erik Krogen
            Priority: Minor


Currently the {{BlockReport(NumOps|AvgTime)}} metrics emitted under the \
{{RpcDetailedActivity}} context and those emitted under the {{NameNodeActivity}} \
context are actually reporting different things despite having the same name. \
{{NameNodeActivity}} reports the count/time of _per storage_ block reports, whereas \
{{RpcDetailedActivity}} reports the count/time of _per datanode_ block reports. This \
makes for a confusing experience with two metrics having the same name reporting \
different values. 

We already have the {{StorageBlockReportsOps}} metric under {{NameNodeActivity}}. Can \
we make {{StorageBlockReport}} a {{MutableRate}} metric and remove \
{{NameNodeActivity#BlockReport}} metric? Open to other suggestions about how to \
address this as well. The 3.0 release seems a good time to make this incompatible \
change.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170607182518</emailId><senderName>"Erik Krogen (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-07 18:25:18-0400</timestampReceived><subject>[jira] [Resolved] (HADOOP-14492) RpcDetailedMetrics and NameNodeMetrics use different rate metrics a</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14492?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Erik Krogen resolved HADOOP-14492.
----------------------------------
    Resolution: Invalid

&gt; RpcDetailedMetrics and NameNodeMetrics use different rate metrics abstraction cause \
&gt;                 the Xavgtime confused
&gt; ---------------------------------------------------------------------------------------------------------
&gt;  
&gt; Key: HADOOP-14492
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14492
&gt; Project: Hadoop Common
&gt; Issue Type: Bug
&gt; Components: metrics
&gt; Affects Versions: 2.8.0, 2.7.4
&gt; Reporter: Lantao Jin
&gt; Assignee: Erik Krogen
&gt; Priority: Minor
&gt; 
&gt; For performance purpose, \
&gt; [HADOOP-13782|https://issues.apache.org/jira/browse/HADOOP-13782] change the \
&gt; metrics behaviour in {{RpcDetailedMetrics}}. In 2.7.4:
&gt; {code}
&gt; public class RpcDetailedMetrics {
&gt; @Metric MutableRatesWithAggregation rates;
&gt; {code}
&gt; In old version:
&gt; {code}
&gt; public class RpcDetailedMetrics {
&gt; @Metric MutableRates rates;
&gt; {code}
&gt; But {{NameNodeMetrics}} still use {{MutableRate}} whatever in the new or old \
&gt; version: {code}
&gt; public class NameNodeMetrics {
&gt; @Metric("Block report") MutableRate blockReport;
&gt; {code}
&gt; It causes the metrics in JMX very different between them.
&gt; {quote}
&gt; name: "Hadoop:service=NameNode,name=RpcDetailedActivityForPort8030",
&gt; modelerType: "RpcDetailedActivityForPort8030",
&gt; tag.port: "8030",
&gt; tag.Context: "rpcdetailed",
&gt; ...
&gt; BlockReportNumOps: 237634,
&gt; BlockReportAvgTime: 1382,
&gt; ...
&gt; name: "Hadoop:service=NameNode,name=NameNodeActivity",
&gt; modelerType: "NameNodeActivity",
&gt; tag.ProcessName: "NameNode",
&gt; ...
&gt; BlockReportNumOps: 2592932,
&gt; BlockReportAvgTime: 19.258064516129032,
&gt; ...
&gt; {quote}
&gt; In the old version. They are correct.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170607204518</emailId><senderName>"Hanisha Koneru (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-07 20:45:18-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14503) MutableMetricsFactory should allow RollingAverages field to be added</subject><body>

Hanisha Koneru created HADOOP-14503:
---------------------------------------

             Summary: MutableMetricsFactory should allow RollingAverages field to be \
added as a metric  Key: HADOOP-14503
                 URL: https://issues.apache.org/jira/browse/HADOOP-14503
             Project: Hadoop Common
          Issue Type: Bug
          Components: common
            Reporter: Hanisha Koneru
            Assignee: Hanisha Koneru


RollingAverages metric extends on MutableRatesWithAggregation metric and maintains a \
group of rolling average metrics. This class should be allowed to register as a \
metric with the MetricSystem.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170607223318</emailId><senderName>"Aaron Fabbri (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-07 22:33:18-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14504) ProvidedFileStatusIterator#next() may throw IndexOutOfBoundsExceptio</subject><body>

Aaron Fabbri created HADOOP-14504:
-------------------------------------

             Summary: ProvidedFileStatusIterator#next() may throw \
IndexOutOfBoundsException  Key: HADOOP-14504
                 URL: https://issues.apache.org/jira/browse/HADOOP-14504
             Project: Hadoop Common
          Issue Type: Sub-task
    Affects Versions: HADOOP-13345
            Reporter: Aaron Fabbri
            Assignee: Aaron Fabbri


[~mackrorysd] noticed this as part of his work on HADOOP-14457.  We are splitting \
that JIRA into smaller patches so we will address this separately.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170607232418</emailId><senderName>"Aaron Fabbri (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-06-07 23:24:18-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14505) simplify mkdirs() after S3Guard delete tracking change</subject><body>

Aaron Fabbri created HADOOP-14505:
-------------------------------------

             Summary: simplify mkdirs() after S3Guard delete tracking change
                 Key: HADOOP-14505
                 URL: https://issues.apache.org/jira/browse/HADOOP-14505
             Project: Hadoop Common
          Issue Type: Sub-task
            Reporter: Aaron Fabbri
            Priority: Minor


I noticed after reviewing the S3Guard delete tracking changes for HADOOP-13760, that \
mkdirs() can probably be simplified, replacing the use of checkPathForDirectory() \
with a simple getFileStatus().

Creating a separate JIRA so these changes can be reviewed / tested in isolation.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170301235045</emailId><senderName>"Mingliang Liu (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-03-01 23:50:45-0400</timestampReceived><subject>[jira] [Reopened] (HADOOP-14129) ITestS3ACredentialsInURL sometimes fails</subject><body>


     [ https://issues.apache.org/jira/browse/HADOOP-14129?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel \
]

Mingliang Liu reopened HADOOP-14129:
------------------------------------

&gt; ITestS3ACredentialsInURL sometimes fails
&gt; ----------------------------------------
&gt; 
&gt; Key: HADOOP-14129
&gt; URL: https://issues.apache.org/jira/browse/HADOOP-14129
&gt; Project: Hadoop Common
&gt; Issue Type: Sub-task
&gt; Components: fs/s3
&gt; Affects Versions: HADOOP-13345
&gt; Reporter: Sean Mackrory
&gt; Assignee: Sean Mackrory
&gt; Fix For: HADOOP-13345
&gt; 
&gt; Attachments: HADOOP-14129-HADOOP-13345.001.patch, \
&gt; HADOOP-14129-HADOOP-13345.002.patch 
&gt; 
&gt; This test sometimes fails. I believe it's expected that DynamoDB doesn't have \
&gt; access to the credentials if they're embedded in the URL instead of the \
&gt; configuration (and IMO that's fine - since the functionality hasn't been in \
&gt; previous releases and since we want to discourage this practice especially now that \
&gt; there are better alternatives). Weirdly, I only sometimes get this failure on the \
&gt; HADOOP-13345 branch. But if the problem turns out to be what I think it is, a \
&gt; simple Assume should fix it.



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170302025246</emailId><senderName>"Zheng Shao (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-03-02 02:52:46-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14137) Allow DistCp to take a file list within a src directory</subject><body>

Zheng Shao created HADOOP-14137:
-----------------------------------

             Summary: Allow DistCp to take a file list within a src directory
                 Key: HADOOP-14137
                 URL: https://issues.apache.org/jira/browse/HADOOP-14137
             Project: Hadoop Common
          Issue Type: New Feature
          Components: tools/distcp
            Reporter: Zheng Shao


DistCp is very slow to start when the src directory has a huge number of \
subdirectories.  In our case, we already have the directory listing (via "hdfs oiv -i \
fsimage" or via nightly "hdfs dfs -lr -r /" dumps), and we would like to use that \
instead of doing realtime listing on the NameNode.

The "-f" option doesn't help in this case because it would try to put everything into \
a single flat target directory.

We'd like to introduce a new option "-list &lt;file&gt;" for distcp.  The &lt;file&gt; contains \
the result of listing the src directory.


In order to achieve this, we plan to:
1. Add a new CopyListing class PregeneratedCopyListing similar to SimpleCopyListing \
which doesn't "-ls -r" into the directory, but takes the listing via "-list" 2. Add \
an option "-list &lt;file&gt;" which will automatically make distcp use the new \
PregeneratedCopyListing class.




--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170302110745</emailId><senderName>"Steve Loughran (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-03-02 11:07:45-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14138) Remove S3A ref from META-INF service discovery, rely on existing cor</subject><body>

Steve Loughran created HADOOP-14138:
---------------------------------------

             Summary: Remove S3A ref from META-INF service discovery, rely on \
existing core-default entry  Key: HADOOP-14138
                 URL: https://issues.apache.org/jira/browse/HADOOP-14138
             Project: Hadoop Common
          Issue Type: Sub-task
          Components: fs/s3
    Affects Versions: 2.9.0
            Reporter: Steve Loughran
            Assignee: Steve Loughran
            Priority: Critical


As discussed in HADOOP-14132, the shaded AWS library is killing performance starting \
all hadoop operations, due to classloading on FS service discovery.

This is despite the fact that there is an entry for fs.s3a.impl in core-default.xml, \
*we don't need service discovery here*

Proposed:
# cut the entry from \
{/hadoop-aws/src/main/resources/META-INF/services/org.apache.hadoop.fs.FileSystem}} # \
when HADOOP-14132 is in, move to that, including declaring an XML file exclusively \
for s3a entries

I want this one in first as its a major performance regression, and one we coula \
actually backport to 2.7.x, just to improve load time slightly there too



--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170302185745</emailId><senderName>"Xiaoyu Yao (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-03-02 18:57:45-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14139) Tracing canonized server name from HTTP request during SPNEGO</subject><body>

Xiaoyu Yao created HADOOP-14139:
-----------------------------------

             Summary: Tracing canonized server name from HTTP request during SPNEGO
                 Key: HADOOP-14139
                 URL: https://issues.apache.org/jira/browse/HADOOP-14139
             Project: Hadoop Common
          Issue Type: Bug
          Components: security
            Reporter: Xiaoyu Yao
            Assignee: Hanisha Koneru
            Priority: Minor


The serverName can be helpful to trouble shoot SPNEGO related authenticated issue.

{code}
 final String serverName = InetAddress.getByName(request.getServerName())
                                           .getCanonicalHostName();
{code}




--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org

</body></email><email><emailId>20170302214845</emailId><senderName>"Vishnu Vardhan (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-03-02 21:48:45-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14140) S3A Not Working 3rd party S3 Interface</subject><body>

Vishnu Vardhan created HADOOP-14140:
---------------------------------------

             Summary: S3A Not Working 3rd party S3 Interface
                 Key: HADOOP-14140
                 URL: https://issues.apache.org/jira/browse/HADOOP-14140
             Project: Hadoop Common
          Issue Type: Bug
    Affects Versions: 2.7.3
            Reporter: Vishnu Vardhan
            Priority: Blocker


Hi:

Connecting S3A to a 3rd party object store does not work. This is a publicly hosted \
grid and i can provide credentials if required. Please see the debug log below

There are two problems -
1. Path Style setting is ignored, and S3A always uses host style addressing
2. Even when host style is specified, it is unable to proceed, see debug log



17/03/02 13:35:03 DEBUG HadoopRDD: Creating new JobConf and caching it for later \
re-use 17/03/02 13:35:03 DEBUG InternalConfig: Configuration override \
awssdk_config_override.json not found. 17/03/02 13:35:03 DEBUG \
AWSCredentialsProviderChain: Loading credentials from BasicAWSCredentialsProvider \
17/03/02 13:35:03 DEBUG S3Signer: Calculated string to sign: "HEAD

application/x-www-form-urlencoded; charset=utf-8
Thu, 02 Mar 2017 21:35:03 GMT
/solidfire/"
17/03/02 13:35:03 DEBUG request: Sending Request: HEAD \
https://solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082 / Headers: \
(Authorization: AWS 2SNAJYEMQU45YPVYC89D:WO0R+mPeYoQ2V29L4dMUJSSSVsQ=, User-Agent: \
aws-sdk-java/1.7.4 Mac_OS_X/10.12.3 \
Java_HotSpot(TM)_64-Bit_Server_VM/25.60-b23/1.8.0_60, Date: Thu, 02 Mar 2017 21:35:03 \
GMT, Content-Type: application/x-www-form-urlencoded; charset=utf-8, )  17/03/02 \
13:35:03 DEBUG PoolingClientConnectionManager: Connection request: [route: \
{s}-&gt;https://solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082][total kept \
alive: 0; route allocated: 0 of 15; total allocated: 0 of 15] 17/03/02 13:35:03 DEBUG \
PoolingClientConnectionManager: Connection leased: [id: 0][route: \
{s}-&gt;https://solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082][total kept \
alive: 0; route allocated: 1 of 15; total allocated: 1 of 15] 17/03/02 13:35:03 DEBUG \
DefaultClientConnectionOperator: Connecting to \
solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082 17/03/02 13:35:03 DEBUG \
RequestAddCookies: CookieSpec selected: default 17/03/02 13:35:03 DEBUG \
RequestAuthCache: Auth cache not set in the context 17/03/02 13:35:03 DEBUG \
RequestProxyAuthentication: Proxy auth state: UNCHALLENGED 17/03/02 13:35:03 DEBUG \
SdkHttpClient: Attempt 1 to execute request 17/03/02 13:35:03 DEBUG \
DefaultClientConnection: Sending request: HEAD / HTTP/1.1 17/03/02 13:35:03 DEBUG \
wire:  &gt;&gt; "HEAD / HTTP/1.1[\r][\n]" 17/03/02 13:35:03 DEBUG wire:  &gt;&gt; "Host: \
solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082[\r][\n]" 17/03/02 13:35:03 \
DEBUG wire:  &gt;&gt; "Authorization: AWS \
2SNAJYEMQU45YPVYC89D:WO0R+mPeYoQ2V29L4dMUJSSSVsQ=[\r][\n]" 17/03/02 13:35:03 DEBUG \
wire:  &gt;&gt; "User-Agent: aws-sdk-java/1.7.4 Mac_OS_X/10.12.3 \
Java_HotSpot(TM)_64-Bit_Server_VM/25.60-b23/1.8.0_60[\r][\n]" 17/03/02 13:35:03 DEBUG \
wire:  &gt;&gt; "Date: Thu, 02 Mar 2017 21:35:03 GMT[\r][\n]" 17/03/02 13:35:03 DEBUG wire: \
&gt;&gt; "Content-Type: application/x-www-form-urlencoded; charset=utf-8[\r][\n]" 17/03/02 \
&gt; &gt; 13:35:03 DEBUG wire:  &gt;&gt; "Connection: Keep-Alive[\r][\n]"
17/03/02 13:35:03 DEBUG wire:  &gt;&gt; "[\r][\n]"
17/03/02 13:35:03 DEBUG headers: &gt;&gt; HEAD / HTTP/1.1
17/03/02 13:35:03 DEBUG headers: &gt;&gt; Host: \
solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082 17/03/02 13:35:03 DEBUG \
headers: &gt;&gt; Authorization: AWS 2SNAJYEMQU45YPVYC89D:WO0R+mPeYoQ2V29L4dMUJSSSVsQ= \
17/03/02 13:35:03 DEBUG headers: &gt;&gt; User-Agent: aws-sdk-java/1.7.4 Mac_OS_X/10.12.3 \
Java_HotSpot(TM)_64-Bit_Server_VM/25.60-b23/1.8.0_60 17/03/02 13:35:03 DEBUG headers: \
&gt;&gt; Date: Thu, 02 Mar 2017 21:35:03 GMT 17/03/02 13:35:03 DEBUG headers: &gt;&gt; \
&gt; &gt; Content-Type: application/x-www-form-urlencoded; charset=utf-8
17/03/02 13:35:03 DEBUG headers: &gt;&gt; Connection: Keep-Alive
17/03/02 13:35:03 DEBUG wire:  &lt;&lt; "HTTP/1.1 200 OK[\r][\n]"
17/03/02 13:35:03 DEBUG wire:  &lt;&lt; "Date: Thu, 02 Mar 2017 21:35:03 GMT[\r][\n]"
17/03/02 13:35:03 DEBUG wire:  &lt;&lt; "Connection: KEEP-ALIVE[\r][\n]"
17/03/02 13:35:03 DEBUG wire:  &lt;&lt; "Server: StorageGRID/10.3.0.1[\r][\n]"
17/03/02 13:35:03 DEBUG wire:  &lt;&lt; "x-amz-request-id: 640939184[\r][\n]"
17/03/02 13:35:03 DEBUG wire:  &lt;&lt; "Content-Length: 0[\r][\n]"
17/03/02 13:35:03 DEBUG wire:  &lt;&lt; "[\r][\n]"
17/03/02 13:35:03 DEBUG DefaultClientConnection: Receiving response: HTTP/1.1 200 OK
17/03/02 13:35:03 DEBUG headers: &lt;&lt; HTTP/1.1 200 OK
17/03/02 13:35:03 DEBUG headers: &lt;&lt; Date: Thu, 02 Mar 2017 21:35:03 GMT
17/03/02 13:35:03 DEBUG headers: &lt;&lt; Connection: KEEP-ALIVE
17/03/02 13:35:03 DEBUG headers: &lt;&lt; Server: StorageGRID/10.3.0.1
17/03/02 13:35:03 DEBUG headers: &lt;&lt; x-amz-request-id: 640939184
17/03/02 13:35:03 DEBUG headers: &lt;&lt; Content-Length: 0
17/03/02 13:35:03 DEBUG SdkHttpClient: Connection can be kept alive indefinitely
17/03/02 13:35:04 DEBUG PoolingClientConnectionManager: Connection [id: 0][route: \
{s}-&gt;https://solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082] can be kept \
alive indefinitely 17/03/02 13:35:04 DEBUG PoolingClientConnectionManager: Connection \
released: [id: 0][route: \
{s}-&gt;https://solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082][total kept \
alive: 1; route allocated: 1 of 15; total allocated: 1 of 15] 17/03/02 13:35:04 DEBUG \
request: Received successful response: 200, AWS Request ID: 640939184 17/03/02 \
13:35:04 DEBUG S3AFileSystem: Getting path status for s3a://solidfire/ () 17/03/02 \
13:35:04 DEBUG S3Signer: Calculated string to sign: "GET

application/x-www-form-urlencoded; charset=utf-8
Thu, 02 Mar 2017 21:35:04 GMT
/solidfire/"
17/03/02 13:35:04 DEBUG request: Sending Request: GET \
https://solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082 / Parameters: \
(max-keys: 1, prefix: , delimiter: /, ) Headers: (Authorization: AWS \
2SNAJYEMQU45YPVYC89D:lfPlFQy/vzQuLHXisTmL1Bb5D/k=, User-Agent: aws-sdk-java/1.7.4 \
Mac_OS_X/10.12.3 Java_HotSpot(TM)_64-Bit_Server_VM/25.60-b23/1.8.0_60, Date: Thu, 02 \
Mar 2017 21:35:04 GMT, Content-Type: application/x-www-form-urlencoded; \
charset=utf-8, )  17/03/02 13:35:04 DEBUG PoolingClientConnectionManager: Connection \
request: [route: {s}-&gt;https://solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082][total \
kept alive: 1; route allocated: 1 of 15; total allocated: 1 of 15] 17/03/02 13:35:04 \
DEBUG PoolingClientConnectionManager: Connection leased: [id: 0][route: \
{s}-&gt;https://solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082][total kept \
alive: 0; route allocated: 1 of 15; total allocated: 1 of 15] 17/03/02 13:35:04 DEBUG \
SdkHttpClient: Stale connection check 17/03/02 13:35:04 DEBUG RequestAddCookies: \
CookieSpec selected: default 17/03/02 13:35:04 DEBUG RequestAuthCache: Auth cache not \
set in the context 17/03/02 13:35:04 DEBUG RequestProxyAuthentication: Proxy auth \
state: UNCHALLENGED 17/03/02 13:35:04 DEBUG SdkHttpClient: Attempt 1 to execute \
request 17/03/02 13:35:04 DEBUG DefaultClientConnection: Sending request: GET \
/?max-keys=1&amp;prefix=&amp;delimiter=%2F HTTP/1.1 17/03/02 13:35:04 DEBUG wire:  &gt;&gt; "GET \
/?max-keys=1&amp;prefix=&amp;delimiter=%2F HTTP/1.1[\r][\n]" 17/03/02 13:35:04 DEBUG wire:  \
&gt;&gt; "Host: solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082[\r][\n]" 17/03/02 \
&gt; &gt; 13:35:04 DEBUG wire:  &gt;&gt; "Authorization: AWS \
&gt; &gt; 2SNAJYEMQU45YPVYC89D:lfPlFQy/vzQuLHXisTmL1Bb5D/k=[\r][\n]"
17/03/02 13:35:04 DEBUG wire:  &gt;&gt; "User-Agent: aws-sdk-java/1.7.4 Mac_OS_X/10.12.3 \
Java_HotSpot(TM)_64-Bit_Server_VM/25.60-b23/1.8.0_60[\r][\n]" 17/03/02 13:35:04 DEBUG \
wire:  &gt;&gt; "Date: Thu, 02 Mar 2017 21:35:04 GMT[\r][\n]" 17/03/02 13:35:04 DEBUG wire: \
&gt;&gt; "Content-Type: application/x-www-form-urlencoded; charset=utf-8[\r][\n]" 17/03/02 \
&gt; &gt; 13:35:04 DEBUG wire:  &gt;&gt; "Connection: Keep-Alive[\r][\n]"
17/03/02 13:35:04 DEBUG wire:  &gt;&gt; "[\r][\n]"
17/03/02 13:35:04 DEBUG headers: &gt;&gt; GET /?max-keys=1&amp;prefix=&amp;delimiter=%2F HTTP/1.1
17/03/02 13:35:04 DEBUG headers: &gt;&gt; Host: \
solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082 17/03/02 13:35:04 DEBUG \
headers: &gt;&gt; Authorization: AWS 2SNAJYEMQU45YPVYC89D:lfPlFQy/vzQuLHXisTmL1Bb5D/k= \
17/03/02 13:35:04 DEBUG headers: &gt;&gt; User-Agent: aws-sdk-java/1.7.4 Mac_OS_X/10.12.3 \
Java_HotSpot(TM)_64-Bit_Server_VM/25.60-b23/1.8.0_60 17/03/02 13:35:04 DEBUG headers: \
&gt;&gt; Date: Thu, 02 Mar 2017 21:35:04 GMT 17/03/02 13:35:04 DEBUG headers: &gt;&gt; \
&gt; &gt; Content-Type: application/x-www-form-urlencoded; charset=utf-8
17/03/02 13:35:04 DEBUG headers: &gt;&gt; Connection: Keep-Alive
17/03/02 13:35:04 DEBUG wire:  &lt;&lt; "HTTP/1.1 200 OK[\r][\n]"
17/03/02 13:35:04 DEBUG wire:  &lt;&lt; "Date: Thu, 02 Mar 2017 21:35:04 GMT[\r][\n]"
17/03/02 13:35:04 DEBUG wire:  &lt;&lt; "Connection: KEEP-ALIVE[\r][\n]"
17/03/02 13:35:04 DEBUG wire:  &lt;&lt; "Server: StorageGRID/10.3.0.1[\r][\n]"
17/03/02 13:35:04 DEBUG wire:  &lt;&lt; "x-amz-request-id: 640939184[\r][\n]"
17/03/02 13:35:04 DEBUG wire:  &lt;&lt; "Content-Length: 256[\r][\n]"
17/03/02 13:35:04 DEBUG wire:  &lt;&lt; "Content-Type: application/xml[\r][\n]"
17/03/02 13:35:04 DEBUG wire:  &lt;&lt; "[\r][\n]"
17/03/02 13:35:04 DEBUG DefaultClientConnection: Receiving response: HTTP/1.1 200 OK
17/03/02 13:35:04 DEBUG headers: &lt;&lt; HTTP/1.1 200 OK
17/03/02 13:35:04 DEBUG headers: &lt;&lt; Date: Thu, 02 Mar 2017 21:35:04 GMT
17/03/02 13:35:04 DEBUG headers: &lt;&lt; Connection: KEEP-ALIVE
17/03/02 13:35:04 DEBUG headers: &lt;&lt; Server: StorageGRID/10.3.0.1
17/03/02 13:35:04 DEBUG headers: &lt;&lt; x-amz-request-id: 640939184
17/03/02 13:35:04 DEBUG headers: &lt;&lt; Content-Length: 256
17/03/02 13:35:04 DEBUG headers: &lt;&lt; Content-Type: application/xml
17/03/02 13:35:04 DEBUG SdkHttpClient: Connection can be kept alive indefinitely
17/03/02 13:35:04 DEBUG XmlResponsesSaxParser: Sanitizing XML document destined for \
handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListBucketHandler
 17/03/02 13:35:04 DEBUG wire:  &lt;&lt; "&lt;?xml version="1.0" encoding="UTF-8"?&gt;[\n]"
17/03/02 13:35:04 DEBUG wire:  &lt;&lt; "&lt;ListBucketResult \
xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;&lt;Name&gt;solidfire&lt;/Name&gt;&lt;Prefix&gt;&lt;/Prefix \
&gt;&lt;Marker&gt;&lt;/Marker&gt;&lt;MaxKeys&gt;1&lt;/MaxKeys&gt;&lt;Delimiter&gt;/&lt;/Delimiter&gt;&lt;IsTruncated&gt;false&lt;/IsTruncated&gt;&lt;/ListBucketResult&gt;"
&gt; 
17/03/02 13:35:04 DEBUG PoolingClientConnectionManager: Connection [id: 0][route: \
{s}-&gt;https://solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082] can be kept \
alive indefinitely 17/03/02 13:35:04 DEBUG PoolingClientConnectionManager: Connection \
released: [id: 0][route: \
{s}-&gt;https://solidfire.vmasgwwebg01-tst.webscaledemo.netapp.com:8082][total kept \
alive: 1; route allocated: 1 of 15; total allocated: 1 of 15] 17/03/02 13:35:04 DEBUG \
XmlResponsesSaxParser: Parsing XML response document with handler: class \
com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListBucketHandler \
17/03/02 13:35:04 DEBUG XmlResponsesSaxParser: Examining listing for bucket: \
solidfire 17/03/02 13:35:04 DEBUG request: Received successful response: 200, AWS \
Request ID: 640939184 17/03/02 13:35:04 DEBUG S3AFileSystem: Not Found: \
                s3a://solidfire/
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: \
s3a://solidfire/  at \
org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)
  at org.apache.spark.rdd.RDD.count(RDD.scala:1157)
  ... 53 elided




--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


</body></email><email><emailId>20170302224345</emailId><senderName>"Vishnu Vardhan (JIRA)"</senderName><senderEmail>jira@apache.org</senderEmail><timestampReceived>2017-03-02 22:43:45-0400</timestampReceived><subject>[jira] [Created] (HADOOP-14142) S3A - Adding unexpected prefix</subject><body>

Vishnu Vardhan created HADOOP-14142:
---------------------------------------

             Summary: S3A - Adding unexpected prefix
                 Key: HADOOP-14142
                 URL: https://issues.apache.org/jira/browse/HADOOP-14142
             Project: Hadoop Common
          Issue Type: Bug
            Reporter: Vishnu Vardhan
            Priority: Critical


Hi:

S3A seems to prefix unexpected prefix to my s3 path

Specifically, in the debug log below the following line is unexpected

&gt; GET /myBkt8/?max-keys=1&amp;prefix=user%2Fvardhan%2F&amp;delimiter=%2F HTTP/1.1

It is not clear where the "prefix" is coming from and why.


I executed the following commands

sc.setLogLevel("DEBUG")
sc.hadoopConfiguration.set("fs.s3a.impl","org.apache.hadoop.fs.s3a.S3AFileSystem")
sc.hadoopConfiguration.set("fs.s3a.endpoint","webscaledemo.netapp.com:8082")
sc.hadoopConfiguration.set("fs.s3a.access.key","")
sc.hadoopConfiguration.set("fs.s3a.secret.key","")
sc.hadoopConfiguration.set("fs.s3a.path.style.access","false")
val s3Rdd = sc.textFile("s3a://myBkt98")
s3Rdd.count()


----


debug log is below


application/x-www-form-urlencoded; charset=utf-8
Thu, 02 Mar 2017 22:40:25 GMT
/myBkt8/"
17/03/02 14:40:25 DEBUG request: Sending Request: GET \
https://webscaledemo.netapp.com:8082 /myBkt8/ Parameters: (max-keys: 1, prefix: \
user/vardhan/, delimiter: /, ) Headers: (Authorization: AWS \
2SNAJYEMQU45YPVYC89D:M8GbLXUuAJ2w5pGx4WJ6hJF3324=, User-Agent: aws-sdk-java/1.7.4 \
Mac_OS_X/10.12.3 Java_HotSpot(TM)_64-Bit_Server_VM/25.60-b23/1.8.0_60, Date: Thu, 02 \
Mar 2017 22:40:25 GMT, Content-Type: application/x-www-form-urlencoded; \
charset=utf-8, )  17/03/02 14:40:25 DEBUG PoolingClientConnectionManager: Connection \
request: [route: {s}-&gt;https://webscaledemo.netapp.com:8082][total kept alive: 0; \
route allocated: 0 of 15; total allocated: 0 of 15] 17/03/02 14:40:25 DEBUG \
PoolingClientConnectionManager: Connection leased: [id: 10][route: \
{s}-&gt;https://webscaledemo.netapp.com:8082][total kept alive: 0; route allocated: 1 of \
15; total allocated: 1 of 15] 17/03/02 14:40:25 DEBUG \
DefaultClientConnectionOperator: Connecting to webscaledemo.netapp.com:8082 17/03/02 \
14:40:25 DEBUG PoolingClientConnectionManager: Closing connections idle longer than \
60 SECONDS 17/03/02 14:40:25 DEBUG PoolingClientConnectionManager: Closing \
connections idle longer than 60 SECONDS 17/03/02 14:40:26 DEBUG RequestAddCookies: \
CookieSpec selected: default 17/03/02 14:40:26 DEBUG RequestAuthCache: Auth cache not \
set in the context 17/03/02 14:40:26 DEBUG RequestProxyAuthentication: Proxy auth \
state: UNCHALLENGED 17/03/02 14:40:26 DEBUG SdkHttpClient: Attempt 1 to execute \
request 17/03/02 14:40:26 DEBUG DefaultClientConnection: Sending request: GET \
/myBkt8/?max-keys=1&amp;prefix=user%2Fvardhan%2F&amp;delimiter=%2F HTTP/1.1 17/03/02 14:40:26 \
DEBUG wire:  &gt;&gt; "GET /myBkt8/?max-keys=1&amp;prefix=user%2Fvardhan%2F&amp;delimiter=%2F \
HTTP/1.1[\r][\n]" 17/03/02 14:40:26 DEBUG wire:  &gt;&gt; "Host: \
webscaledemo.netapp.com:8082[\r][\n]" 17/03/02 14:40:26 DEBUG wire:  &gt;&gt; \
"Authorization: AWS 2SNAJYEMQU45YPVYC89D:M8GbLXUuAJ2w5pGx4WJ6hJF3324=[\r][\n]" \
17/03/02 14:40:26 DEBUG wire:  &gt;&gt; "User-Agent: aws-sdk-java/1.7.4 Mac_OS_X/10.12.3 \
Java_HotSpot(TM)_64-Bit_Server_VM/25.60-b23/1.8.0_60[\r][\n]" 17/03/02 14:40:26 DEBUG \
wire:  &gt;&gt; "Date: Thu, 02 Mar 2017 22:40:25 GMT[\r][\n]" 17/03/02 14:40:26 DEBUG wire: \
&gt;&gt; "Content-Type: application/x-www-form-urlencoded; charset=utf-8[\r][\n]" 17/03/02 \
&gt; &gt; 14:40:26 DEBUG wire:  &gt;&gt; "Connection: Keep-Alive[\r][\n]"
17/03/02 14:40:26 DEBUG wire:  &gt;&gt; "[\r][\n]"
17/03/02 14:40:26 DEBUG headers: &gt;&gt; GET \
/myBkt8/?max-keys=1&amp;prefix=user%2Fvardhan%2F&amp;delimiter=%2F HTTP/1.1 17/03/02 14:40:26 \
DEBUG headers: &gt;&gt; Host: webscaledemo.netapp.com:8082 17/03/02 14:40:26 DEBUG headers: \
&gt;&gt; Authorization: AWS 2SNAJYEMQU45YPVYC89D:M8GbLXUuAJ2w5pGx4WJ6hJF3324= 17/03/02 \
&gt; &gt; 14:40:26 DEBUG headers: &gt;&gt; User-Agent: aws-sdk-java/1.7.4 Mac_OS_X/10.12.3 \
&gt; &gt; Java_HotSpot(TM)_64-Bit_Server_VM/25.60-b23/1.8.0_60
17/03/02 14:40:26 DEBUG headers: &gt;&gt; Date: Thu, 02 Mar 2017 22:40:25 GMT
17/03/02 14:40:26 DEBUG headers: &gt;&gt; Content-Type: application/x-www-form-urlencoded; \
charset=utf-8 17/03/02 14:40:26 DEBUG headers: &gt;&gt; Connection: Keep-Alive
17/03/02 14:40:26 DEBUG wire:  &lt;&lt; "HTTP/1.1 200 OK[\r][\n]"
17/03/02 14:40:26 DEBUG wire:  &lt;&lt; "Date: Thu, 02 Mar 2017 22:40:26 GMT[\r][\n]"
17/03/02 14:40:26 DEBUG wire:  &lt;&lt; "Connection: KEEP-ALIVE[\r][\n]"
17/03/02 14:40:26 DEBUG wire:  &lt;&lt; "Server: StorageGRID/10.3.0.1[\r][\n]"
17/03/02 14:40:26 DEBUG wire:  &lt;&lt; "x-amz-request-id: 563477649[\r][\n]"
17/03/02 14:40:26 DEBUG wire:  &lt;&lt; "Content-Length: 266[\r][\n]"
17/03/02 14:40:26 DEBUG wire:  &lt;&lt; "Content-Type: application/xml[\r][\n]"
17/03/02 14:40:26 DEBUG wire:  &lt;&lt; "[\r][\n]"
17/03/02 14:40:26 DEBUG DefaultClientConnection: Receiving response: HTTP/1.1 200 OK
17/03/02 14:40:26 DEBUG headers: &lt;&lt; HTTP/1.1 200 OK
17/03/02 14:40:26 DEBUG headers: &lt;&lt; Date: Thu, 02 Mar 2017 22:40:26 GMT
17/03/02 14:40:26 DEBUG headers: &lt;&lt; Connection: KEEP-ALIVE
17/03/02 14:40:26 DEBUG headers: &lt;&lt; Server: StorageGRID/10.3.0.1
17/03/02 14:40:26 DEBUG headers: &lt;&lt; x-amz-request-id: 563477649
17/03/02 14:40:26 DEBUG headers: &lt;&lt; Content-Length: 266
17/03/02 14:40:26 DEBUG headers: &lt;&lt; Content-Type: application/xml
17/03/02 14:40:26 DEBUG SdkHttpClient: Connection can be kept alive indefinitely
17/03/02 14:40:26 DEBUG XmlResponsesSaxParser: Sanitizing XML document destined for \
handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListBucketHandler
 17/03/02 14:40:26 DEBUG wire:  &lt;&lt; "&lt;?xml version="1.0" encoding="UTF-8"?&gt;[\n]"
17/03/02 14:40:26 DEBUG wire:  &lt;&lt; "&lt;ListBucketResult \
xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;&lt;Name&gt;myBkt8&lt;/Name&gt;&lt;Prefix&gt;user/vardha \
n/&lt;/Prefix&gt;&lt;Marker&gt;&lt;/Marker&gt;&lt;MaxKeys&gt;1&lt;/MaxKeys&gt;&lt;Delimiter&gt;/&lt;/Delimiter&gt;&lt;IsTruncated&gt;false&lt;/IsTruncated&gt;&lt;/ListBucketResult&gt;"
 17/03/02 14:40:26 DEBUG PoolingClientConnectionManager: Connection [id: 10][route: \
{s}-&gt;https://webscaledemo.netapp.com:8082] can be kept alive indefinitely 17/03/02 \
14:40:26 DEBUG PoolingClientConnectionManager: Connection released: [id: 10][route: \
{s}-&gt;https://webscaledemo.netapp.com:8082][total kept alive: 1; route allocated: 1 of \
15; total allocated: 1 of 15] 17/03/02 14:40:26 DEBUG XmlResponsesSaxParser: Parsing \
XML response document with handler: class \
com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListBucketHandler \
17/03/02 14:40:26 DEBUG XmlResponsesSaxParser: Examining listing for bucket: myBkt8 \
17/03/02 14:40:26 DEBUG request: Received successful response: 200, AWS Request ID: \
563477649 17/03/02 14:40:26 DEBUG S3AFileSystem: Not Found: s3a://myBkt8/user/vardhan
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: \
s3a://myBkt8  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)
  at org.apache.spark.rdd.RDD.count(RDD.scala:1157)
  ... 53 elided




--
This message was sent by Atlassian JIRA
(v6.3.15#6346)

---------------------------------------------------------------------
To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org
For additional commands, e-mail: common-dev-help@hadoop.apache.org


